%\documentclass{SLT2024}

%\interspeechcameraready 

%\captionsetup{labelfont=bf}

%\chapter{LANGUAGE BIAS IN SELF-SUPERVISED LEARNING FOR AUTOMATIC SPEECH RECOGNITION}
\chapter{Language Bias in Self-Supervised Learning for Automatic Speech Recognition}
\label{workchapter2}%% Change label when proper section title has been decieded

%\name[affiliation={1,2}]{Edward}{Storey}
%\name[affiliation={1}]{Naomi}{Harte}
%\name[affiliation={2}]{Peter}{Bell}

%\address{
%  $^1$Sigmedia Lab, School of Engineering, Trinity College Dublin, Ireland\\
%  $^2$Centre for Speech Technology Research, School of Informatics, The University of Edinburgh, UK
%  }

%\email{storeyed@tcd.ie, nharte@tcd.ie, peter.bell@ed.ac.uk}

%\keywords{Speech recognition, self-supervised learning, language bias, language-specific subnetworks, model pruning}

%\newcommand{\red}[1]{\textcolor{red}{#1}}

%\begin{document}
\iffalse
\begin{itemize}
    \item I need something to start this chapter that references the last chapter and carries on from there.
    \item it would start here with no section header, that is what mark and DJ have done Ayushi has this section named introduction
    \item previous chapter is all about small models and efficiency, this chapter is all about a large model and how effective it really is
    \item maybe I could use some of the abstract for this one?
\end{itemize}
\fi


%\maketitle
\iffalse
\begin{abstract}
Self-supervised learning (SSL) is used in deep learning to train on large datasets without the need for expensive labelling of the data. Recently, large Automatic Speech Recognition (ASR) models such as XLS-R have utilised SSL to train on over one hundred different languages simultaneously. However, deeper investigation shows that the bulk of the training data for XLS-R comes from a small number of languages. Biases learned through SSL have been shown to exist in multiple domains, but language bias in multilingual SSL ASR has not been thoroughly examined. In this paper, we utilise the Lottery Ticket Hypothesis (LTH) to identify language-specific subnetworks within XLS-R and test the performance of these subnetworks on a variety of different languages. We are able to show that when fine-tuning, XLS-R bypasses traditional linguistic knowledge and builds only on weights learned from the languages with the largest data contribution to the pretraining data.
\end{abstract}
\fi
%\todo{add in section here linking to the previous chapter}
\section{Introduction} %\change{title needs changing}
\iffalse
\begin{itemize}
    \item In the previous chapter we explored CNN based ASR for small models for small applications
    \item we analysed CNN models from multiple angle
        \begin{itemize}
            \item WER, CER
            \item IWER
            \item Accents
            \item monolingual vs multilingual
            \item adapters
        \end{itemize}
    \item We concluded that even with good data and lots of training time small CNNs are simply too limited and the training time performance is not worth the lack of accuracy
    \item If small parameter counts and inefficient \hl{whats the words for CNNs, transformers, LSTMs etc as a group?} don't produce the results we want wht does?
    \item XLS-R is a multilingual SSL ASR etc etc
    \item we can talk about the size and inefficiency of transformers, we can talk about how CNN were still used as a feature extractor
    \item Do I need to mention that I had more access to compute power? This was a deciding factor in my experiments
    \item XLS-R reports good results but is it actually better, the size and pretraining data are a deciding factor but what are they doing and how are they learning?
\end{itemize}
\fi
In the previous chapter we explored Automatic Speech Recognition (ASR) through a small Convolutional Neural Network (CNN) model with a low parameter count. We explored how to make efficient training datasets for small ASR models in order to improve accuracy of second language (L2) accented speech. In order to improve L2 speech from first language (L1) Spanish speakers speaking L2 English we trained Quartznet 15x5 \cite{quartznet} on data that contained mixed training data from both Spanish and English in order to introduce models to phonemes may exist as minimal pairs \cite{minimal pairs} in L2 English from an L1 Spanish speaker. We found that by mixing datasets Quartznet 15x5 suffered higher error rates in both languages and on L2 accented English. 

We hypothesised that Quartznet 15x5 was not able to retain enough information from both languages is because of the comparably low parameter count of the model when compared to larger transformer based models and due to the inefficiency of CNNs for learning when compared to transformers \cite{some citations to back up this}.\thoughts{maybe I could bring the scalability stuff in here, although I think that these studies would be better to go into detail in the discussion section of the previous chapter} These hypotheses are backed up by advances made in ASR that allow for Self-Supervised Learning (SSL) to process much larger datasets that do not need to be labelled and by using larger parameter counts more specific information can be stored in a model \cite{something?}. The effect of model scalability \info{if I keep this I need to mention it previously} has been shown in multiple monolingual or single domain models \cite{wav2vec, hsu2021hubert, chen2022wavlm} that are able to archive low error rates even when finetuning to small subsets of the language they were pretrained on. Advances released during the undertaking of the research in Chapter \ref{WC1} also show how transformers and SSL can improve multilingual learning and downstream error rates for multiple languages in one models \cite{XLSR-53, babu2021xlsr}. However, while models such as XLS-R can be trained on more than 100 languages the multilingual information learned by such a large model has not been thoroughly explored. Questions such as how does the model store language-specific information, does the language use linguistic information such as language family to learn new languages when finetuning or whether the model is in fact storing information for low-resource languages that it was pretrained on remain unanswered. In this chapter we will explore some of these questions in order to understand the benefits and downsides of using large multilingual SSL ASR models as a comparison to our smaller, but less adaptable models in Chapter \ref{WC1}. \info{remember that I may need to change some of this when the previous chapter is actually written}

Large transformer based ASR models can, unlike smaller models, have enough parameters to process numerous languages within a single model and achieve low error rates when finetuned even on low-data languages \cite{babu2021xlsr, whisper}. It has been consistently shown that fine-tuning large pretrained models provides the best accuracy on a single language task, when compared to smaller models \cite{Huang_CrossLingual, Toshniwal_MultiLingLSTM}. This higher accuracy in multilingual ASR models is achieved by having using architectures that are efficient for learning sequential dependencies seen in speech \cite{transformers?}. Due to the neural scaling principle \cite{neural scaling stuff?} having hundreds of millions of parameters and being pretrained on hundreds of thousands of hours of speech data will increase accuracy and lower finetuned error rates in SSL ASR models. \thoughts{not sure about this wording needs a think} Self-supervised learning (SSL) is increasingly utilised to process such large amounts of data without the need for expensive labelling and has become common in open-source ASR \cite{wav2vec, hsu2021hubert, chen2022wavlm}. However, previous commercial and open-sourced supervised learning models have been shown to produce higher errors when presented with speech data outside the domain of their training data \cite{koenecke2020racial, towards_inclusive}. 

Studies have shown that the data that SSL ASR models are trained on can impact downstream tasks. Boito et al. \cite{sslbias_gender} have experimented with wav2vec 2.0 by pretraining on data that was not balanced equally in gender and then fine-tuning to gender-balanced data. They found that if pretraining data was not gender-balanced, the error would increase when fine-tuning to gender-balanced speech. Meng et al. \cite{sslbias_speechrate} showed that biasing pretraining data towards slow speech rates for SSL transformer-based models improves downstream accuracy, whereas fast speech in pretraining has a performance drop. To show the impact of this pretraining Fuckner et al. \cite{sslbias_Dutch} investigate wav2vec 2.0 performance when fine-tuned to Dutch speech data. They find that Whisper \cite{whisper}, a supervised learning model trained on multilingual data, outperforms wav2vec 2.0 \cite{wav2vec} which is pretrained only using English data. Zhang et al. \cite{zhang2023fast} pretrain the wav2vec 2.0 architecture on a single language. They analysed data from multiple languages and selectively included the utterances that contained the most language similarity with the target language into the pretraining data. With this approach, they could attain baseline results with significantly reduced data and training time. These studies show that biasing the pretraining data towards certain data domains can affect the downstream performance of SSL ASR models.

With this in mind it is vital to analyse the data open-source SSL ASR models are trained on. SSL excels when trained on large datasets and the most abundant source of speech data is English language data. Several models \cite{wav2vec, hsu2021hubert} used for multilingual ASR are trained solely on English data such as LibriSpeech \cite{librispeech} or Libri-Light \cite{librilight}. Other SSL models are pretrained on multilingual data \cite{babu2021xlsr, XLSR-53}, but they will often contain more English data than other languages. Multilingual LibriSpeech (MLS) \cite{MLS} is commonly used as a source of multilingual data for SSL pretraining. However, out of the 50k hours of speech data in MLS 44k are English, since the remaining 8 languages share the other 6k hours of data this becomes a very imbalanced dataset to pretrain a model on. 

The research conducted in this chapter will identify the impact that the imbalance of English data in SSL pretraining has on open-source commercial ASR and understand how large multilingual models are learning language. This will allow us to better understand how large commercial ASR outperforms our previous experiments and whether they are worth the trade-off in size and training time. In order to achieve this, we require techniques that identify low-level behaviours of large deep learning models. Model compression techniques such as model distillation \cite{distillation}, low-rank adaptation \cite{hu2021lora} and model pruning \cite{han2015learning} all look to exploit specific behaviours in large pretrained deep learning models in order to reduce the size of a model without affecting performance. 

Through the use of the Lottery Ticket Hypothesis (LTH) \cite{frankle2018LTH} to prune deep learning models, several studies have shown the existence of language-specific weight groupings or ``subnetworks'' within large ASR models \cite{PARP}. These subnetworks of weights contribute more to learning on a downstream language than other groups. These language-specific subnetworks have been used to improve accuracy in sparsely pruned networks for data of the same or related languages \cite{Lu_Language_adap_xlsr, yang2023learning}. By analysing the differences between subnetworks for different languages in a pretrained multilingual model, we can understand how the model is learning languages.

For this chapter we chose to test XLS-R \cite{babu2021xlsr}, XLS-R is an open-source SSL ASR model based on the same architecture as wav2vec 2.0 \cite{wav2vec}, but expanded to include larger layers and more transformer blocks in the encoder. We evaluate the 300 million parameter version that was pretrained through SSL on multilingual data from 128 different languages. This chapter will utilise LTH in order to identify language-specific weights and subnetworks within XLS-R and evaluate the extent to which language balance in the pretraining data affects bias towards or against performance on various languages in downstream fine-tuning tasks.

\section{Background}
In the previous chapter we attempted to improve Word Error Rates (WER) on multilingual and second language (L2) accents on small low parameter-count Convolutional Neural Network (CNN) Automatic Speech Recognition (ASR) models. We found that multilingual training increased WER on small models and what  large transformer based Self-Supervised Learning (SSL) models were able to achieve much lower WER on the same data. For this section we will explain the methods used to train multilingual SSL ASR models. We will then introduce the concepts behind model compression techniques and why they are useful in understanding how large models learn language. Finally we will explain the methodology we used to apply model compression to multilingual SSL ASR and the results we expect to see.

\subsection{Self-Supervised Learning and XLS-R} \label{ssec:SSL_and_XLSR}
Among widely cited open source SSL ASR models, wav2vec 2.0 and HuBERT were trained on Librispeech \cite{librispeech} and Libri-Light \cite{librilight}, which only include English language data \cite{wav2vec, hsu2021hubert}.XLS-R \cite{babu2021xlsr}, however, is a large SSL ASR model pretrained on 128 different languages. It is built on the same architecture as wav2vec 2.0 but expanded to 24 or 48 transformer blocks with an expanded embedding size of 1024 in the encoder. 

Both wav2vec 2.0 and XLS-R utilise contrastive loss to teach the transformer encoder to reproduce the output of the CNN feature extractor which input to the encoder at the output layer of the encoder \cite{wav2vec, XLSR-53, babu2021xlsr}. Contrastive Loss is designed to allow large SSL models to pretrain on large quantities of data without needing expensive transcriptions. Once contrastive pretraining has completed wav2vec 2.0 or XLS-R can be finetuned on smaller transcribed datasets using Connectionist Temporal Classification (CTC) loss for an ASR task. XLS-R demonstrates the benefit of pretraining on large quantities, in this case hundreds of thousands of hours, of multilingual speech data by achieving low error rates on many languages when finetuned \cite{babu2021xlsr}.  \thoughts{do I need to cite ruso et al here as well?} 

\begin{table}[h]
\centering
\begin{tabular}{lll}
\cline{2-3} \\[-2ex]
 & \multicolumn{2}{c}{Proportion of The Pretraining Data} \\ \cline{1-3} 
\multicolumn{1}{r|}{Language} & \multicolumn{1}{c}{No of Hours} & \multicolumn{1}{c}{Percentage (\%)} \\ \hline
\multicolumn{1}{r|}{English} & \multicolumn{1}{c}{69.5k} & \multicolumn{1}{c}{15.9} \\ %\hline
\multicolumn{1}{r|}{German} & \multicolumn{1}{c}{25.4k} & \multicolumn{1}{c}{5.8} \\ %\hline
\multicolumn{1}{r|}{French} & \multicolumn{1}{c}{24k} & \multicolumn{1}{c}{5.5} \\ %\hline
\multicolumn{1}{r|}{Spanish} & \multicolumn{1}{c}{22.3k} & \multicolumn{1}{c}{5.1} \\ %\hline
\multicolumn{1}{r|}{Polish} & \multicolumn{1}{c}{20.9k} & \multicolumn{1}{c}{4.8} \\ %\hline
\multicolumn{1}{r|}{Catalan} & \multicolumn{1}{c}{691} & \multicolumn{1}{c}{0.16} \\ \hline
\multicolumn{1}{r|}{Total Hours} & \multicolumn{1}{c}{162.8k} & \multicolumn{1}{c}{37.33}   %\hline
\end{tabular}
\caption{\textbf{The proportions to which each language tested in this study exists within the XLS-R pretraining data} XLS-R pretraining data is 436k hours in total}%\\[-6.5ex]}
\label{tab:XLSR_Data_Splits}
\end{table}

%\thoughts{does the position of this table need to be changed?}


However, like other multilingual SSL ASR models, such as its predecessor XLSR-53 \cite{XLSR-53}, the pretraining data for XLS-R contains more English data than any other language, due to the largest datasets included in the XLS-R pretraining data \cite{MLS, wang-etal-2021-voxpopuli}. Table \ref{tab:XLSR_Data_Splits} shows a breakdown of the number of hours of several languages that are included in the pretraining data and the percentage of total pretraining data each language has, the languages shown in Table \ref{tab:XLSR_Data_Splits} are all tested throughout the research in this chapter, a breakdown of the number of hours per language can be found in the original XLS-R paper \cite{babu2021xlsr}.

English language speech data accounts for approximately 15.9\% of the total training data as seen in Table \ref{tab:XLSR_Data_Splits}. Additionally, the first 24 languages account for 98\% of the total data XLS-R was trained on \cite{babu2021xlsr}. We chose XLS-R for this study due to its variety of languages and also the imbalance in the number of hours for each language in the pretraining data. Studies have shown that increasing parameter size and the size of the pretraining dataset decrease the final error on languages that are unseen or low-data in pretraining . For languages that have the highest number of hours in pretraining can be negatively affected by multilingual pretraining when compared to monolingual pretraining, but this effect can be mitigated by larger models and more pretraining data \cite{liscalingasru, pratap20c_interspeech}. Given theses findings and that the XLS-R pretraining is highly imbalanced towards a small number of languages it is important that we understand how large open-source SSL ASR models are learning speech.\change{does this need better wording?}


%If XLS-R has learned speech representations through 69.5k hours of English data but only 3 hours of Irish, for example, how well is it learning Irish when compared to English and is it truly multilingual?\thoughts{I should add in these references and make a point that multilingual pretraeining can degrade the final results \cite{liscalingasru}\cite{pratap20c_interspeech}} Moreover how does it learn new languages when only a few languages are so dominant in the pretraining data.



\subsection{Model Compression and Network Pruning}
Model compression is an area of study that aims to reduce the size of large pretrained models while preserving their accuracy. All of these techniques aim to find the most high-value weights within a model that contribute more to a downstream task than other weights. So, for our purposes, we can adapt them as tools to monitor how weights behave across the model when subjected to a variety of data. There are multiple active areas of research when it comes to compressing large deep-learning models such as model distillation \cite{distillation}, low-rank adaptation \cite{hu2021lora} and model pruning \cite{frankle2018LTH}. 

Model pruning allows for the removal or zeroing of low-value weights across the model if they are not deemed necessary for a downstream tasks \cite{han2015learning}. The Lottery Ticket Hypothesis (LTH) \cite{frankle2018LTH} states that unstructured pruning of weights within a pretrained network can uncover ``winning tickets''. These are subnetworks of weights that contribute more to learning downstream tasks than other groups. For our purposes, analysing these subnetworks can give insight into which weights are most active when different language data stimuli are introduced to a model.

%\newpage
There are multiple approaches to finding winning tickets in a Deep Neural Network (DNN) such as: unstructured pruning \cite{frankle2018LTH} and structured pruning \cite{li2017pruning}. Unstructured pruning selects the highest value weights across a selected region, most commonly layer-wise or globally across the whole model. A pruning target, or sparsity, is set before removing any weights and that percentage of the lowest value weights will be removed from the layer or model. For example, if 50\% sparsity was the pruning target across one linear layer of a DNN, 50\% of the lowest weights across that layer would be zeroed.

It is, however, removing or skipping individual weights with unstructured pruning requires dedicated hardware or software libraries to achieve due to their connection between different neurons \cite{cheng2024survey}, thus making it often impractical. Structured pruning solves this problem by pruning whole neurons within the model based on the summed values of all of its weights. Structured pruning, however, achieves lower final accuracies when compared to unstructured pruning due to its broader approach to finding winning tickets \cite{mao2017exploring, yvinec2021red}. Structured pruning can be abstracted further by pruning whole layers in a model, which again is more computationally efficient but creates even less accurate compressed final models \cite{gromov2024unreasonable}.

Winning tickets can be found at different scales, layer wise or globally. Pruning by layer orders all of the weights in a layer and selects the winning tickets from the highest value weights in each layer, making sparsity consistent at every layer. Global pruning orders all of the weights across every layer of a DNN and selects the winning tickets from the highest value weights across the whole model. With global pruning sparsity can differ per layer. %For example when pruning by layer layer 0 could have 50\% of its weights pruned and layer 1 will also have 50\% of weights pruned. Whereas when globally pruning weights the highest value weights will not be equally valued when pruning and layers can have different percentages pruned, i.e. layer 0 could have 60\% of weights pruned and layer 1 only 40\%.

The most common technique is to prune a model and find winning tickets is to prune after finetuning a pretrained model, this is referred to as One-Shot Magnitude Pruning (OMP) \cite{frankle2018LTH, PARP}, however some other approaches have been suggested. Pruning a pretrained model before finetuning can be effective for reducing computation while finetuning, but will be less accurate than pruning after \cite{PARP}. Iterative Magnitude Pruning (IMP) prunes a percentage of the target sparsity at multiple stages during finetuning\cite{frankle2018LTH}. For example if the target sparsity was 50\% and the model was finetuned for 5 epochs 10\% of the models total weights could be pruned at the end of every epoch. This approach would reach the target sparsity of 50\% after epoch 5 had completed. Other techniques such as pruning after pretraining but before finetuning or pruning before even pretraining have been tested, but have been found to be less effective than either OMP or IMP \cite{frankle2018LTH, liu2018rethinking, You2020Drawing, PARP}

\subsection{Language-Specific Subnetworks}
Several studies have shown that language-specific subnetworks can be obtained when pruning large multilingual ASR models. Lu \textit{et al}. \cite{Lu_Language_adap_xlsr} test joint training with multiple language-specific subnetworks. They pretrain their own version of XLS-R with multiple languages from the Common Voice dataset \cite{ardila-etal-2020-common} and then prune to 40\% sparsity to find the language specific subnetworks for each language. They then continue pretraining, they update the weights shared by each subnetwork when training on all languages, but only update the language-specific weights when pretraining on the relevant language to those weights. They find this approach has less degradation on the initial high-resource language performance than when pruning with a language-agnostic approach. 

Similarly, in 2023, Yang \textit{et al}. \cite{yang2023learning} show that language-specific subnetworks have better performance than language-agnostic approaches. They pretrain a Recurrent Neural Network Transducer (RNN-T) \cite{graves2012rnnt} multilingual model and iteratively prune to find language-specific subnetworks to a high sparsity of 70\% they then train shared weights across subnetworks with all languages and language-specific weights only on the respective language. They find that training on language-specific subnetworks significantly outperforms language agnostic pruning. \info{There is another paper I could include that was published after this work, it doesn't add much but could be included \cite{xiedynamic}} %Finally, Xie et al. \cite{xiedynamic} introduce an adaptive structured pruning technique that zeros weights outside of a pruning mask but allows them to be trained in the next step and then adjusts which weights are pruned if new more valuable weights are formed.\info{this paper was published after my work was submitted so was not included in the relevant paper, is it worth including here?}\change{I need to update this to make it clear how it's different to PARP}

In 2021, Lai \textit{et al}. \cite{PARP} studied language-specific networks in wav2vec 2.0 \cite{wav2vec} and XLSR-53 \cite{XLSR-53}. They introduce a new unstructured pruning technique name Prune Adjust Re-Prune (PARP). PARP prunes over multiple steps by using OMP on a pretrained model to a target sparsity through zeroing weights, these weights are then allowed to update on the next step and the model is pruned again allowing for the pruning mask to be adjusted. They see some improvements over standard OMP and iterative pruning techniques.

While assessing the effectiveness of PARP Lai \textit{et al}. \cite{PARP} use the Intersection Over Union (IOU) to show the overlap of weights in different language-specific subnetworks, finding high overlap between different subnetworks, other than those that are randomly generated. They also test the performance of language-specific subnetworks on various different languages. They find that there is a large variability in performance depending on which language-specific subnetwork is used for which downstream language. However, this study does not explore an English subnetwork and concentrates on the language-agnostic benefits of their own pruning algorithm. In this paper, we explore how pretraining data affects language-specific subnetworks and then assess their performance on a wide array of languages. Approaching language-specific subnetworks with this method will give us insight into how different pretraining languages impact multilingual SSL ASR performance.

\clearpage
\vspace*{\fill}
\begin{figure}[h]
  \centering
  \includegraphics[scale=0.5]{4_chapter_4/figures/Training_Strategy_V2.png}
  \caption{\textbf{Training pipeline for all models} The already pretrained XLS-R is fine-tuned to an upstream language. We then prune the upstream model to a target sparsity and finetune the pruned model on a downstream language. If the upstream and downstream languages match we finetune the pruned upstream model for 10 epochs. If the upstream and downstream languages do not match we freeze the encoder of the upstream model and finetune for 1 extra epoch before unfreezing the encoder and finetuning for 10 epochs}
  \label{fig:Training_Strategy}
\end{figure}
\vspace*{\fill}


\newpage
\section{Experimental Design}
In this section we outline the Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) model and datasets used throughout the experiments in this chapter. We then outline the training strategies and hyperparameters used in all training. Finally we outline the pruning strategies we use to find language-specific subnetworks within our pretrained and finetuned models and then the strategies we use to analyse these subnetworks. 

\subsection{Model} 
For the experiments in this chapter we analyse the SSL ASR model XLS-R \cite{babu2021xlsr}. XLS-R is a large multilingual SSL ASR model, it expands on the basic architecture and training strategies of wav2vec 2.0 \cite{wav2vec}, as outline in Chapter \ref{letreview},\info{this refers to the literature review which I haven't written yet.} expanding the number of transformer blcoks in the econder and expanding the number of hidden states in each layer, XLS-R also uses SSL contrastive loss to pretrain the transformer encoder. XLS-R comes in three sizes a "Base" 24 transformer block encoder with approximately 300m parameters and a layer size of 1204, a "Large" 48 transformer block encoder with approximately 1B parameters and an expanded "Large" model with 48 transformer blocks a layer size of 1920 and approximately 2B parameters. XLS-R \cite{babu2021xlsr} is pretrained on 128 separate languages and has high performance on many languages when fine-tuned. The "base" version of XLS-R with 300M parameters was selected for this paper. 

\subsection{Pruning}
In all of these experiments, we apply L1-norm unstructured one-shot global weight pruning to the encoder of XLS-R. L1-normalisation is the sum of the magnitude of the weights, as seen in equation \ref{pru_equ:l1_norm}, in this case all of the weights across every transformer block in the encoder. We use this as the baseline for which weights to prune depending on the target sparsity. One-shot pruning refers to pruning once at the end of training, in our case after finetuning the pretrained model. Global pruning was used across the transformer-based encoder to target more valuable connections that may exist across layers and not force the pruning of a single layer more than is necessary \cite{frankle2018LTH}. Our pipeline is: finetune XLS-R on FLEURs data, collect all of the weights across every transformer block of the encoder calculate the L1-norm and then prune the encoder to the target sparsity, finally we finetune to the downstream language.


\iffalse
Given a weight vector \( \mathbf{w} = [w_1, w_2, \dots, w_n] \), L1 normalization scales each component of the vector by the sum of the absolute values of all components.
\fi
\newpage
L1 Normalization:\thoughts{not sure if I need this equation, and if I do do I need the other ones to further}

\begin{equation}\label{pru_equ:l1_norm}
\centering
    \mathbf{w}_1 = \sum_{i=1}^{n} |w_i|
\end{equation}
\iffalse
2. Component-wise L1 Normalization

\begin{equation}
\hat{w}_i = \frac{w_i}{\mathbf{w}_1} = \frac{w_i}{\sum_{j=1}^{n} |w_j|}
\end{equation}

3. Vector Form of L1 Normalization

\begin{equation}
\hat{\mathbf{w}} = \frac{\mathbf{w}}{\mathbf{w}_1}
\end{equation}
\fi

Finally we evaluate the weights that have been pruned by calculating the Intersection Over Union (IOU) between 2 or more pruning masks as seen in \cite{PARP}. A pruning mask is generated when pruning a model and is simply a matrix of the weights that will be zeroed or kept\thoughts{is there a better word than kept?}, weights that will be zeroed are set to 0 and weights that will be kept after pruning are set to 1 in the pruning mask. To calculate the IOU we multiply 2 or more pruning masks, here a zeroed weight multiplied by a kept weight or 2 zeroed weights multiplied will always equal to 0 whereas two kept weights multiplied will always equal to 1. This technique allows us to evaluate which weights are commonly used across different languages and which are not.

\subsection{Training Strategies}
To obtain subnetworks for each language tested, we must first finetune XLS-R. We adapt the hyperparameters Rouditchenko et al. \cite{FLEURS_XLSR_WHISPER}\thoughts{we could expand this rather than just referencing the paper, the HP would be LR, GA, BS} used for XLS-R for our setup. We train on 8 2080ti GPUs, use a learning rate of 5e-5 a batch size of 8 split across the 8 GPUs and no gradient accumulation. First, we train XLS-R for 100 epochs and select the point in training with the lowest CTC loss on the validation set for the final upstream model. Finetuning XLS-R to one of the FLEURs 12 hour subsets gives us what will be described from here on as the "upstream model" and the FLEURs language used at this stage will be referred to as the "upstream language". 

The next step is then to prune the encoder of the upstream model to various target sparsities and finetune it again on more FLEURs data, this stage of training will be referred to as the "downstream model" and the FLEURs data used here will be referred to as the "downstream language". When finetuning the pretrained XLS-R we change task from contrastive loss to Connectionist Temporal Classification (CTC) Loss in order to produce character as an output from the model. A single linear output layer is added to the model after the encoder that takes the output from the final transformer block of the XLS-R encoder as an input and outputs characters from the language. The character outputs are collected by iterating through each utterance in the language subset of FLEURs and collecting every text character in that subset. Each character is an output of this final layer and the number of output is different for every language we test.

\newpage
If we consider the following situation: XLS-R has been finetuned on English as the upstream language to create the upstream model, it is then pruned and finetuned again on English as the downstream language to create the downstream model. We consider this as the upstream and downstream languages matching. When the the upstream and downstream languages match they contain all of the same characters so the output layer after the encoder will have the same number of outputs and can be reused and does require retraining. Next, consider the case where the upstream language was again English but this time after pruning we finetune to Spanish. Spanish has a different alphabet to English and a different set of characters in the FLEURs dataset. In this scenario we require a new output layer that will need to be retrained, this would also be true for any 2 different upstream and downstream languages. When the upstream and downstream languages are not the same we will say that the upstream and downstream languages do not match. 

In the scenario that the upstream language and the downstream language match (i.e. English upstream and English downstream) we train the downstream model for 10 epochs at sparsities increasing in steps of 10\%. For downstream languages that do not match the upstream language (i.e. an English model trained on downstream Spanish), we first freeze the encoder of the upstream pruned model for one epoch of training before unfreezing and continuing to train for the full 10 epochs. This extra step is done to give the linear output layer one extra epoch to train and settle its weights before we finetune the full model, we saw in testing that this step reduced final downstream error rates. We evaluate the performance of all models based on Character Error Rate (CER) as in \cite{FLEURS_XLSR_WHISPER}. An illustration of the training pathways can be seen in Figure \ref{fig:Training_Strategy}.

\subsection{Data}
For the experiments in this chapter we use the FLEURS: FEW-Shot Learning Evaluation of Universal Representations of Speech dataset \cite{FLEURS}. FLEURs contains data from 101 languages with approximately 12 hours of data per language. For our upstream languages, we selected English, German, French, Spanish and Polish. These languages are all Indo-European languages from three sub-groups: Germanic, Latin and Slavic \cite{bauer2007linguistics}. They are all high-data languages within the XLS-R pretraining data, as seen in Table \ref{tab:XLSR_Data_Splits}. All of these languages were tested downstream alongside Asturian and Xhosa which are languages unseen by XLS-R in its pretraining \cite{babu2021xlsr}. Asturian is a language spoken in Northern Spain and like Spanish is a Latin language \cite{Asturian_Muñiz-Cachón_2018}. Xhosa is a language spoken in South Africa with no linguistic relationship to the other languages in these experiments \cite{bauer2007linguistics}. Catalan is a low-data language in XLS-R pretraining data and is a Latin language from Spain.






%\newline
%\newpage

\clearpage
\def\figscale{0.83}
\def\capsscale{1}
\begin{figure}
  \centering
  \captionsetup{width=\capsscale\linewidth}
  \includegraphics[width=\linewidth]{4_chapter_4/figures/English_toV6.pdf}
  \caption{\textbf{Upstream English to multiple downstream Languages} an English upstream model is fine-tuned to downstream English, French, German, Polish and Spanish while pruning from 0\% up to 90\% sparsity}
  \label{fig:English_to_0to90}
\end{figure}


\begin{figure}
  \includegraphics[width=\linewidth]{4_chapter_4/figures/Spanish_toV6.pdf}
  \caption{\textbf{Upstream Spanish to five downstream languages} the Spanish upstream model is fine-tuned to downstream English, French, German, Polish and Spanish at 70\%, 80\% and 90\% sparsities}
  \label{fig:Upstream_Spanish}
\end{figure}

\clearpage
\begin{figure}[h]
  \includegraphics[width=\linewidth]{4_chapter_4/figures/Polish_to_V3.pdf}
  \caption{\textbf{Upstream Polish to five downstream Languages} the Polish upstream model is fine-tuned to downstream English, French, German, Polish and Spanish at 70\%, 80\% and 90\% sparsities}
  \label{fig:Upstream_Polish}
\end{figure} 


\section{Results} \label{sec:Results}
\begin{itemize}
    \item write preamble
\end{itemize}
\subsection{Language Specific Subnetworks}
Figure \ref{fig:English_to_0to90} \improvement{Maybe I can expand here too I skip over a lot of the technical aspects}shows XLS-R trained on English and pruned to sparsities starting at 0\% up to 90\% sparsity in increments of 10\%. Figure \ref{fig:English_to_0to90} corroborates findings in previous papers \cite{ding2022audio, Losses_NEURIPS2022_83d349b6, Unstructured_Speech} that show that sparse networks can achieve minimal degradation to the baseline unpruned accuracy of a model up to 50\% or 60\%. The impact of language-specific subnetworks is more pronounced in sparsities at or above 70\%. As such our findings throughout the rest of this paper will concentrate on 70\%, 80\% and 90\% sparsities. 

Figure \ref{fig:English_to_0to90}, Figure \ref{fig:Upstream_Spanish} and Figure \ref{fig:Upstream_Polish} show the same experiments performed with different upstream languages. Figure \ref{fig:English_to_0to90} shows upstream English as a base \improvement{be more consistent with language here "for a base is not very technical"} for the language-specific subnetworks, Figure \ref{fig:Upstream_Spanish} shows upstream Spanish and Figure \ref{fig:Upstream_Polish} shows upstream Polish, three languages from separate Indo-European language families \cite{bauer2007linguistics} and all high-data within the XLS-R pretraining data. \todo{maybe I can split this into 2 paragraphs}Across the three figures, we can observe a clear trend of English as an outlier to the other languages-specific subnetworks. Figure \ref{fig:English_to_0to90} has the lowest CER across all languages, even when the downstream language matches the upstream language as with Spanish in Figure \ref{fig:Upstream_Spanish} and Polish in Figure \ref{fig:Upstream_Polish}. Figure \ref{fig:Upstream_Spanish} and Figure \ref{fig:Upstream_Polish} both show substantial increases in CER on English at 80\% and 90\% sparsity when compared to the other language-specific subnetworks tested. This may suggest that the English language subnetwork is more effective for downstream training. Adverse to the findings in \cite{PARP} Figure \ref{fig:Upstream_Spanish} and Figure \ref{fig:Upstream_Polish} suggest that not training on an English subnetwork is in fact detrimental to downstream fine-tuning, even on languages unrelated to English.

We\thoughts{it needs to be a discussion with Naomi about the we/I/they and how to manage that, Johanna's note generally she changed everything from we to I, but sometimes she has a passive possession and other times a note to say that someone else did this. I'm not sure that this will be necessary for my paper though} further corroborate these findings in Figure \ref{fig:Language_Averages} which shows the average CER for each upstream model. To gauge how well these models generalise, we exclude the language the model was trained on for the results in Figure \ref{fig:Language_Averages}, i.e. the average performance for the English language-specific subnetworks fine-tuned to French, German, Polish and Spanish at each sparsity. We see in Figure \ref{fig:Language_Averages} that the CER at 80\% and 90\% sparsity is the lowest among all subnetworks with an absolute difference of 26.92\% CER at 90\% sparsity between the highest and lowest error and 21.46\% CER at 80\%. While the English CER at 70\% is not the lowest among subnetworks, it is within 1\% error from the lowest of the French subnetwork and 15.09\% less than the highest CER at 70\% sparsity for the Spanish subnetwork. This shows that across all other languages, English language-specific subnetworks perform as well or better than the other subnetworks.

%\newpage
Finally with Figure \ref{fig:AveragedLanguages} we measure the average CER for each language across all upstream models except for when the upstream and downstream languages match. For example, English refers in Figure \ref{fig:AveragedLanguages} to the mean average CER of the French, German, Polish and Spanish upstream models when fine-tuned to English. Figure \ref{fig:AveragedLanguages} shows English as having the highest CER at 80\% and 90\% sparsity and the second highest after French at 70\% sparsity. This is especially apparent at 90\% sparsity when the average CER for English is 92.55\% and the next lowest is German at 47.33\%. These results show English again as the outlier, here however it is the language that causes the highest number of errors among the other languages tested. 

\clearpage
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{4_chapter_4/figures/Language_Averages_ByUpstream.pdf}
  \caption{\textbf{Mean average results for each language-specific subnetwork when tested on all other downstream languages} each subnetwork is fine-tuned to the four other languages, these results are then averaged and plotted at 70\%, 80\% and 90\% sparsity \hl{I wasn't totally ok with the figure captions especially for these mean plots, maybe I could look into expanding them?}}%\\[-3ex]}
  \label{fig:Language_Averages}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{4_chapter_4/figures/Downstream_Averages_V2.pdf}
  \caption{\textbf{Mean average results for each downstream language} each language is trained from an upstream model fine-tuned to each of the four other languages, these results are then averaged and plotted at 70\%, 80\% and 90\% sparsity. Results when the downstream and upstream languages match are not included in the averaging}%\\[-4ex]}
  \label{fig:AveragedLanguages}
\end{figure}
\clearpage
\subsection{Evaluation of the English Subnetwork}
In Table \ref{tab:70perc90perc_English_Mask_Results} we compare the results of the English language-specific subnetwork to the Spanish language-specific subnetwork by fine-tuning to Asturian and Xhosa as well as English and Spanish. We compare the performance from the English subnetwork to the Spanish subnetwork at 70\% and 90\% sparsity. We also test how well Spanish and English fine-tuned models perform when pruned with subnetworks generated from each of the other languages. En / Es is the English fine-tuned model pruned to the Spanish subnetwork and Es / En is the Spanish fine-tuned model pruned to the English subnetwork. In both cases, the English upstream model pruned with the English subnetwork has the lowest average CER.

Finally, in Table \ref{tab:90perc_EnglishSpanish_Mixed_Mask} we show that combining all surviving weights from both English and Spanish subnetworks, noted in Table \ref{tab:90perc_EnglishSpanish_Mixed_Mask} as subnetwork En / Es, does not decrease CER more than the English single-language subnetwork alone. While the error on English is reduced on the Spanish upstream model, it is still 3\% higher in CER than the upstream English model with the English subnetwork.

\begin{table}[h]
\centering
\begin{tabular}{llll}
\cline{2-4} \\[-1.5ex] 
 & \multicolumn{3}{c}{\textbf{CER (\%) at 90\% Sparsity }} \\ \cline{2-4}{} \\[-1.5ex]
 & \multicolumn{3}{c}{\textbf{Language}} \\[0.5ex] \hline
\multicolumn{1}{l|}{\textbf{Model / Subnet}} & \multicolumn{1}{l}{\textbf{En}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{\textbf{Es}}  \\ \hline
\multicolumn{1}{r|}{\textbf{En / En}} & \multicolumn{1}{l}{\textbf{31.79}} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{28.51}   \\ %\hline
\multicolumn{1}{r|}{\textbf{Es / Es}} & \multicolumn{1}{l}{97.97} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{\textbf{23.33}}  \\ %\hline
\multicolumn{1}{r|}{\textbf{En / EnEs}} & \multicolumn{1}{l}{34.88} & \multicolumn{1}{l}{} &\multicolumn{1}{l}{27.74} \\ %\hline
\multicolumn{1}{r|}{\textbf{Es / EnEs}} & \multicolumn{1}{l}{40.94} & \multicolumn{1}{l}{} & \multicolumn{1}{l}{23.86}   \\ \hline 
\end{tabular}
\caption{\textbf{Upstream English and Spanish models trained with mixed weight subnetworks at 90\% sparsity} we test the effect of combining the surviving weights across Spanish and English subnetworks after pruning at 90\% sparsity \hl{needs moving to more appropriate section}}%\\[-8ex]}
\label{tab:90perc_EnglishSpanish_Mixed_Mask}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{4_chapter_4/figures/IOU_EPSB_V2.pdf}
  \caption{\textbf{Intersection Over Union (IOU) at 90\% sparsity} IOU overlap of weights in subnetworks between English, Spanish and Polish subnetworks and the subnetwork found in the base model before fine-tuning. All subnetworks pruned to 90\%.}
  \label{fig:IOU90perc}
\end{figure}

%\begin{table*}
\begin{table}
\centering
\begin{tabular}{lllllllllll}
& \\ \cline{2-11} \\[-2.3ex] 
& \multicolumn{5}{c}{\textbf{CER (\%) at 70\% Sparsity}} & \multicolumn{5}{|c}{\textbf{CER (\%) at 90\% Sparsity}} \\[0.75ex] \cline{2-11} \\[-2.35ex]
& \multicolumn{5}{c}{\textbf{Language}}  & \multicolumn{5}{|c}{\textbf{Language}} \\[0.5ex] \hline
\multicolumn{1}{l|}{\textbf{Model / Subnet}} & \multicolumn{1}{l}{\textbf{En}} & \multicolumn{1}{l}{\textbf{Es}} & \multicolumn{1}{l}{\textbf{As}} & \multicolumn{1}{l}{\textbf{Xh}} & \multicolumn{1}{l}{\textbf{Avg}} & \multicolumn{1}{|l}{\textbf{En}} & \multicolumn{1}{l}{\textbf{Es}} & \multicolumn{1}{l}{\textbf{As}} & \multicolumn{1}{l}{\textbf{Xh}} & \multicolumn{1}{l}{\textbf{Avg}} \\ \hline
\multicolumn{1}{r|}{\textbf{En / En}} & \multicolumn{1}{l}{\textbf{11.74}} & \multicolumn{1}{l}{6.61} & \multicolumn{1}{l}{\textbf{11.53}} & \multicolumn{1}{l}{\textbf{10.57}} & \textbf{10.11} & \multicolumn{1}{|l}{\textbf{31.79}} & \multicolumn{1}{l}{28.51} & \multicolumn{1}{l}{\textbf{33.31}} & \multicolumn{1}{l}{{23.06}} & \textbf{29.17} \\ %\hline
\multicolumn{1}{r|}{\textbf{En / Es}} & \multicolumn{1}{l}{12.74} & \multicolumn{1}{l}{15.46} & \multicolumn{1}{l}{21.34} & \multicolumn{1}{l}{21.65} & 17.8 & \multicolumn{1}{|l}{44} & \multicolumn{1}{l}{28.86} & \multicolumn{1}{l}{34.61} & \multicolumn{1}{l}{{22.77}} & 32.56 \\ %\hline
\multicolumn{1}{r|}{\textbf{Es / Es}} & \multicolumn{1}{l}{{28.28}} & \multicolumn{1}{l}{10.87} & \multicolumn{1}{l}{16.96} & \multicolumn{1}{l}{20.55} & 19.17 & \multicolumn{1}{|l}{{97.97}} & \multicolumn{1}{l}{\textbf{23.33}} & \multicolumn{1}{l}{33.49} & \multicolumn{1}{l}{\textbf{21.48}} & 44.07 \\ %\hline
\multicolumn{1}{r|}{\textbf{Es / En}} & \multicolumn{1}{l}{14.11} & \multicolumn{1}{l}{\textbf{5.31}} & \multicolumn{1}{l}{27.4} & \multicolumn{1}{l}{23.25} & 17.52 & \multicolumn{1}{|l}{42.5} & \multicolumn{1}{l}{{26.52}} & \multicolumn{1}{l}{38.27} & \multicolumn{1}{l}{22.61} & 32.48 \\ \hline
\end{tabular}
\caption{\textbf{Upstream English and Spanish models trained with exchanged subnetworks at 70\% and 90\% sparsity} we test the effect of pruning with a Spanish subnetwork on an English upstream model and an English subnetwork on a Spanish upstream model across multiple languages at 70\% and 90\% sparsity \hl{this needs to be split into 2 separate tables and table references need to be changedsthis needs to be split into 2 separate tables and table references need to be changeds} \hl{since I am splitting the data into 2 tables do I need to include a third table that includes 70\% sparsity results}}%\\[-3.6ex]}
\label{tab:70perc90perc_English_Mask_Results}
%\end{table*}
\end{table}
\subsection{Intersection Over Union}
The results in this section so far have shown that fine-tuning on the English subnetwork achieves lower error at high sparsities than any other subnetwork. To explore further how this behaviour is occurring we can use the Intersection Over Union (IOU) equation from \cite{PARP}. IOU is a measure of how many weights overlap between two subnetworks. Figure \ref{fig:IOU90perc} shows the overlap of saved weights between two subnetworks when a model is pruned to 90\%. We test IOUs between English, Polish and Spanish subnetworks. Figure \ref{fig:IOU90perc} shows the lowest IOU at 80.67\% when comparing the English and Spanish subnetworks. This corroborates the findings in \cite{PARP} that discovered high overall overlap between different language-specific subnetworks. However, given our findings in Section \ref{sec:Results} we can surmise that the remaining weights not overlapping have significant influence over downstream training. We also see in Figure \ref{fig:IOU90perc} that the English language subnetwork has the most saved weights in common with the base XLS-R model. Adversely the Spanish subnetwork has the least in common with the base XLS-R weights. This may imply that fine-tuning XLS-R to English requires the least change in value for weights from the base model, prior to fine-tuning, when compared to other languages. \todo{maybe there could be a deeper discussion of the IOUs and maybe some more graphs here I do have more data (pretty sure)}

Finally Figure \ref{fig:IOU90perc_ACSvsE} shows the IOUs calculated between four languages: English, Spanish, Catalan and Asturian. Spanish, Catalan and Asturian are all languages of Spain and all Latin languages whereas English is a Germanic language \cite{bauer2007linguistics}. Spanish and English are high-data languages in the XLS-R pretraining data, Catalan is a low-data language and Asturian is unseen in the XLS-R pretraining data. We see in Figure \ref{fig:IOU90perc_ACSvsE} that both Asturian and Catalan subnetworks have higher overlap in saved weights with the English language subnetwork than the Spanish subnetwork. Catalan has an IOU of 78.75\% with Spanish and 80.84\% with English and Asturian has an IOU of 81.01\% with Spanish and 84.66\% with English. This suggests that when learning low-data or new languages XLS-R is making use of the weights specific to English more than it is using weights specific to Spanish. This shows that XLS-R builds on weights learned for English when learning new or low-resource languages regardless of their language families. \thoughts{does there need to be a discussion on why I chose the 90\% sparsity? I have enough graphs to justify it.}
\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{4_chapter_4/figures/IOU_ESAC_V4.pdf}
  \caption{\textbf{Intersection Over Union (IOU) at 90\% sparsity for Spanish languages and English} Overlap of weights in subnetworks between English with Asturian and Catalan, then Spanish with Asturian and Catalan. Subnetworks pruned to 90\%.}%\\[-5ex]}
  \label{fig:IOU90perc_ACSvsE}
\end{figure}  

\section{Discussion}
Throughout Section \ref{sec:Results} we see the English subnetworks in XLS-R producing lower Character Error Rates (CER) when fine-tuned across all languages when compared to other language-specific subnetworks. Previous studies of language-specific subnetworks make the assumption that a subnetwork generated for a language will be the most efficient to further train on data from the same language or a linguistically related language \cite{Lu_Language_adap_xlsr, yang2023learning}. However, our experiments show that when pretraining data is imbalanced, this is not the case. The results in this paper instead show that regardless of linguistic content lower error will always be achieved by training on weights generated for the language with the highest data in pretraining, in this case English. The English dominance of XLS-R's performance is true for Germanic languages, such as English and German, but also languages from other Indo-European language families and one South African language. 

Through the use of the Intersection Over Union (IOU) we next analyse where overlap between the weights of different language-specific subnetworks occurs. We find the minimum overlap to be 78.75\%, which broadly corroborates previous results showing high overall overlap \cite{PARP}. However, given our previous findings, the remaining weights that are specific to one language do appear to impact downstream accuracy significantly. We compare each subnetwork with the base weights of XLS-R after pretraining but before fine-tuning. This shows us that the English subnetwork has the least deviation from the base model. This implies that the reliance on English has been taught to the model at the pretraining stage and not during downstream fine-tuning. Additionally, we also measure the overlap between subnetworks generated from two languages from Spain, Catalan and Asturian, to the English and Spanish subnetworks. We find that both Catalan and Asturian language-specific subnetworks have more overlap in saved weights with the English language-specific subnetwork. This reinforces our findings that when fine-tuning, XLS-R weights learned in pretraining from English language data are more impactful to training. This is regardless of the linguistic relation to English the downstream training data has.

This paper shows that Self-Supervised Learning (SSL) in Automatic Speech Recognition (ASR) can bias fine-tuning tasks towards weights learned from the data domains most present in its pretraining data. Open-source SSL ASR pretraining data is highly imbalanced towards a small number of languages. These models are over-reliant on the features learned from those languages which leads them to ignore linguistic relationships when fine-tuning to unseen or low-data languages. Previous research \cite{zhang2023fast} has shown that downstream tasks can benefit from linguistically guided pretraining so fine-tuning on linguistically unrelated features is both unintuitive and inefficient. Given this, we recommend balancing the pretraining data by language and linguistic relationships. When deploying pretrained open-source models researchers must carefully analyse the data the model has been pretrained with. Imbalanced pretraining data that is abundant in just one or a few languages, but limited across many others, is not best suited for efficient multilingual SSL ASR.


\section{Future Work}
%% May not need this section in this chapter we will see

\section{Conclusion}


%\section{FILLER SECTION}
%\section{FILLER SECTION}

\nomenclature[Z]{DNN}{Deep Neural Network}
\nomenclature[Z]{L2}{Second Language}
\nomenclature[Z]{OMP}{One-Shot Magnitude Pruning}
\nomenclature[Z]{IMP}{Iterative Magnitude Pruning}
\nomenclature[Z]{RNN-T}{Recurrent Neural Network Transducer}
\nomenclature[Z]{IOU}{Intersection Over Union}

%\section{ACKNOWLEDGEMENTS}
%\textit{\footnotesize This work was conducted with the financial support of the Science Foundation Ireland %Centre for Research Training in Digitally-Enhanced Reality (d-real) under Grant No. 18/CRT/6224 and ADAPT SFI %Research Centre under Grant No. 13/RC/2106 P2. For the purpose of Open Access, the author has applied a CC BY %public copyright licence to any Author Accepted Manuscript version arising from this submission} 

%\newpage
%\bibliographystyle{IEEEtran}
%\bibliography{bibs/workchapter2}
%\end{document}