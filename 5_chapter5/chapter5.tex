\iffalse\title{Analyzing the Effect of Data Balance on the Learning of Multilingual Self-Supervised Learning}
\begin{abstract}
We analyse how multilingual self-supervised learning (SSL) Automatic Speech Recognition (ASR) models learn speech representations when pretrained with monolingual and multilingual training data. We find that the English LibriSpeech data had the lowest Character Error Rate (CER) on the monolingual English model and the highest CER on the multilingual Spanish model with CER steadily increasing as the amount of English data was reduced in the pretraining data. The Germanic languages Danish and German both have a positive difference towards English and the only language with a more positive difference than these languages is English. We suggest that error rates on low-resource languages could be reduced by using data from languages that do not have to have any relation to the target finetuning language. We conclude by suggesting approaches to data balance that could improve the performance SSL training in future work and create efficient and effective training methods going forward.
\end{abstract}\fi
%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Fifth Chapter **********************************
%*******************************************************************************
%\chapter{Phonetic Understandings of Multilingual Self-Supervised Learning Automatic Speech recognition Models}
\chapter{Analysing the Effect of Data Balance on the Learning of Multilingual Self-Supervised Learning\naominote{shorter title}}
\section{Introduction}
\perplexnote{[referencing whole section] There are multiple uses of “we then,” which makes the writing feel repetitive. Try varying sentence openers, e.g., “Subsequently,” “Next,”}In the previous chapter, we analysed the multilingual performance of a widely cited open-source multilingual self-supervised Learning (SSL) Automatic Speech Recognition (ASR) model XLS-R \cite{babu2021xlsr}. We found through pruning \cite{frankle2018LTH} that when finetuning the pretrained model to data from languages that were unseen in the XLS-R pretraining data, that weights learned from the English language were used as basis for learning.\perplexnote{[for previous sentence] Suggestion: Split into two sentences. Consider: [perplexity suggestion]} This effect was observed \thiswillnotshow{\naominote{[\textbf{Done}]"observed" I'm not sure exactly what Naomi meant by this}} even when the finetuned language was present but low-data in pretraining.\iffalse \thoughts{doesn't make sense needs completely rewording} When a related language was present and high-data, such as Catalan and Asturian using \thiswillnotshow{\naominote{[\textbf{Done}]Sentence too long.}}English weights rather than Spanish weights to learn from\cite{storeyedbiasSSL}.\perplexnote{[Previous sentence] This is very complex and would benefit from revision.} \fi This effect in XLS-R appears to stem from the imbalance of languages in the pretraining, with English having the most amount of hours when compared to all other languages. For this chapter, we explore in more depth how the balance of languages in pretraining affects SSL ASR.\thiswillnotshow{\naominote{* Is that your question for the chapter?\textbf{ED:} I could change this into more of a question or I could remove or move it}}
\summary{This paragraph summarises the previous chapter}
\iffalse
All popular open-source SSL ASR models are either pretrained on a single language \cite{wav2vec, hsu2021hubert, chen2022wavlm} or have an imbalance in the number of hours of the languages in pretraining, with English as the dominant language \cite{XLSR-53, babu2021xlsr}. It is therefore important we understand how the ratio of hours between each language in the pretraining data affects the performance of an SSL ASR model. Previous work has suggested that monolingual pretraining\info{I need to keep this phrase consistent "monolingual pretraining" vs "multilingual pretraining"}, pretraining an SSL model on a single language, and then finetuning to the same language used in pretraining is the most efficient approach to SSL ASR when compared to other approaches.\info{I have mentioned efficiency now}\thoughts{"other appraoches" is not clear however, I do then explain some of those approaches, maybe I should expand on the findings of the papers instead of this uncited sentence} This approach to pretraining can achieve low error rates with less training time and fewer parameters when compared to other approaches \cite{ashihara2023exploration, poncelet2021comparison}. Low error rates can also be achieved by combining pretraining data from languages that show linguistic similarity, through clustering of extracted features \cite{zhang2023fast, gupta2021clsril}\info{these two references both use clustering of extracted features from DNNs to show language similarity}.\hl{I think I should expand on these citations instead of just referencing them}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many popular open-source SSL ASR models are pretrained on a single language \cite{wav2vec, hsu2021hubert, chen2022wavlm}. Previous studies have found monolingual pretraining, pretraining an SSL model only on one language before finetuning and evaluation, to be highly efficient and resulting in low error rates, reduced training time, and fewer parameters compared with multilingual pretraining strategies \cite{ashihara2023exploration, poncelet2021comparison}.\thoughts{it needs to be made clear we are finetuning on the same language} Additionally, successful outcomes have been reported when pretraining incorporates linguistically similar languages. Grouping related languages or clustering data by shared speech features can help maintain strong recognition performance even for low resource languages \cite{zhang2023fast, gupta2021clsril}. \summary{This chapter introduces the effectiveness of monolingual pretraining and related langauges in pretraining}

SSL ASR models that are multilingual typically use data with a significant imbalance in language representation, with English as the dominant language \cite{XLSR-53, babu2021xlsr}. Given the findings in Chapter \ref{chapter4}, understanding how the ratio of training hours among languages affects downstream ASR performance is crucial. It will help us understand why monolingual pretraining has been effective, as well as revealing if well designed pretraining data can improve the performance of multilingual SSL ASR. In order to progress, we first need to understand which aspects of speech SSL ASR models are learning in the SSL pretraining stage.

\iffalse \hl{How does mixing language data in pretraining affect how SSL learns phonetic and acoustic information? Since SSL does not learn semantic information, why is monolingual pretraining so effective?}\thoughts{Actually these chapter questions can go later!!}\thoughts{These are the chapter questions, they could be better worded}\summary{Here I say it is important because of the biases that could be introduced. I then state the chapter queestions.} %\thoughts{ Probably what needs to happen here is that I need to introduce the concept of pretraining with imbalanced multilingual data being bad first, then sqitch to the technical "How do we figure out why it's bad part"}% Building on this, Storey and Bell show that models pretrained on imbalanced multilingual data tend to rely disproportionately on representations learned from high-resource languages such as English, which can influence both downstream accuracy and the way phonetic and linguistic structure are encoded across languages \cite{storeyedbiasSSL}.\summary{This chapter needs to explain how SSL models learn and why this is important}\thoughts{However, I think the message is confused I summarise the smae information from teh last chapter here after explaining how SSL learns}
\fi
SSL ASR models learn speech representations in two stages: an SSL pretraining stage followed by supervised finetuning. During pretraining, models are trained on large unlabelled speech datasets, which greatly expands the range of usable data sources. Because SSL pretraining does not involve labels, a second stage of supervised finetuning on labelled data is required.\summary{Here I have started to explain how SSL is different to SL} 
\iffalse
\begin{itemize}
    \item At this point we know:
    \item multilingual models learn from their most prominent data source/language
    \item monolingual pretraining is more efficient than multilingual
    \item related languages can help with recognition even in finetuning later
    \item We need to know then what is being learned in pretraining:
    \item phonetic and acoustic features only, semantic features are entirely introduced in finetuning
    \item We know from cormac's work (\hl{NEED TO ADD THIS THEN}) that the pretrained model can recognise phonemes well but of course cannot understand semantic information
    \item Pasad shows that the change in weights in shallow layers is small
    \item The effectiveness of phoneme recognition during pretraining is having an effect on the final performance
    
    \item --------------------------------------------------------------------------
    \item What we don't know
    \item how much data imbalance affects downstream finetuning
    \item Where the line is in multilingual data? does some multilingual help and is there a perfect ratio?
    \item how SSL ASR learns phonemes when there are multiple languages? If a phoneme shared by more than one language appears in both pretraining datasets will the accuracy of that phoneme get better or worse?
    \item is there an upside to multilingual training when the parameters are even?
    \item why must the pretraining data be from related languages if you can't find enough of the source language?
    \item --------------------------------------------------------------------------

    \iffalse
    \item However, it has never been studied how well a pretrained model understands phonemes when the data is mixed, we have only looked at the final performance
    \item We know that performance will likely be lower, but why? how do the languages involved affect phoneme recognition.
    \item we pretrain with two European languages that have significant overlap in their phoneme dictionaries, but each with a significant number of phonemes that do not overlap
    \item These languages are not related and so we can experiment with how specific phoneme groups are affected when more or less data is introduced
    \item testing both CER and phoneme accuracy on models with gradually increasing/decreasing amounts of each language shows us not 
    \fi
\end{itemize}
\fi

Pasad et al. \cite{pasad2021layer} show that the SSL pretrained wav2vec 2.0 model learns local acoustic features and phone identity without needing finetuning. English et al. \cite{english2022domain} show through layer probing that a pretrained wav2vec 2.0 is not only effective at learning phoneme representations, but that the models cluster phonemes by articulatory and positional relationships. With this information, we can gauge firstly that SSL pretraining alone is effective for learning phonetic information. Then secondly, that the phonetic knowledge information it learns has a significant impact on the final ASR performance of the model. However, since both papers experiment with a monolingual English wav2vec 2.0, there are some gaps in our knowledge on what and how these models are learning. This is especially apparent in a multilingual setting where phoneme relations and accuracy have not been measured.

\iffalse
Pasad et al. \cite{pasad2021layer} show that the SSL pretrained wav2vec 2.0 model learns local acoustic features and phone identity, but does not learn semantic tasks until it has been finetuned. \iffalse The semantic tasks are learned predominantly in the deepest layers after finetuning. This is because during pretraining, the deepest layers are trained for an autoencoder style task unrelated to the semantic finetuning task.\fi While the semantic task is introduced solely in finetuning, we know that the pretraining data has a significant impact on finetuned results. It is phonetic and acoustic information learned in pretraining, therefore has the most impact on the final performance of the model. Analysing an SSL ASR model after pretraining but before finetuning will help us understand how well SSL learns phonetic information. Applying this principal to multilingual models will show what impact multilingual pretraining data has on this accuracy.\thoughts{Not sure about the wording here, also don't talk about language specific information} \summary{This paragraph explains Pasad's work giving a broad overview of how w2v works.} 

\iffalse
LAST DRAFT
English et al. devise experiments study what kind of phonetic information is encoded in the internal representations of wav2vec 2.0 \cite{english2022domain}. They train linear layer probes for phoneme recognition on the outputs of each layer of the pretrained wav2vec 2.0. This model is pretrained on the LibriSpeech 960 dataset \cite{librispeech} but not finetuned on any data and the phonemes are taken from the TIMIT dataset \cite{timit}. English et al. find that wav2vec 2.0’s internal representations encode rich phonetic information, and that this information is not uniformly distributed across layers. This is similar to Pasad et al. \cite{pasad2021layer} who found that the middle transformer layers of wav2vec 2.0 contained the highest accuracy for phonetic tasks. \summary{This paragraph covers cormac's discover of pretrained being good for phoneme accuracy and the discover of layer 7 as the best layer}

English et al. split phonemes into domain-informed groups such as manner of articulation and place of articulation and then analyse probe performance across these groups. They find that classification accuracy for both manner and place peaks around intermediate transformer layers (around layer 7). Their confusion heatmaps and hierarchical clustering show that the model’s representations increasingly align with phonetic structure as depth increases, up until layer 7. \hl{Cormac papers}\thoughts{I don't know that we need to go into detail here about cormac's other papers maybe we can do that in the background section} \summary{This paragraph summarises cormac's phoneme classes}
\fi

English et al. devise experiments study the phonetic information encoded in the internal representations of wav2vec 2.0 \cite{english2022domain}. They train linear layer probes for phoneme recognition on the outputs of each layer of the pretrained wav2vec 2.0. This model is a monolingual pretrained model, using the LibriSpeech 960 dataset \cite{librispeech} but not finetuned on any data and the phonemes are taken from the TIMIT dataset \cite{timit}. They split phonemes into domain-informed groups such as manner of articulation and place of articulation, and then analyse probe performance across these groups. They find that classification accuracy for both manner and place is high and peaks around the intermediate transformer layers. Their confusion heatmaps and hierarchical clustering show that the model’s representations increasingly align with phonetic structure as depth increases. \thoughts{I don't know that we need to go into detail here about cormac's other papers maybe we can do that in the background section} \summary{This paragraph summarises cormac's phoneme classes}

\iffalse
From previous research, we know that the most effective methods for low finetuning error rates are monolingual pretraining or pretraining on languages related to the finetuning language. We also know that the SSL pretraining stage learns phonetic and acoustic features and that the intermediate layers of the SSL ASR model wav2vec 2.0 \cite{wav2vec} effectively learn phonemes from a range of groups, when pretrained on English data. Open source multilingual SSL ASR models are trained on datasets with an imbalance of hours of data in the languages included.\thoughts{badly worded} When multilingual SSL ASR models are finetuned the finetuning language relies on weights learned from the most abundant language in pretraining data. \summary{This paragraph is a summary of the info so far}\thoughts{may be easy to delete this paragraph}
\fi

\fi
\iffalse With this information so far, we can gauge that SSL pretraining alone is effective for learning phonetic information, but there are some gaps in our knowledge on what and how these models are learning.\fi For example, what happens when the ratios of unrelated languages in a multilingual SSL ASR model are not imbalanced? Do languages rely on a different language in pretraining data if English is not the most abundant? \thoughts{dunno if I should keep this in} Are individual phonemes learned as effectively when the pretraining data is multilingual compared to when it is monolingual? \summary{chapter questions} %\hl{DOI WE NEED ANYTHING AFTER THIS / I could end it here OR I CAN HAVE A SHORT EXPLANATION!! AND THEN COVER IT IN THE BACKGROUND}

%\hl{THEN WE NEED TO EXPLAIN WHAT IS MISSING HERE}

%\hl{NEED TO THINK ABOUT HOW TO WORD THIS AND WHY IT RELATED TO WHAT IM DOING}
\iffalse
To answer these questions, we first pretrain multiple versions of the wav2vec 2.0 \cite{wav2vec} model in the fairseq framework \cite{ott-etal-2019-fairseq} on two different datasets: LibriSpeech \cite{librispeech} and the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. In total, we pretrain 7 different models, two monolingual models, one trained only on English LibriSpeech data and one trained only on MLS Spanish. We then train five more models with different ratios of the two datasets, always adding up to a total of 900 hours. The English/Spanish ratios are: 800/100, 600/300, 450/450, 300/600 and 100/800. \thiswillnotshow{\naominote{Ed Note: edited these last 2 sentences to remove waffle as per Naomi's comment}} \summary{this paragraph covers our pretraining ratios and datasets}
\fi

In this chapter, we answer these question by pretraining multiple iterations of wav2vec 2.0 on varying rations of English and Spanish data. We collect finetuned error rates to show the impact of changing ratios of the two languages. We then probe each of our models in order to analyse how phoneme recognition at the SSL pretraining stage is impacted by different ratios of multilingual data. We analyse how well phonemes that overlap between the languages are recognised when the ratios are shifted. We also break down the data into articulatory groups and language-specific groupings of phonemes.

Overall, our findings show that language-specific information is encoded during pretraining. However, individual groups of phonemes can benefit when they are highly represented in pretraining data even when the languages they are sourced from are unrelated. Through testing we provide evidence that by designing your pretraining dataset by phoneme content instead of language content, phoneme accuracy can improve with SSL training. This is true for phoneme groups even when the testing language is unrelated to the pretraining languages. 

\begin{itemize}
    \item \textbf{The Story of the chapter to keep consistent}
    \item \textbf{Intro and Background:}
    \item Popular SSL ASR are pretrained on a single language OR pretrained on multiple languages but imbalanced towards one or two
    \item We have no examples on scale for models trained with balanced hours or controlled ratios
    \item We know that SSL ASR models perform best when the pretraining language is monolingual and matches or is closely related to the finetuning language
    \item How do SSL ASR models learn their pretraining data then?
    \item They learn acoustic and phonetic information in the intermediate layers \cite{pasad2021layer}
    \item They learn articulatory features in the early layers and fully realised phonemes in the intermediate layers \cite{english2022domain}
    \item They learn phonemes, therefore the information that affects the final performance is their performance on phoneme accuracy, not word accuracy
    \item Since we saw in the previous chapter that multilingual XLS-R is overly reliant on it's most abundant source of data English, we need to know how multilingual data affects phoneme accuracy as this will give insight into how models learn
    \item \textbf{Experiments to test these ideas}
    \item We pretrain multiple multilingual models with varying ratios of mutlilingual data
    \item as expected, monolingual models do best when finetuned to the same language
    \item However, it is notable that error decreases is high-resource match pretraining and decreases in low-resource match pretraining
    \item This effect with matched pretraining languages is recreated when finetuning is in FLEURS and is unseen data
    \item With related-language setting, we see a similar situation to the matched setting
    \item with unrelated language we also see the same effect as the matched monolingual, high resource and low resource pretraining
    \item when the finetuning language does not match either monolingual or any high-resource languages in pretraining, this effect is less clear.
    \item Here we see that the ratios of languages effect downstream performance and that the more high-resource a language is in pretraining the lower the final error rates are, inversely the error rates increase as the matching language becomes more low-resource
    \item Given that our SSL models are learning phonemes, how are multilingual pretraining effecting this.
    \item We introduce our probing experiments here first we want to see the difference between phonemes that exist in both languages vs phonemes that only exist in English and phonemes that only exist in Spanish.
    \item \hl{\textbf{For the background section we may need to mention the limit on datasets that MFA could handle and the pipeline I used for MFA and the OOV words}}
    \item Given the overlapping phonemes will be in high quantities no matter the ratio we would expect to see more significant changes in accuracy between the low to high resource changes for phonemes that exist in the matching language.
    \item We also test these groups when spoken in other languages, French and Polish.
    \item French is related to Spanish and has lots of phonetic overlap with English. While Polish is unrelated
    \item these experiments show us that even when the phonemes are spoken in another language the shared phonemes have higher accuracy 
    \item So we know at this stage that phoneme accuracy can be language-specific but that phonemes that occurr in all models have higher average accuracy even when spoken in an unrelated language
    \item \textbf{Do I need to mention amounts here? Probably I would wait until the next section}
    \item Next, We split our phonemes into multiple sub groups by POA and MOA like in \cite{english2022domain} and then also into two language-specific groups
    \item These tests allow us to separate phonemes into smaller groups and test how MOA and POA affect language-specific accuracy are phonemes 
    \item we can also test here how representation of phonemes (i.e. the total number) affect accuracy, some phonemes have less overall phonemes and other have high numbers on one language but not the other
    \item we also test all of these different angles on French and Polish as well to see if the same patterns hold true for different languages
    \item \textbf{Discussion and Conclusion}
\end{itemize}

\newpage
\textbf{From Perplexity}
A good way to keep this clear and consistent is to define a small set of terms once, then reuse them. For example:

\textbf{1) Pretraining / finetuning language match}
\begin{itemize}
    \item “Matched-language setting”: Pretraining and finetuning use the same language.
    \item “Mismatched-language setting”: Pretraining and finetuning use different languages.
    \item “Related-language setting”: Pretraining and finetuning languages are different but linguistically related (e.g. English → German).
    \item “Unrelated-language setting”: Pretraining and finetuning languages are different and not closely related (e.g. English → Polish).
\end{itemize}

\textbf{2) Amount of matching language in pretraining (for your mixed-ratio models)
You can define thresholds once, then refer back to them:}
\begin{itemize}
    \item “Monolingual pretraining”: 100% of the pretraining data is the finetuning language.
    \item “High-resource match pretraining”: The finetuning language is the majority of the pretraining data (e.g. >50%).
    \item “Low-resource match pretraining”: The finetuning language is present but only a small fraction of the pretraining data (e.g. <30%).
    \item “No-match pretraining”: The finetuning language is absent from the pretraining data.
\end{itemize}

Then you can write short, clean sentences like:
\begin{itemize}
    \item “We compare matched-language and mismatched-language settings.”
    \item “For English finetuning, we test monolingual, high-resource match, and low-resource match pretraining conditions.”
    \item “Spanish finetuning is evaluated under related-language, unrelated-language, and no-match pretraining.” 
\end{itemize}
If you like, paste a short methods paragraph and these labels can be woven in explicitly.

\newpage
\textbf{When we have multilingual pretraining dataset with many languages that have very different numbers of hours per language.}

A short, standard-sounding phrase here is:
\begin{itemize}
    \item “imbalanced multilingual pretraining data”
    \item or more explicitly: “heavily imbalanced multilingual pretraining distribution
\end{itemize}

\textbf{When the languages are unrelated but have an equal number of hours in pretraining how would I refer to this situation.}

A short label plus a brief definition will keep this tidy. For the case you describe, something like:
\begin{itemize}
    \item “Balanced-unrelated pretraining”: the pretraining data consists of two (or more) unrelated languages, each contributing an equal number of hours, and the finetuning language is one of these.
    \item If the finetuning language is not one of them, you could call it “balanced-unrelated, no‑match pretraining”.
\end{itemize}

\textbf{The ideas as a whole, like the concept of studying the different number of hours in languages in pretraining whether that is monolingual high or low resource or unrelated or related.}

A compact, umbrella phrase you can use is something like:
\begin{itemize}
    \item “Pretraining language data balance and match conditions”
\end{itemize}

After defining it once that, you can shorten it in running text to:
\begin{itemize}
    \item “different pretraining language conditions”
    \item “pretraining data balance conditions”
    \item “pretraining language balance and match”
\end{itemize}

Example uses:
\begin{itemize}
    \item “We analyse how different pretraining language conditions affect downstream ASR performance.”
    \item “These experiments compare pretraining data balance conditions (monolingual, high-resource match, low-resource match, balanced-unrelated) for English and Spanish.”
\end{itemize}









\newpage
To answer these questions, we first pretrain multiple versions of the wav2vec 2.0 \cite{wav2vec} model on two different datasets: LibriSpeech \cite{librispeech} and the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. In total, we pretrain seven different models, two are monolingual models in English and Spanish. We then pretrain five more models with different ratios of the two datasets, always adding up to a total of 900 hours. We then finetune each of the models to English and Spanish data. This tests whether ASR error rate is related to pretraining data and how introducing varying amounts of a new language affects the final error rate. Further finetuning is undertaken with multiple languages from multiple language groups, each language is a  subset of the FLEURs dataset \cite{FLEURS}. These languages include languages related to English or Spanish, Germanic or Romance languages, and unrelated Slavic languages are also tested. These experiments test the impact of related and unrelated pretraining data has on the final error rate. They also reveal the impact missing unrelated data in pretraining has on this affect.\summary{this paragraph covers our pretraining ratios and datasets and the FLEURs experiments}

\iffalse
We then finetune each of these models to smaller 100 hour subsets of LibriSpeech and MLS Spanish to test whether ASR error rate is related to pretraining data. We then finetune each of the models to English and Spanish subsets of the FLEURs dataset \cite{FLEURS} to confirm whether the same patterns are observed with unseen data. Further finetuning is undertaken with multiple languages from multiple language groups. These languages are either related to English or Spanish, Germanic or Romance languages, and unrelated Slavic languages are also tested. \summary{this paragraph covers FLEURs experiments} 
\fi
\iffalse
Next,\thoughts{this paragraph needs some rewriting with our new narative and further exploration of Cormac's stuff} we analyse how our pretrained SSL speech models learn phonetic information by training probes for phoneme recognition. Analysing phoneme accuracy at this stage allows us to understand how SSL ASR models learn multilingual speech. If phonemes shared across English and Spanish are well understood across all models, then SSL pretraining is language independent. If phonemes are learned independently of the languages they are spoken in, better pretraining data can be designed. However, if phonemes accuracy is lower for phonemes shared by both languages when the pretraining data contains multiple languages, then SSL pretraining is language dependant. This outcome may shed light on why monolingual pretraining is so effective. Phonemes spoken in English, Spanish, French and Polish will be tested in these experiments.\summary{This chapter explains the shared/ not shared work}
\fi
%\newpage  
Next,\thoughts{this paragraph needs some rewriting with our new narative and further exploration of Cormac's stuff} we analyse how our pretrained SSL speech models learn phonetic information by training probes for phoneme recognition. We analyse the relationship between phonemes that exist in both of our pretraining languages and phonemes that only exist in either English or Spanish as separate groups.\thoughts{may need rewording} Analysing phoneme accuracy at this stage allows us to shed light on why monolingual pretraining is so effective. \summary{This chapter explains the shared/ not shared work}


Finally,\thoughts{this will also need rewriting} we separate out phoneme data into multiple groups of phonemes and analyse their accuracy across each of our models. We choose nine\thoughts{do I usually use 9 or nine?} different groups of phonemes for analysis. These groups are a mixture of groups by manner of articulation and groups that are unique or more frequent in either English or Spanish.\thoughts{do I need to reference Cormac here to justify why it is 9 groups?} Analysing the phoneme accuracy for each language on each model will give us insight into the extent to which language impacts the learning of individual phonemes. Phonemes spoken in English, Spanish, French and Polish will be tested in all the probing experiments.

%Certain groups such as Tap \& Trills or Diphthongs occur much more frequently in one language or another. If accuracy is higher, than carefully selected multilingual pretraining data could improve downstream performance. However, if these groups are not improved by their inclusion in pretraining data then language dependant aspects of speech, such as coarticulation, may be more impactful in the pretraining stage of learning. \summary{Paragraph summarises phoneme groups work} Analysing the phoneme accuracy for each language on each model will give us insight into the extent to which language impacts the learning of individual phonemes.

\iffalse
Finally,\thoughts{this will also need rewriting} we separate out phoneme data into multiple groups of phonemes and analyse their accuracy across every model in English, Spanish, French and Polish. We choose nine\thoughts{do I usually use 9 or nine?} different groups of phonemes for analysis.\thoughts{do I need to reference Cormac here to justify why it is 9 groups?} Certain groups such as Tap \& Trills or Diphthongs occur much more frequently in one language or another. The phoneme accuracy for each model in these groups will show how much unrelated languages benefit from including more data from language dependant phonemes. If accuracy is higher, than carefully selected multilingual pretraining data could improve downstream performance. However, if these groups are not improved by their inclusion in pretraining data then language dependant aspects of speech, such as coarticulation, may be more impactful in the pretraining stage of learning. \summary{Paragraph summarises phoneme groups work}
\fi
\iffalse
Other phoneme groups in these experiments appear more frequently regardless of the language, groups such as Vowels and Approximants. Analysing the phoneme accuracy for each language on each model will give us insight into the extent to which language impacts the learning of individual phonemes. If a language that is related to the language  most represented in pretraining has high phoneme accuracy in a language independent group then language dependant features are being learned during pretraining. The inverse would be true if a preference is not shown to any language ratio.\thoughts{not sure if this sentence makes sense} If however phoneme accuracy shows little variance when the pretraining languages \hl{are changed}\thoughts{better wording} then the origin language of a phoneme is unimportant when testing unrelated language.\summary{Paragraph also summarises phoneme groups work}
\fi
\iffalse This analysis gives insight into how SSL speech models learn discrete individual phonemes.\naominote{This lays out what you do in this chapter but not clearly why. \textbf{Ed Note} Have I explained why I did it now or do I need to add in notes to say "probing tests phoneme accuracy of SSL models" something like that.} \fi

Overall, our findings in this chapter gives new insight into how SSL pretraining for ASR learns speech features when pretrained with multilingual or monolingual pretraining data. \iffalse why finetuned multilingual and monolingual performance is affect by the SSL stage. \fi We finish by suggesting approaches to data balance that could improve the performance in future work and create efficient and effective training methods going forward.\naominote{Nuance around the relationship between languages has not been brought out. Maybe the related work needs to come first to motivate. Regardless, need a clear statement on what you want to establish. More focussed}\summary{Conclusion, probably needs more of a rework}

\newpage
\iffalse
\textbf{Background Section}
\begin{itemize}
    \item SSL ASR, Multilingual SSL ASR and the problem of language imbalance in pretraining data
    \item we also need to cover monolingual or monofamily pretraining vs multilingual
    \item It is important to note that we have no examples of models that are completely balanced between unrelated languages OR models that are deliberately imbalanced towards a certain language
    \item An overview of the wav2vec architecture, training regime and loss function
    \item An overview of Pasad et al.'s work and how each layer works
    \item this must link back to how SSL works and why it must be acoustic and phonetic information learned in pretraining that affects the final WER or CER
    \item an overview of Cormac's work and how phoneme accuracy works for a pretrained only model
    \item An overview of my work and the previous chapter and how all of this relates
    \item Then finally a summary of the questions that are still open in this field and how this chapter answers them.
\end{itemize}
\fi
\section{Background and Related Work}\label{ed_sec:background}

In this section, we will investigate the background of how SSL ASR learns language. We will use previous research to understand which factors may have contributed to the findings in Chapter \ref{chapter4}. Then, finally, we identify the gaps in the current knowledge on multilingual SSL ASR. \thoughts{revist this I think}

\subsection{Multilingual and Cross-Lingual SSL ASR}
Wav2vec 2.0 is an SSL ASR model that has been pretrained on multiple languages, usually in a monolingual setting, and achieves low error when finetuned \cite{wav2vec}. XLS-R \cite{babu2021xlsr} is a large SSL ASR model with the same but expanded architecture as wav2vec 2.0 \cite{vaswani2017attention} and is pretrained on 128 different languages. XLS-R performs well across various languages \cite{FLEURS_XLSR_WHISPER}. However, it has been found that the model relies heavily on weights learned from the most high-resource language, a language with a high number of hours when compared to others, in pretraining \cite{storeyedbiasSSL}. The most high-resource language in pretraining is, in this case, English. English is often the most high-resource language in multilingual SSL ASR pretraining data due to its abundant data sources \cite{librispeech, librilight, MLS, wang-etal-2021-voxpopuli}. \summary{This paragraph introduces XLS-R and compares in to wav2vec 2.0, we also introduce English as the high-resource language and the concept of imbalanced data (although doesn't use the word balance)}

%----------------------------------------------------------------------------------------------------------

Due to the neural scaling principle, we know that the performance of Deep Neural Networks (DNNs)\thoughts{do I need to redefine CNNs and DNNs?} scales with the amount of data they are trained on. If a model is both high in parameter count and is trained on a large data source, it will perform better in more diverse settings than a small model trained on a small dataset \cite{hestness2017deep, kaplan2020scaling}. This scaling effect has also been shown to exist in ASR\cite{zhang2022bigssl, pu21_interspeech}. XLS-R was pretrained on 128 languages from hundreds of thousands of hours of data. The parameter count was therefore  was scaled for its large data multilingual task \cite{babu2021xlsr}. \summary{This paragraph cover neural scaling principle and is currently here to justify using a smaller model later.}\thoughts{maybe move neural scalability to the next section?}

%----------------------------------------------------------------------------------------------------------

However, studies have shown that monolingual pretraining and finetuning on a matching language, the same language as used in pretraining, can yield low error rates. This can be done with fewer parameters than large models pretrained with large multilingual datasets, often with lower finetuned error rates \cite{ashihara2023exploration, poncelet2021comparison}. Other studies have shown that pretraining on related-language datasets, data containing two or more related languages, yields similar low error results for related-language, languages related to those included in pretraining, finetuning \cite{zhang2023fast, gupta2021clsril}. \summary{this paragraph tells the benefits of monolingual and related-language pretraining}

There are examples in literature of monolingual pretraining, multilingual related-language pretraining and multilingual unrelated and imbalanced, the number of hours vary per language, pretraining data. However, to date, there has been no research studying the effect of data balance, how many hours of each language are included in pretraining, for unrelated multilingual SSL pretraining. Neither has there been an extensive study into the effect on accuracy, when an SSL ASR model is pretrained on unrelated but balanced languages, when compared to any other pretraining strategies. In order to investigate these open areas of study in SSL ASR, we will first need to understand how the wav2vec 2.0 family of models learn. \summary{this paragraph covers what research has yet to be done, where the holes in research are}

%\newpage
\subsection{Wav2vec 2.0 and Self-Supervised Speech Pretraining}\label{ed_subsec:background_wav2vec}
Wav2vec 2.0  is an SSL ASR model utilising a Convolutional Neural Network (CNN) \cite{krizhevsky2012imagenet} feature extractor and transformer block \cite{vaswani2017attention} encoder.\naominote{[Done] split into 2 sentences} Wav2vec 2.0 exists as 12 transformer block "Small" and a 24 block "Large" versions of the model \cite{wav2vec}. Wav2vec 2.0 \naominote{[Done] removed "for this study"} shares the same architecture and SSL pretraining loss function as XLS-R \cite{babu2021xlsr} but with fewer transformer blocks and smaller layers, trading lower pretraining time for lower overall accuracy. Wav2vec 2.0 and its successor models XLSR-53 \cite{XLSR-53} and XLS-R \cite{babu2021xlsr} use contrastive loss to pretrain their models on large unlabelled datasets \cite{wav2vec}.\info{Do I need a diagram of the wav2vec architecture in this section? I will have one in the Literature Review when that's done.} \thoughts{a lot of this (maybe all) paragraph can be removed}

Contrastive loss pits the output of the pretrained feature extractor against the output of the transformer encoder. This trains the transformer blocks to recreate the input to block 0 at the output of the final block, block 11.\naominote{[for this previous sentence] Clarity?} This loss objective teaches wav2vec 2.0 to perform an autoencoder \cite{hinton2006reducing} style task. Thereby, learning the features of the input before recreating them at the output. Contrastive loss allows wav2vec 2.0 to be trained on large amounts of unlabelled data. 

For deployment in downstream tasks, wav2vec 2.0 will then be finetuned to a smaller dataset.\naominote{[Done] I have split this into 2 sentences} For ASR, Connectionist Temporal Classification (CTC) Loss \cite{graves2006connectionist} applied to transcribed speech audio data is the most common approach. There is a dissonance between the pretraining loss objective and the final task, autoencoder to transcription. It is therefore important to understand what wav2vec 2.0 learns during the contrastive pretraining stage and what information it is learning\naominote{[for this final sentence] Rephrase}

\subsection{Layer-Wise Analysis of Pretrained Wav2vec 2.0}
Previous studies show significant difference between the tasks learned on each layer\cite{belinkov2017analyzing} for DeepSpeech2 \cite{amodei2016deep}, a previous supervised ASR model. They also show that different categories of phonemes are learned with higher accuracy at different layers. Choi et al. \cite{choi2024self} have found that SSL ASR speech representations exhibit more phonetic similarity than semantic similarity \cite{choi2024self}. \summary{this paragraph introduces layer analysis with deepspeech}

Pasad et al. \cite{pasad2021layer} take these ideas further by assessing the performance of each layer of wav2vec 2.0  in multiple domains: local acoustic features, phone identity, word identity and word meaning. Pasad et al. find that local acoustic features are best represented in the earliest layers of the pretrained but not finetuned wav2vec 2.0. For both the small and large wav2vec 2.0 models, shallow layers represent local acoustic features best. Accuracy then lowers significantly for the middle layers of the  models. They then see a rise in accuracy again in the later layers. Phonetic information is best recognised in the middle layers of the pretrained models, without an increase in the deep layers. \summary{this paragraph give an overview of the phonetic and acoustic findings from Pasad et al.}

Word identity and word meaning are both low in accuracy in the pretrained model when compared to the finetuned model. This suggests that the pretrained model does not learn semantic tasks well during the SSL pretraining stage. \naominote{[referencing whole section] Sentences too long and complex simple technical style needed.} Shallow layers that learn phonetic and acoustic tasks are also shown \hl{to change in accuracy the least}\naominote{This suggests change the least... [Ed: that's correct?]} when comparing the pretrained and finetuned models. Given that this information learned in pretraining affects finetuned ASR performance, we next need to understand how phonetic information is being learned during pretraining.\naominote{use commas![Done]}\summary{this paragraph give an overview of the semantic findings from Pasad et al. and links this to the next section}

It is worth noting that De\naominote{It is worth noting that... [Done]} Heer Kloots et al. \cite{deheerkloots25_interspeech} undertook work published in the Interspeech 2025 proceedings that pretrained a monolingual Dutch wav2vec 2.0 model for analysis. They pretrain a wav2vec 2.0 model using fairseq \cite{ott-etal-2019-fairseq} for 100k steps on Dutch speech data from various sources. They compare this model with the \textbf{fb-en} model trained by meta on LibriSpeech for 400k steps and \textbf{fb-voxp-100k} a multilingual model trained on 100k hours of voxpopuli data \cite{wang-etal-2021-voxpopuli}. They then train probes to analyse for phonetic analysis and lexical analysis and compare these findings against downstream finetuned ASR performance. They found that pretraining on Dutch data gave higher accuracy for  both phonetic and lexical tasks, to varying degrees. Their work was undertaken in parallel with the research in this chapter and published after our work was completed.\naominote{Rephrase. Say in this chapter to distinguish [Done]} It did not influence any decisions made during our research. In Section \ref{ed_sec:discussion} we consider our results in the context of their findings.\thoughts{Don't forget this bit it may need rewording when the discussion is written} \naominote{In Section X, we consider our results in the context of their findings.[Done]}\naominote{Don't just ignore.}

\subsection{Layer Probing for Phoneme Recognition}
English et al. devise experiments to study what kind of phonetic information is encoded in the internal representations of wav2vec 2.0 \cite{english2022domain}. They train linear layer probes  \cite{immer2021probing, schneiderleveraging} for phoneme recognition on the outputs of each layer of the pretrained wav2vec 2.0. This model is pretrained on the LibriSpeech 960 dataset \cite{librispeech} but not finetuned on any data, and the phonemes are taken from the TIMIT dataset \cite{timit}. English et al. find that wav2vec 2.0’s internal representations encode rich phonetic information, and that this information is not uniformly distributed across layers. This is similar to Pasad et al. \cite{pasad2021layer} who found that the middle transformer layers of wav2vec 2.0 contained the highest accuracy for phonetic tasks. \summary{This paragraph covers cormac's discover of pretrained being good for phoneme accuracy and the discover of layer 7 as the best layer}

English et al. split phonemes into domain-informed groups such as manner of articulation and place of articulation and then analyse probe performance across these groups. They find that classification accuracy for both manner and place peaks around intermediate transformer layers (around layer 7). Their confusion heatmaps and hierarchical clustering show that the model’s representations increasingly align with phonetic structure as depth increases, up until layer 7. \hl{Cormac papers}\thoughts{I don't know that we need to go into detail here about cormac's other papers maybe we can do that in the background section} \summary{This paragraph summarises cormac's phoneme classes }

\begin{itemize}
    \item For paper 1 \cite{english2024following}
    \item They probe wav2vec 2.0 to find out if learns sounds in the same way that linguistics organises phonological features
\end{itemize}

\hl{COME BACK TO THIS!! I need a paragraph that summarises cormac's other papers, possibly some other probing applications and then leads on nicely to the experimental design}

\newpage
\section{Experimental Design}\label{ed_sec:experimental_design}
In this section, we outline the datasets and configurations used in the experiments in this chapter. The training parameters used for both the SSL pretraining and finetuning for ASR stages of  training. We then outline the methods used to obtain phoneme labels for our data, as well as the hyperparameters and training configurations for training linear layer probes. Finally, we summarise the evaluation metrics used across all of these experiments.\thoughts{maybe come and lookat this again at the end}

\subsection{Model}
To answer the open questions in research around how SSL ASR models learn multilingual data, we must choose an appropriate model and appropriate datasets to experiment with. Wav2vec 2.0 was selected as the SSL ASR model for these experiments. It offers strong performance in monolingual settings \cite{wav2vec} and remains small enough that we can pretrain it multiple times under different data balance conditions. XLS‑R, used in chapter \ref{chapter4}, is another candidate for multilingual SSL pretraining. It is built on the same underlying wav2vec 2.0 architecture, but it increases the number of transformer blocks and the width of layers. This makes it much more expensive to train and analyse systematically. Using wav2vec 2.0 therefore lets us train a larger number of models with different pretraining language balances across multiple languages in the time available. 

\subsection{Pretraining Languages}
English and Spanish were selected as the pretraining languages, we did this for several reasons. They come from different language branches: English is a Germanic language and Spanish is a Romance language \cite{bauer2007linguistics}. They differ in grammar and phonology while still being Indo‑European languages. Both languages also have large amounts of clean, read speech data. It is available within single, consistent corpora, which reduces variation in recording conditions and transcription quality \cite{librispeech,MLS}. These corpora provide enough hours of speech to support both monolingual pretraining and mixed‑language conditions without needing to combine multiple datasets. Although English and Spanish share a number of phonemes, but also have several unique to each language \cite{phoneme_similarities_english_spanish}\thoughts{Could I reference MFA here or do I need a proper reference?}. This makes them a useful pair for studying how multilingual SSL models learn shared and language‑specific phonemes. In addition, both languages have existing pronunciation dictionaries for the Montreal Forced Aligner (MFA). This makes it straightforward to obtain time‑aligned phoneme labels for later analyses \cite{mfa}.

\subsection{Datasets and Pretraining}\label{ed_subsec:datsets}
For pretraining wav2vec 2.0, we use combinations of the two datasets outlined in Section \ref{DATASET_SECTION_IN_LR}\info{I haven't written this section yet, it will be in the Lit Review}: LibriSpeech \cite{librispeech} and the Spanish subset of MLS \cite{MLS}. First, we pretrain wav2vec 2.0, described in Section~\ref{ed_subsec:background_wav2vec}, on the full LibriSpeech dataset, which contains 960 hours of English speech; this monolingual English model is denoted 960 / 0. Next, we pretrain wav2vec 2.0 on the MLS Spanish subset, which contains 920 hours of Spanish speech; this monolingual Spanish model is denoted 0 / 920.

We also pretrain five mixed-language models with different balances of LibriSpeech (English) and MLS Spanish. Using the notation (English / Spanish), the total pretraining hours are fixed to 900, with the following splits: 800 / 100, 600 / 300, 450 / 450, 300 / 600 and 100 / 800. For each split, we select utterances by iterating through a fixed list, so that the same hours of speech are reused across conditions (for example, the first 100 hours from each dataset are identical wherever they appear). All pretraining sets are balanced by gender, with 50\% male and 50\% female speech.

Next, in Section \ref{ed_sec:experimental_results}, we finetune each pretrained model on 100 hours of LibriSpeech and 100 hours of MLS Spanish separately. These 100-hour subsets are constructed in the same way as the pretraining splits, by selecting from a fixed list of utterances and enforcing a 50\% / 50\% male / female balance. We then finetune each model on several languages from the FLEURS dataset. As outlined in Section~\ref{DATASET_SECTION_IN_LR}\info{I haven't written this section yet, it will be in the Lit Review}, each FLEURS language provides 10 hours of speech, and we use these languages only for finetuning and evaluation. Table~\ref{tab:datasets_overview} summarises the datasets and how we use them.

\begin{table}[b]
    \centering
    \begin{tabular}{l l l l p{4cm}}
        \hline
        Corpus & Lang. & Hours & Setting & Use \\
        \hline
        LibriSpeech & En & 960 & Pretrain & Monolingual model  \\
        MLS & Es & 920 & Pretrain & Monolingual model \\
        LibriSpeech + MLS & En+Es & 900 & Pretrain & Mixed models  \\
        LibriSpeech & En & 100 & Finetune & ASR evaluation \\
        MLS & Es & 100 & Finetune & ASR evaluation \\
        FLEURS & Multi & 10 / lang & Finetune & Additional languages \\
        LibriSpeech & En & 10 & Probe & Layerwise probes \\
        MLS & Es/Fr/Pl & 10 / lang & Probe & Layerwise probes \\
        \hline
    \end{tabular}
    \caption{Overview of corpora, languages, and their roles in pretraining, finetuning, and probing.}
    \label{tab:datasets_overview}
\end{table}

Finally, we use 10-hour subsets of LibriSpeech and of the Spanish, French, and Polish subsets of MLS to train our layerwise probes, as described in Section \ref{ed_seubsec:exp_des_probing}. These 10-hour subsets are constructed in the same way as the 100-hour subsets for LibriSpeech and MLS Spanish, again using a fixed list of utterances and a 50\% / 50\% gender balance. The selected speech is then segmented into windows, and wav2vec 2.0 features are extracted from each pretrained model for use in training the linear probes. 

Our aim in training multiple wav2vec 2.0 models is to isolate the effect of pretraining language balance and language match and mismatch conditions on downstream performance. By varying the proportion of English and Spanish in pretraining while keeping the total number of hours fixed, and then finetuning each model on the same 100-hour and 10-hour subsets and additional languages, we can directly compare how different pretraining conditions affect ASR error rates and, later, the structure of learned phonetic representations. This requires a family of closely matched models that differ primarily in their pretraining language composition rather than in architecture or optimisation settings.




\subsection{Pretraining Hyperparameters} \label{ed_pretraining_configuration}
With our data selected, we next pretrain our wav2vec 2.0 models from random initialisation. We pretrain wav2vec 2.0 \cite{wav2vec} on the 960‑hour LibriSpeech training set, the 920‑hour Spanish subset of MLS and on each of the five 900‑hour English/Spanish mixed pretraining conditions described in Section~\ref{ed_subsec:datsets}. We follow the unsupervised pretraining recipe provided in the wav2vec 2.0 repository, implemented in the fairseq framework \cite{ott-etal-2019-fairseq}.\footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/README.md}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/README.md}}

The original wav2vec 2.0 model was trained on 64 V100 GPUs with 16Gb of VRAM each and used a \texttt{max\_tokens} value of 1,400k \cite{wav2vec}. In fairseq, \texttt{max\_tokens} controls the total number of audio frames per batch. Fairseq also includes an \texttt{update\_freq} parameter, which implements gradient accumulation: gradients from several smaller batches are summed before a single optimiser update is applied.\thoughts{Do I need a citation for GA? There isn't really a simple one I could use} Accumulating gradients over multiple forward passes before backropagation has a similar effect to increasing the batch size while using more multiple GPUs, because the weight update is computed from more data before each step.

For the experiments in this chapter, we used a single Nvidia RTX A6000 GPU with 48Gb of VRAM. We experimented with increasing \texttt{max\_tokens} and decreasing \texttt{update\_freq} when pretraining on the 960‑hour LibriSpeech training set. With our final settings, the effective batch size was 6.4 times larger than in the original wav2vec 2.0 training, and we used an \texttt{update\_freq} of 10 on a single GPU. This combination simulated training with gradients accumulated over the equivalent of 64 smaller GPUs and achieved a similar final pretraining loss at 80k updates\thoughts{Do I need exact numbers? It's a bit irrelevant without the full diagrams} to using the original script with a smaller batch and \texttt{update\_freq} of 64, while reaching 80k updates in substantially fewer updates. We also finetuned both setups on 100 hours of LibriSpeech every 10k updates; the resulting WERs \hl{Add in exact number later}\thoughts{add in numbers for WER} were within an acceptable margin for our purposes.

By default, the fairseq pretraining script runs for 400k updates and with our setup, training took approximately one day per 12k updates. To reduce total training time, we looked for a number of updates at which additional pretraining would give diminishing returns while still achieving an acceptably low WER after finetuning. We monitored performance while running multiple pretraining experiments for several days and finetuned checkpoints on a 100 hour subset of LibriSpeech every 10k updates. The original wav2vec 2.0 model attains a WER of 3.4\% on the LibriSpeech 100 hour clean set after 400k updates \cite{wav2vec}. In our setup, we found that pretraining on the full 960‑hour LibriSpeech dataset for 50k updates was sufficient to reach a WER below 10\% on this evaluation set. Beyond 50k updates, we observed diminishing returns in WER improvements. For our purposes, a WER of 9.6\% represented an acceptable trade off between accuracy and training time, especially when pretraining multiple separate models. The final hyperparameters we changed were: \texttt{max\_tokens} = 8,960k, \texttt{distributed\_world\_size} = 1, \texttt{update\_freq} = 10 and \texttt{max\_update} = 50k; all other settings followed the original wav2vec 2.0 configuration \cite{wav2vec}.




%\newpage
\subsection{Finetuning and Evaluation Languages}\label{ed_subsec:finetuning}
When finetuning our model to 100 hours of data  we follow the Fairseq finetuning scripts for LibriSpeech, regardless of the dataset we are finetuning on. We modify them to run on a single GPU, with all other parameters unchanged \footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_100h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_100h.yaml}}. We double the max\_tokens from 3200k to 6400k and double the update\_freq from -4 to -2, while setting the world\_size, or number of GPUs in use, from 2 to 1. The script for 100 hours runs for 80k updates, this and all other parameters are left unchanged. For finetuning to 10 hour datasets, we use the 10 hour finetuning script for LibriSpeech \footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_10h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_10h.yaml}}. We make the same changes to max\_tokens, update\_freq and world\_size. All other parameters were left unchanged.

%\subsection{Finetuning}
When finetuning our models on 100 hours of data, we follow the Fairseq LibriSpeech finetuning configuration and apply it to all datasets. We modify the configuration to run on a single GPU, while leaving all other parameters unchanged.\footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_100h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_100h.yaml}} Concretely, we double \texttt{max\_tokens} from 3{,}200k to 6{,}400k and adjust the gradient accumulation setting so that, on a single GPU, the effective batch size matches the original 2-GPU setup, while setting \texttt{distributed\_world\_size} (number of GPUs) from 2 to 1. The 100-hour finetuning script runs for 80k updates, and this setting, along with all other hyperparameters, is left unchanged.

For finetuning on 10-hour datasets, we use the corresponding LibriSpeech 10-hour finetuning configuration.\footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_10h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_10h.yaml}} We make the same modifications to \texttt{max\_tokens}, the gradient accumulation setting, and \texttt{distributed\_world\_size} as above, and leave all other parameters unchanged.
\iffalse
Once we have the results of our finetuning experiments, we will understand how pretraining language balance affects downstream finetuning results. We expect, based on previous findings \cite{ciations from beginning of background}, for matched language pretraining to lower error in finetuning and mismatched pretraining settings to raise error. Similarly, we may expect high-resource matched settings to have low-error compared to mismatched settings but higher than matched settings\thoughts{Have I used the phrase high-resource matched-mismatched yet? }. By this logic, high-resource mismatched and low-resource matched settings may have low relative error. Error may rise or lower depending on the balance of matched and mismatched languages in pretraining. 

Once we have our results, we need to analyse how pretraining learns speech. If our predictions are true, then pretraining language balance in a mixed language setting will have a profound impact on finetuning results. However, as outlined in Section \ref{ed_sec:background} SSL pretraining does not learn semantic tasks and only learns phonetic and acoustic information \cite{pasad2021layer}. Due to these findings, we cannot evaluate pretraining learning solely on finetuned semantic error rates. Due to previous research, we know that a pretrained SSL ASR model can achieve high phoneme accuracy when probed \cite{english2022domain}. We can use probing for phoneme accuracy to measure which groups of phonemes are learned best when pretraining data is mismatched and how introducing mismatched languages affects phoneme accuracy. 
\fi
\iffalse
Once we have the results of our finetuning experiments, we can assess how pretraining language balance influences downstream ASR performance. Based on previous work on multilingual SSL ASR \cite{ciations from beginning of background}, we expect matched-language pretraining to give lower error rates than fully mismatched pretraining. We also expect high-resource matched pretraining to outperform low-resource matched and no-match conditions. For mixed pretraining, we expect error rates to be lowest in high‑resource matched settings, higher in low‑resource matched settings, and highest in mismatched and no‑match settings. We also expect lower error in mixed settings where the non‑matched data comes from related languages and higher error where it comes from less‑related languages.
\fi

Once we have the results of our finetuning experiments, we can assess how pretraining language balance influences downstream ASR performance. Based on previous work on multilingual SSL ASR \cite{ciations from beginning of background}, we expect matched-language pretraining to give lower error rates than fully mismatched pretraining. We also expect high-resource matched pretraining to outperform low-resource matched and no-match conditions, with mixed pretraining conditions falling between these extremes.

Pasad et al.\ show that SSL pretraining does not learn semantic or lexical tasks and instead learns acoustic and phonetic structure in the lower and middle layers \cite{pasad2021layer}. If our predictions about the impact of pretraining language balance on finetuned ASR are correct, then these acoustic and phonetic representations have a strong effect on downstream performance. We therefore need to analyse the pretrained models directly, rather than relying only on word-level error after finetuning. Prior work shows that pretrained SSL ASR models can support accurate phoneme recognition when probed with simple classifiers \cite{english2022domain}. We use phoneme-level probing to measure which phoneme groups are learned best under different pretraining language balances. We also use it to test how matched, low-resource, and mismatched pretraining conditions affect the quality of the learned phonetic representations.


\subsection{Forced Alignment and Phoneme Accuracy}
\iffalse
In order to evaluate phoneme accuracy in our pretrained models, we train linear layer probes to recognise phonemes at a frame level. To obtain frame level phoneme labels we use the Montreal Forced Aligner (MFA) \cite{mcauliffe2017montreal} and apply it to 10 hour subsets of LibriSpeech, MLS Spanish and MLS French that were compiled using the gender balanced approach as in Section \ref{ed_seubsec:exp_des_probing}. The dictionaries used for phoneme alignment were english\_us\_mfa, spanish\_spain\_mfa, French\_mfa and polish\_mfa. For alignment each dictionary has a dictionary of words and the phoneme's used to pronounce them, each 10 hour training subset had some words that were Out Of Vocabulary (OOV). For the LibriSpeech 10 hour subset there were 0.75\% OOV words, for the MLS Spanish 10 hour subset 7.7\% words were OOV, MLS French 10 hour subset 7\% of the words were OOV and for MLS Polish 10 hour subset 4\% of words were OOV. To include OOV words in the dictionary we used the Grapheme to Phoneme (G2P)\footnote{\href{https://mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}{mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}} models to create pronunciations dictionaries and added them to a custom dictionary with the all the words from the original dictionaries that was then used to phonetically transcribe each of the datasets. Phoneme transcriptions were done for the whole 960 hour Librispeech training set and the 920 hour MLS Spanish training set in order to analyse the phonetic makeup of the pretraining data. OOV words were also transcribed using G2P models for the LibriSpeech 960 hour training set 0.88\% of words were OOV and for the 920 hour MLS Spanish training set 8.63\% were OOV. 

\hl{this next part was removed from ealier subsection I could use it here}

 but the transcripts are not text-normalised and contain symbols and numbers. \hl{This lack of normalisation makes the dataset unsuitable for our later forced-alignment and probing experiments using the Montreal Forced Aligner (MFA), so those analyses rely only on the text-normalised LibriSpeech} \cite{librispeech} \hl{and Multilingual LibriSpeech corpora.}\thoughts{Do I need this stuff about text normalisation?}
\fi
\subsection{Forced Alignment and Phoneme Labels}

To evaluate phoneme accuracy in our pretrained models, we require frame-level phoneme labels aligned to the speech signal. We obtain these labels using the Montreal Forced Aligner (MFA) \cite{mcauliffe2017montreal}. As outlined in Section~\ref{DATASET_SECTION_IN_LR}, the FLEURS transcripts are not text-normalised and contain symbols and numbers, which makes them unsuitable for reliable forced alignment. We therefore restrict our phoneme-level analyses to LibriSpeech \cite{librispeech} and the Multilingual LibriSpeech (MLS) corpora, which provide text-normalised transcripts.

For probing, we apply MFA to 10-hour subsets of LibriSpeech, MLS Spanish, MLS French, and MLS Polish, compiled using the same gender-balanced procedure as in Section~\ref{ed_seubsec:exp_des_probing}. We use the MFA pronunciation dictionaries \texttt{english\_us\_mfa}, \texttt{spanish\_spain\_mfa}, \texttt{french\_mfa}, and \texttt{polish\_mfa}, each of which maps words to their phoneme sequences. Each 10-hour subset contains some out-of-vocabulary (OOV) words not covered by the default dictionaries: 0.75\% of words are OOV for LibriSpeech, 7.7\% for MLS Spanish, 7.0\% for MLS French, and 4.0\% for MLS Polish.

To handle OOV items, we use MFA Grapheme-to-Phoneme (G2P) models\footnote{\href{https://mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}{mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}} to generate pronunciations and merge these with the original dictionaries to form custom pronunciation lexicons. These custom lexicons are then used by MFA to force-align each 10-hour subset and produce frame-level phoneme labels for probe training and evaluation.

We also run MFA on the full 960-hour LibriSpeech training set and the 920-hour MLS Spanish training set in order to analyse the phonetic composition of the pretraining data. In these full training sets, 0.88\% of LibriSpeech words and 8.63\% of MLS Spanish words are OOV, which is close to the OOV rates observed in the corresponding 10-hour subsets. This indicates that our 10-hour subsets are representative of the full corpora with respect to vocabulary coverage and alignment conditions.

%\newpage
\subsection{Layer Probing}\label{ed_seubsec:exp_des_probing}
To train our layer-wise probes for phoneme accuracy, we used the same parameters as English et al. \cite{english2022domain}. We use a two layer probe taking as input the hidden states of each layer of the wav2vec 2.0 encoder and as output the phonemes included in the dataset. The input for all probes is the same size as the output to each transformer block in the wav2vec 2.0 encoder, 768 inputs.

%%%%%%%%%%%%%%%

We follow English et al. \cite{english2022domain} and train layer-wise probes to estimate phoneme accuracy from wav2vec~2.0 representations. Each probe is a two-layer feed-forward network that takes as input the hidden states from a single transformer layer of the wav2vec~2.0 encoder. The input size to each probe is 768, the dimension of each transformer block, and predicts a phoneme label for each frame. During probe training the wav2vec~2.0 encoder is kept frozen, and probes are trained with the Adam optimiser, a learning rate of \(1e-3\) and using cross-entropy loss.

%%%%%%%%%%

To train the probes, we separate each utterance into windowed audio and extract the features with the model's feature extractor. The extracted features are then collated into a dataloader alongside the phoneme in that frame. Each set of extracted features is iteratively fed into the encoder of each model, and the output of each layer is extracted. The outputs of each of the 12 wav2v2ec 2.0 layers are then used to train 12 separate probes, one for each layer. Cross entropy loss is then used to match the phoneme at that frame with the output of each probe. 

For testing, we will use both a small 10 hour subset of LibriSpeech and a larger 100 hour subset of LibriSpeech. We found that both datasets started to overfit after 5 epochs of training. We also found that when the probes were trained on the 100 hour subset of LibriSpeech, that the average phoneme accuracy was higher but the training time was longer. When training the probes on the 10 hour subset the training time was drastically reduced and while the overall accuracy was lower, the relationship between the accuracy of each category of phoneme was unchanged. Due to these findings we used 10 hours subsets of each of the larger datasets as a compromise for time and accuracy for training our probes.

\begin{table}[h]
\centering
\begin{tabular}{r|cc|cc}
\toprule
\multicolumn{1}{r}{\textbf{Pretraining Hours:}} & \multicolumn{2}{c}{\textbf{English}} & \multicolumn{2}{c}{\textbf{Spanish}} \\
\multicolumn{1}{r}{\textbf{LibriSpeech /}} & & & & \\
\multicolumn{1}{r}{\textbf{MLS Spanish}} &  \textbf{Layer } & \textbf{Acc } & \textbf{Layer } & \textbf{Acc } \\
\midrule
960 / 0 & Layer 7 & 44.7\% & Layer 5 & 57.5\% \\
800 / 100 & Layer 6 & 44.6\% & Layer 7 & 61.0\% \\
600 / 300 & Layer 7 & 43.3\% & Layer 5 & 61.6\% \\
450 / 450 & Layer 7 & 42.8\% & Layer 7 & 62.2\% \\
300 / 600 & Layer 7 & 40.9\% & Layer 5 & 60.0\% \\
100 / 800 & Layer 6 & 39.3\% & Layer 7 & 62.4\% \\
0 / 920 & Layer 4 & 34.1\% & Layer 5 & 62.2\% \\
\bottomrule
\end{tabular}
\caption{Table of the layers with the highest average accuracy after probing and the phoneme accuracy found on this layer for both English and Spanish. The wav2vec 2.0 encoder has 12 transformer layers with the first being layer 0 and the 12th being layer 11.}
\label{ed_tab:best_layer_average_accuracy}
\end{table}

Table \ref{ed_tab:best_layer_average_accuracy} shows the layers with the highest mean average accuracy for each model when tested on the two pretraining languages, English and Spanish. We see in Table \ref{ed_tab:best_layer_average_accuracy} that the most common layer with the highest accuracy for the English dataset is layer 7 with the second most common being layer 6. For the Spanish dataset, the most common highest accuracy layer is layer 5 and the second most common is layer 7. These results are consistent with previous research that suggests that layers in this range have the highest accuracy for phoneme recognition \cite{english2022domain, pasad2021layer}. The probing results in the graphs in Section \ref{ed_sec:shared_vs_not_shared} and Section \ref{ed_sec:phonemes_classes} chose a single layer to take accuracy results from, in this case we chose layer 7 for all languages tested as it has consistently high accuracy for both datasets and is consistent with English et al. \cite{english2022domain}. 

\subsection{Metrics and Evaluation Criteria}
Here we list out all for the metrics used to track loss and evaluation during and after pretraining, finetuning and training our probes. For SSL pretraining we monitor progress on the model through contrastive loss and every 10k updates we finetune to evaluate the Word Error Rate (WER) to keep consistent with the fairseq pretraining tutorial evaluation metrics. For all finetuning experiments we train using Connectionist Temporal Classification (CTC) Loss and evaluate after training using Character Error Rate (CER) to keep consistent with the FLEURs finetuning experiments in Chapter \ref{chapter4}. For training our layer-wise probes we train each probe using Cross-Entropy Loss and then evaluate the probes after training using phoneme accuracy to keep consistent with previous work \cite{english2022domain}.

\begin{table}[h]
\centering
\begin{tabular}{l|l|l|}
\cline{2-3}
                                  & \multicolumn{1}{c|}{Loss During Training} & \multicolumn{1}{c|}{Evaluation After Training} \\ \hline
\multicolumn{1}{|l|}{Pretraining} & Contrastive Loss                          & WER                                            \\ \hline
\multicolumn{1}{|l|}{Finetuning}  & CTC Loss                                  & CER                                            \\ \hline
\multicolumn{1}{|l|}{Probing}     & Cross Entropy Loss                        & Accuracy                                       \\ \hline
\end{tabular}
\caption{Table of Loss functions for each stage of training and evaluation metrics used after training}
\end{table}

\newpage
\section{Experimental Results}\label{ed_sec:experimental_results}

In this section we cover the results of our experiments as described in Section \ref{ed_sec:experimental_design}. First in Section \ref{ed_subsec:same_data_finetuning} we analyse the results of our finetuning experiments to English and Spanish data from data included in pretraining. Next in Section \ref{ed_subsec:gain_across_languages} we test whether English and Spanish data have the same behaviours when they are finetuned from unseen data that was not included in pretraining. We then further analyse these patterns by including data from multiple languages from several different language groups: Germanic, Romance and Slavic. 

In Section \ref{ed_sec:shared_vs_not_shared} we explore how Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models learn speech representations by probing the encoder layer for phoneme accuracy across multiple languages. We test the accuracy of phonemes from several groups: phonemes that exist in both Spanish and English, phonemes unique to English and phonemes unique to Spanish. We test these different groups in both English and Spanish for every one of our pretrained models. We then measure phoneme accuracy for French and Polish and compare phonemes that are shared by both English and Spanish to phonemes unique to French and phonemes unique to Polish. Finally in Section \ref{ed_sec:phonemes_classes} we expand our analysis of English, Spanish, French and Polish phonemes to measuring accuracy across different phoneme groups. The groups include: vowels, diphthongs, approximants, nasals, tap \& trills, stops, affricates, fricatives and closures. 

\subsection{Effect of Pretraining Composition on Finetuning}\label{ed_subsec:same_data_finetuning}

In this section we will evaluate the applied character level recognition when each of the models outlined in Section \ref{ed_pretraining_configuration} are finetuned to data from the datasets that they were pretrained on LibriSpeech and the Spanish subset of Multilingual Librispeech. The original wav2vec 2.0 model trained by meta is evaluated by first pretraining on the entire 960 hour LibriSpeech dataset and then finetuning to various smaller subsets of the same dataset. Figure \ref{ed_fig:fine_tuning_100hours} shows the results of finetuning each of our pretrained models to 100 hours of the either LibriSpeech or MLS Spanish data, both sets of data had already been seen in the pretraining data.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_engspamls100h.pdf}
    \caption{CER when models pretrained with varying ratios of English and Spanish data are finetuned to 100 hours of English or 100 hours of Spanish data }
    \label{ed_fig:fine_tuning_100hours}
\end{figure}%\label{fine_tuning_100hours}

Figure \ref{ed_fig:fine_tuning_100hours} shows the Character Error Rate (CER) results obtained after finetuning each of our pretrained models on 100 hours of labelled data from the same datasets used during pretraining—LibriSpeech (English) and MLS Spanish. When finetuning to English data from the 100 hour LibriSpeech subset we see that the lowest CER is on the model trained only on English LibriSpeech data and the highest CER is on the model is found on the model trained only on Spanish MLS data. When finetuning to the Spanish MLS 100 hour subset the lowest error is found on the model trained on 800 hours of MLS Spanish and 100 hours of English LibriSpeech, while the highest error is found on the model trained on 800 hours of English LibriSpeech and 100 hours of Spanish MLS data. The model trained on only English data also has some of the highest error when finetuning to Spanish and inversely the model trained only on Spanish has some of the lowest.

These results show that while SSL ASR pretraining only learns broad phonetic representations it still has an impact on how the model learns semantic information. However, since the models have been finetuned to subsets of the same data they were trained on we cannot at this stage determine if the pretraining has overfitted to the pretraining data or whether there is linguistic information specific to each language that is being learned during pretraining.

\subsection{Multilingual and Cross Lingual Transfer Gains}\label{ed_subsec:gain_across_languages}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_engspafleurs.pdf}
    \caption{CER when each pretrained model is finetuned to unseen English and Spanish data, the finetuning data consists of 10 hours each from the FLEURs dataset}
    \label{ed_fig:multilingual_linegraph}
\end{figure}
To test the impact the SSL ASR pretraining has on downstream language learning we design a new experiment. In this section we fine-tune each of our models to data from multiple different languages from several different language groups in order to observe the effect that learning broad speech representations from one or multiple languages affects the finetuned error rates of unseen data and unseen languages in pretraining. Figure \ref{ed_fig:multilingual_linegraph} shows the results of finetuning our models to 10 hour English and Spanish subsets for the FLEURs dataset, this data has not been seen in pretraining but are in the same languages the models was pretrained on. We see the same pattern replicated here as in Figure \ref{ed_fig:fine_tuning_100hours} with the CER when finetuning to English being lowest on the models with the most English data and the CER when finetuning to Spanish being lowest on the models with the most Spanish data. This shows that the phonetic information learned in pretraining is language-specific and affects the finetuning accuracy even when the finetuning data was unseen in pretraining. It should also be noted that the Spanish CER is lower at every point than the English CER, this is consistent with the results found with FLEURs in Section \ref{chapter4}.



Having observed the same pattern when finetuning our SSL models on both seen and unseen English and Spanish data, next we can test whether the pretraining data affects the learning of different language families. Figure \ref{ed_fig:multilingual_barchart} shows the difference in CER for multiple languages from the FLEURs dataset. Each of the 2 monolingual are finetuned to the same language and the difference in CER is taken. A positive difference reflects higher CER on the English monolingual model and a negative difference reflects higher CER on the Spanish monolingual model.

\begin{equation}\label{ed_equ:CER_differrence_mutlilingual}
\centering
    CER\_on\_monolingual\_English - CER\_on\_monolingual\_Spanish
\end{equation}

All of the error rates on every model for each FLEURs language can be found on Figure \ref{app_fig:multilingual_linegraph} in Appendix \ref{app_sec:chapter_5_appendix}.

\newpage
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/barchart_abs_diff.pdf}
    \caption{Absolute Character Error Rate (CER) change between the monolingual English model and the monolingual Spanish model when finetuned on multiple downstream languages}
    \label{ed_fig:multilingual_barchart}
\end{figure}
In Figure \ref{ed_fig:multilingual_barchart} we see that the most positive change in CER occurs in English whilst the most negative change in CER occurs in Spanish. These show that the monolingual models react most positively to the languages that they were originally pretrained in. The next most positive languages we tested were Danish and German, these are both Germanic languages the same language family as English. The languages that are most negative are Asturian, Catalan and Italian which are all Romance languages which is the same language family as Spanish. Of the three remaining languages Russian shows the smallest negative change in CER while French gives the lowest positive change and Polish shows lower CER on the English monolingual model but a smaller difference between the two monolingual models than all of the Germanic languages tested. The Slavic languages do not show a clear trend towards either language in the pretraining, with both positive and negative change in CERs. French acts as an outlier among the Romance languages, showing higher error on the English monolingual model. These results seem to suggest that the languages that our SSL ASR models are trained on affect the downstream error of languages within the family of original pretraining language. To investigate these results further we must measure exactly how SSL ASR models learn phonetic information and how language balance in pretraining may impact this. 

\newpage
\subsection{Shared vs. Unique Phoneme Accuracy} \label{ed_sec:shared_vs_not_shared}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/phoneme_shared_notshared_line_graph_updated.pdf}
    \caption{Phoneme accuracy taken at layer 7 of each of our models for English and Spanish data. The phonemes are split into groups of phoneme that are either shared between Spanish and English languages or phonemes that are unique to either English or unique to Spanish.}
    \label{ed_fig:phonemes_shared_not_shared}
\end{figure}

Figure \ref{ed_fig:phonemes_shared_not_shared} shows the phoneme accuracy gathered from our probing experiments outlined in Section \ref{ed_seubsec:exp_des_probing} full lists of the phonemes measured and their accuracy for each language can be found in Appendix \ref{app_sec:chapter_5_appendix}. We separate our phonemes into four different groups of phonemes and take the average accuracy on each model for each group. The phoneme groups are: English-Only, English-Shared, Spanish-Only and Spanish-Shared. English-Only contains phonemes that are only found in the English dataset and Spanish-Only contains phonemes that are only found in the Spanish dataset. The phonemes in the English-Only and Spanish-Only groups are then phonemes that only exist in each language and are only tested on the relevant language, i.e. English-Only are only tested on English data. The values in Figure \ref{ed_fig:phonemes_shared_not_shared} are the mean average of all the phonemes in each group when tested on each of the models outlined in Section \ref{ed_subsec:datsets}. %A list of the phonemes found in the English-Only and Spanish-Only groups can be found in Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phonemes_only_groups}.

In Figure \ref{ed_fig:phonemes_shared_not_shared} we first look at the English-Only group of phonemes, here we see a difference of 21\% between the phoneme accuracy recorded between the monolingual English model (960 / 0) and the Spanish monolingual model (0 / 920). While the accuracy does not consistently decrease from left to right of Figure \ref{ed_fig:phonemes_shared_not_shared} as the amount of English data in pretraining, we do see that the English-Only phonemes have the highest accuracy on the models with the most English data included in pretraining. For the Spanish-Only phoneme group we see the inverse trend with a difference of -14\% between the English monolingual and Spanish monolingual models, showing higher accuracy for Spanish-Only phonemes on the model only trained on Spanish pretraining data. Similarly, we see an increasing of accuracy from right to left of Figure \ref{ed_fig:phonemes_shared_not_shared} as more Spanish data is introduced in pretraining. 

For the phoneme group English-Shared in Figure \ref{ed_fig:phonemes_shared_not_shared} there is an absolute difference of 19.24\% between the accuracy on the monolingual English model and the monolingual Spanish model. This group also exhibits the same behaviour as English-Only with accuracy increasing from right to left of the graph as more English data is introduced to the pretraining. Spanish-Only also exhibits the same pattern, having a change in accuracy of -5.32\% between the English and Spanish monolingual models and an increase in accuracy with models with more Spanish data in pretraining. This shows that even when phonemes exist in both languages they are not learned equally well and the models are learning more than discrete phoneme representations, such as coarticulations between phonemes that might only exist in one language. 

Finally, we can note that in Figure \ref{ed_fig:phonemes_shared_not_shared} that the average phoneme accuracy for the English-Shared group is on every model higher than the average accuracy for the English-Only phoneme group. It is also true that the phoneme accuracy is higher on every model for the Spanish-Shared phoneme group when compared to the Spanish-Only phoneme group. This suggests that while shared phonemes are better recognised when they are spoken in a single language, phonemes that exist in both pretraining datasets still benefit from more examples in training.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/barchart_shared_not_shared_fre_pol.pdf}
    \caption{Average phoneme for French and Polish when tested for phonemes that exist in either English or French vs Phonemes that are unique to French or Polish}
    \label{ed_fig:phonemes_shared_french_polish}
\end{figure}

Figure \ref{ed_fig:phonemes_shared_french_polish} shows the average phoneme accuracy for several different groups: phonemes that exist in both the French language and English or Spanish, Phonemes that exist in French but not in English or Spanish, then Phonemes that exist in Polish and also in English or Spanish and finally Phonemes that only exist in Polish but not in English or Spanish. Both languages were taken from the Multilingual LibriSpeech (MLS) dataset \cite{MLS} and phonemes found by using the Montreal Forced Aligner (MFA) \cite{mcauliffe2017montreal}. 

In all models in Figure \ref{ed_fig:phonemes_shared_french_polish} the French language data has higher accuracy on phonemes that exist in both French and English or Spanish while having lower average accuracy on phonemes that are unique to French and do not exist in either of the pretraining languages, English or Spanish. This pattern stays true with the accuracy for Polish data with average accuracy for phonemes that exist in both Polish and English or Spanish having a higher average accuracy when compared to phonemes that only exist in Polish and not in English or Spanish.
\iffalse
Throughout this section we have shown that phonemes that overlap between the English phoneme sets and the Spanish phoneme sets have the highest accuracy due to them existing in all wav2vec 2.0 models we have pretrained. Phonemes that are unique to English have significant accuracy increases when tested on models with high ratios of English data in pretraining. The inverse is true for Spanish with the highest accuracy phonemes that are unique to Spanish being recorded on the model trained only on Spanish pretraining data. Figure \ref{ed_fig:phonemes_shared_french_polish} shows that for both the French language and Polish language phonemes that exist in either language as well as English or Spanish always achieve higher accuracy than phonemes that are unique to each language. We have seen in Section \ref{ed_subsec:gain_across_languages} that the best approach for low error transcription is to pretrain the model on the language you are finetuning or a language closely related to it.
\fi

The information in Figure \ref{ed_fig:phonemes_shared_french_polish} suggests that language is not the only factor in determining whether a phoneme will be recognised with high accuracy. The findings suggest that if we were carefully select pretraining data to include certain phonemes in high number we may be able to achieve higher accuracy even if our testing language was not included in or related to the languages in pretraining. To further dissect this idea we can evaluate performance across different phoneme groups and different languages. %\thoughts{This section could probably be moved to discussion LOOK HERE ED!!!}

%\newpage

\subsection{Phoneme Accuracy in Phoneme Classes}\label{ed_sec:phonemes_classes}

 %%% FULL TABLE WITH ALL PHONEMES AND ALL VARIATIONS %%%%%%%%%
\iffalse
\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{ Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
%Vowel & \textipa{i, i:, {\ae}, 5, A, A:, 6, 6:, @, \textrhookschwa, E, \textrhookrevepsilon, \i, 0, 0:, U} \\ %% All 
& Vowel & \textipa{i, i:, {\ae}, 5, A, A:, 6, 6:, @, \textrhookschwa, E, \textrhookrevepsilon, \i, 0, 0:, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, b\super j, c, c\super h, c\super w, d, d\super j, \|[{d}, k, k\super h, k\super w, p, p\super h, p\super j, p\super w, t, t\super h, t\super j, t\super w, \|[{t}, \textObardotlessj, \textObardotlessj\super w, g, g\super w, P} \\
& Nasal & \textipa{m, m \super j, \textsyllabic{m}, n, \textsyllabic{n}, N, M, \textltailn} \\
& Fricative & \textipa{f, f\super j, h, s, v, v\super j, z, \c{c}, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \textltilde, \textsyllabic{\textltilde}, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R, R\super j, \~{R}} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c,  \|[{d}, k, p, \|[{t}, \textbardotlessj, \textbardotlessj J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, \c{c}, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, \~{A}, O, \~{O}, \textschwa, E, \~{E}} \\
& Stop & \textipa{b, c, d, k, p, t, \textObardotlessj, g} \\
& Nasal & \textipa{m, m\super j, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, \~{O}, E, \~{E}, 1} \\
& Stop & \textipa{b, b\super j, c, \|[{d}, k, p, p\super j, \|[{t}, t\super j, \textObardotlessj, g, P} \\
& Nasal & \textipa{m, m\super j, \|[{n}, N, \textltailn} \\
& Fricative & \textipa{f, f\super j, \|[{s}, v, v\super j, x, \|[{z}, \c{c}, C, \:s, \textcommatailz, \textctz} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, \|[{d}\|[{z}, t\c{c}, t\:s, \|[{t}\|[{s}} \\
& Approximant & \textipa{j, \~{j}, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, r\super j} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used.}
\label{tab:phoneme_groups_by_language}
\end{table}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% change layout to phoneme groups being in a separate column to language
%\iffalse
%\newpage
\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
& Vowel & \textipa{i, {\ae}, 5, A, 6, @, E, \textrevepsilon, \i, u, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g, P} \\
& Nasal & \textipa{m, n, N, M, \textltailn} \\
& Fricative & \textipa{f, h, s, v, z, c, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c, k, p, \j, \j J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, c, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, O, \textschwa, E, \oe} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L, 4} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, E} \\
& Stop & \textipa{b, c, d, k, p t, \j, g, P} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, x, z, c, C, s} \\
%& Affricate & \textipa{dz, tc, ts} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, t\c{c}, t\:s} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Table of phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used, including those reflecting phonetic variations. This table is simplified so that variations are nor included, a full list of all MFA phoneme tokens for each group can be found in the Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phoneme_groups_by_language}.}
\iffalse
\caption{Phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used, including those reflecting phonetic variations such as extended duration of vowels (\textipa{i:, A:}), labialization (\textipa{\textObardotlessj\super w,  p\super w}), aspiration (\textipa{p\super h, k\super h}), palatalization (\textipa{R\super j, m\super j}), nasalisation (\textipa{\~{E}, \~{A}}), syllabification (\textipa{\textsyllabic{n}, \textsyllabic{\textltilde}}), dental placement \hl{noun?} (\textipa{\|[{d}, \|[{t}}) and rhotacization (\textipa{\textrhookschwa, \textrhookrevepsilon}). \hl{Extra variants} (\textipa{0, \textObardotlessj, \c{c}, \textltilde, \textbardotlessj J, 1, \:s, \textcommatailz, \textctz, d\textcommatailz, d\textctz, t\c{c}, t\:s, C})Phonetic variations are not included in the table, for full list of tokens used see Appendix table \ref{app:phoneme_groups_by_language} \hl{will have to revisit and reqrite this table description, still many variations included}}
\fi
\label{ed_tab:phoneme_groups_by_language}
\end{table}
%\fi

To further test this theory we can break down our phoneme data into sub-groups, this will allow us to explore whether the language or the phoneme groups are more important for recognition. Table \ref{ed_tab:phoneme_groups_by_language} shows the breakdown of phonemes in each phoneme group, the phonemes are selected from all of the phonemes in the MFA dictionaries for English and Spanish. All phonemes modifications, such as \textipa{a:} or \textipa{p\super j,}, are also included in the group with their unmodified phonemes. The full list of phoneme used in each language can be found in Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phoneme_class_acc_with_diff}.

The groups of phonemes chosen follow the same groupings as found in English et al. \cite{english2022domain} with some exceptions. All vowels were grouped together with any stessers also included. Diphthongs, that only exist in the English MFA dictionary, were separated into their own group, in order to test whether coarticulation between the phonemes may be relevant to the pretraining languages. Closure simply includes the "sil" token which denotes a lack of any sound for a short period of time. Finally the phonemes \textipa{r} and  \textipa{R} were separated into their own group, Tap \& Trill, while they could have been included in the Approximant group both \textipa{r} and \textipa{R} are much more frequent in the Spanish data so may be affected by pretraining data more than other approximants.

Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} shows a breakdown of the average accuracy of different phoneme classes from the validation data from the English LibriSpeech dataset, the Multilingual LibriSpeech (MLS) Spanish subset, the MLS French subset and the MLS Polish dataset. The groups show accuracy from layer 7 of each of the 7 pretrained models for the nine different phoneme groups described in Table \ref{ed_fig:Phonemes_Groups_English_linegraph}. Figure \ref{ed_fig:Phonemes_Groups_English_barchart} shows the difference in accuracy of each phoneme group for each language when taken between the accuracy of the monolingual English model trained only on LibriSpeech data and the monolingual Spanish model trained only on the Multilingual LibriSpeech (MLS) Spanish subset as shown in equation \ref{ed_equ:CER_differrence_phoneme_groups}.

\begin{equation}\label{ed_equ:CER_differrence_phoneme_groups}
\centering
    CER\_on\_monolingual\_English - CER\_on\_monolingual\_Spanish
\end{equation}

In the vowel plot on Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we see that Spanish vowels have the highest accuracy across all models being above 80\% in all cases except the English monolingual model (960 / 0). Notably, Spanish has the fewest total individual vowels of all of the languages as seen in Table \ref{app_tab:phoneme_groups_by_language}, Spanish has no variants to the phonemes in the vowels which can occur less frequently in both training and validation data and therefore lower accuracy the average accuracy. It is also notable that the Polish vowels have the highest accuracy on the balanced mixed model (450 / 450), this could imply that when the testing language is unrelated to the pretaining languages individual phonemes are more impactful than the relationships between phonemes. Figure \ref{ed_fig:Phonemes_Groups_English_barchart} confirms that Polish vowels have the least difference between accuracy on each of the monolingual models, whilst English has the highest difference. 

Dipthongs were separated into a separate figure from vowels in order to evaluate whether language specific coarticulation affected accuracy, Dipthongs were only available in the English MFA dictionary. We see in \ref{ed_fig:Phonemes_Groups_English_linegraph} that English Dipthongs have higher accuracy on every model than their vowel counterparts, possibly due to the reduced set. English Dipthongs also have a higher difference between accuracy on monolingual models of 16.2\%. A full list of results for every model and the difference between the monolingual models can be found in Appendix \ref{app_sec:chapter_5_appendix}, Table \ref{app_tab:phoneme_class_acc_with_diff}.

For approximants in both Spanish and English the highest phoneme accuracy exists on their respective monolingual models. While the difference between models for Polish in Figure \ref{ed_fig:Phonemes_Groups_English_barchart} is in favour of the English monolingual model accuracy for Polish approximants is also high for the 800 / 100 model and the 300 / 600 model. French on the other hand has little difference between accuracy on either of the monolingual models and little variation for approximants across all models.

For nasals Figure \ref{ed_fig:Phonemes_Groups_English_barchart} shows all four languages having higher accuracy on the English monolingual model than on the Spanish monolingual model. Spanish has higher accuracy for nasal phonemes on the English monolingual when compared to the Spanish monolingual model, although it should be noted that Spanish nasals have the highest accuracy on the 100 / 800 model, so still a model with mostly Spanish training data. This may however show some nuance to the idea that all speech data benefits from monolingual or single domain pretraining. If a model is better at recognising phonemes from a a certain group this may be more beneficial for downstream recognition than simply pretraining on the same language. Since English, French and Polish also show higher accuracy on the English monolingual model this may support that theory. 

For the Tap \& Trill category Spanish has higher accuracy across all models when compared to both Polish and English and has 9.6\% more accuracy on the Spanish monolingual model when compared to the English monolingual mode. While in Figure  \ref{ed_fig:Phonemes_Groups_English_barchart} English has 15.3\% higher accuracy on the English monolingual model when compared to the Spanish monolingual model. However, on Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we can see that there is also an 11.8\% difference between the English monolingual model (960 / 0) and the model with balanced pretraining data (450 / 450), since the English accuracy for this group does not go above 33\% this suggests that recognition for phonemes in this group is not as dependant on the pretraining data as other groups. Similarly for Polish the average accuracy is low across all groups with the highest accuracy on the Spanish monolingual model at 21.4\%. While Polish does have higher accuracy on the Spanish monolingual model than on the English monolingual model it seems that phonemes in this group have low accuracy regardless of pretraining data. %\thoughts{do I need to come back to this when I know what's happening in the pretraining data?}

For Stops we see the pattern shown in previous sections with English phonemes having higher accuracy on the English monolingual model when compared to the Spanish monolingual model with a difference of 19.3\%. The Inverse is then true for Spanish, although to a lesser degree, with Spanish spoken phonemes having higher accuracy on the Spanish monolingual model and lower on the English model although this time with a difference of 1.3\%. Interestingly, the highest accuracy for Spanish is found on the balanced model (450 / 450), suggesting Spanish is benefitting from balanced monolingual pretraining in this category. French and Polish both have higher accuracy on the English monolingual model when compared to the Spanish monolingual model. This effect may be due to English having a larger variation of phonemes in this group when compared to Spanish, this may be beneficial for accuracy for phonemes in this group. French nasal phonemes have the highest accuracy across all models except the 300 / 600 model. 

For Affricates Polish shows the least variation with only 1.4\% difference between accuracy on the English and Spanish monolingual models. English has the highest accuracy on the English monolingual model and the lowest on the Spanish monolingual model showing preference to pretraining on the same language. Spanish however, has higher accuracy on the English monolingual model when compared to the Spanish monolingual model. The highest recorded accuracy is on the 450 / 450 model while the lowest is on the 300 / 600 model, giving Spanish the most variation of the languages in this category. As seen in Table \ref{app_tab:phoneme_groups_by_language} Spanish only contains one phoneme in this category, \textipa{tS}, and there are only 391 examples of the phoneme in the training data for the probes as seen in Table \ref{app_tab:phoneme_counts_per_language_train_10h_3}, when compared to phonemes that often have more than 10,000 examples. Training probes to recognise a single phoneme, with few examples may contribute to the variation in this case.

For Fricatives we see English again having higher accuracy on the English monolingual model when compared to the accuracy on the Spanish monolingual model. Spanish fricatives then have the highest accuracy on the Spanish monolingual model and the lowest on the English monolingual model. Both French and Polish have higher accuracy across all models than either English or Spanish, which is not true of any other phoneme group.

Finally for the closure group for French, Polish and English the accuracy on every model is higher than the accuracy in any other group for each model and for Spanish the only higher accuracy is found an all model except the English monolingual model in the vowel category. The closure category only contains the silence token (sil) given by the Montreal Forced Aligner when no sound is made. The high accuracy in Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} across all languages and the lack of clear preference to either language in Figure \ref{ed_fig:Phonemes_Groups_English_barchart} suggests that silence is recognised accurately regardless of the model or the language the closure exists in.

Overall the findings in this section show that phoneme accuracy is not consistent across different categories of phonemes. As seen in nasals and affricates the concept that pretraining on the same or a related language always increases accuracy is not always true and in the case of Spanish stops and nasals, a balanced mix of pretraining can give the highest accuracy. In most cases however phonemes spoken in English do have higher accuracy on the models with the most English in pretraining and phonemes spoken in Spanish have the highest accuracy on the models with the most Spanish data in pretraining. 

For French, in all categories except closure phonemes in this language have higher accuracy recorded on the monolingual English model when compared to the monolingual Spanish model, which is consistent with the findings in Figure \ref{ed_fig:multilingual_barchart}. Accuracy is not consistent across all categories with a difference of 38\% between nasals and fricatives on the monolingual English model. For Polish, average accuracy is also inconsistent across categories, but is also less consistent across models with vowels having the highest accuracy on the 450 / 450 and approximants performing well on both the 800 / 100 model and the 300 / 600 model. 

These findings show that when the testing language is not in pretraining they do not necessarily benefit from monolingual training. 

%\thoughts{I feel that I need to expand this, I could possibly do this in the discussion section though}


%\newpage
\begin{figure}[H]
    %\centering
    \includegraphics[height=0.9\textheight, width=0.75\paperwidth]{5_chapter5/figures/Rotate_phoneme_groups_line_graph_wpolish.pdf}
    \caption{Phoneme accuracy for 9 different phoneme groups. Accuracy is shown across all 7 pretrained models for four different languages: English, Spanish, French and Polish.}
    \label{ed_fig:Phonemes_Groups_English_linegraph}
\end{figure}

%\newpage
\begin{figure}[H]
    %\centering
    \includegraphics[height=0.9\textheight, width=0.75\paperwidth]{5_chapter5/figures/Rotate_phoneme_groups_bar_chart_wpolish.pdf}
    \caption{The difference between the accuracy of 9 different phoneme groups when measured between the English monolingual model and teh Spanish monolingual model. Accuracy is shown across all 7 pretrained models for four different languages: English, Spanish, French and Polish.}
    \label{ed_fig:Phonemes_Groups_English_barchart}
\end{figure}


\newpage
\section{Discussion}\label{ed_sec:discussion}
This section started by training 7 models, one model trained only on the data from the LibriSpeech dataset \cite{librispeech}, one model trained only on the Spanish subset of Multilingual LibriSpeech \cite{MLS} and 5 models trained on different ratios of these 2 datasets, always adding up to 900 hours.The ratios of hours of speech data with English of the left and Spanish on the right (Eng / Spa) were as follows: 800 / 100, 600 / 300, 450 / 450, 300 / 600 and 100 / 800. As shown in previous work \cite{ashihara2023exploration, poncelet2021comparison}, we prove in Section \ref{ed_subsec:same_data_finetuning} that the languages in your pretraining affect downstream finetuning. Figure \ref{ed_fig:fine_tuning_100hours} shows that when there is more English data in pretraining the model has lower Character Error Rate (CER) after finetuning to English data. Similarly when more Spanish data is present in pretraining lower CER is found after finetuning to Spanish. This is then replicated with unseen FLEURs data in Section \ref{ed_subsec:gain_across_languages} Figure \ref{ed_fig:multilingual_linegraph}.

We then confirm findings in \cite{zhang2023fast, gupta2021clsril} with our models that prove that languages that are closely related to the language most prominent in pretraining data for a model achieve lower CER when compared to finetuning them on models pretrained on a language that is unrelated to the finetuning data. Figure \ref{ed_fig:multilingual_barchart} Romance languages such as Catalan, Asturian and Italian have lower CER on the monolingual Spanish model when compared to their CER on the monolingual English model. We can also see in Figure \ref{ed_fig:multilingual_barchart} that the Germanic languages such as Danish and German have a lower CER on the monolingual English model when compared to their results on the monolingual Spanish model, showing that languages related to English benefit from more English in pretraining. 

French shows lower error on the English monolingual model but less so than the Germanic languages, possibly due to it's phonetic overlap with English \cite{roth2010explore} despite being classified as a romance language \cite{bauer2007linguistics}. The two Slavic languages Russian and Polish show lower error on different models with Polish having a preference for English pretraining data and Russian having a preference for Spanish pretraining data. This implies that when a language is unrelated to the language most prominent in the pretraining data it does not directly benefit in accuracy.

Next we look to delve deeper into why related languages benefit each other when relating to pretraining and finetuning. The wav2vec 2.0 Self-Supervised Learning (SSL) pretraining stage uses contrastive loss to recreate the input to the encoder's first layer of the transformer encoder to  at the encoder's final layer. Pasad et al. \cite{pasad2021layer} show that wav2vec 2.0 learns different speech tasks in different layers of the encoder such as local acoustic features in the early shallow layers and phone identity in the late middle layers. They also show that when finetuned to a semantic task of producing English sentences the final layers change task from reconstruction of the input to grapheme output, while the middle to early layers do not change to the same extent. English et al. \cite{english2022domain} then use single linear layers to probe the English trained wav2vec 2.0 and test the phone recognition across each layer of the model. Similarly to Pasad et al. English finds the late middle layers having the highest phone accuracy, they however find that different phoneme group have varying accuracy across layers and across the phoneme groups. 

Using this information we can surmise that the phonetic but not semantic information learned during pretraining has a significant impact on the final error rate after finetuning and that this learned information will not change. This means that it is very important to understand how SSL ASR models are learning phonetic information, in our case we can take this idea further and look to discover how mixing languages affects pretrained phone recognition. We use the same technique as English et al. \cite{english2022domain} and probe each layer for phone recognition, we then select one of the highest performing layer (layer 7) and measure how well our models recognise different groups of phonemes. 

In Section \ref{ed_sec:shared_vs_not_shared}, we collect the phoneme accuracy data for all of the phonemes found in the English LibriSpeech dataset and the Spanish subset of MLS. For our first experiment we group phonemes by whether they appear in both datasets and therefore the pretraining data for all models vs phonemes that are unique to either English or Spanish. Figure \ref{ed_fig:phonemes_shared_not_shared} shows that when tested on English data only phonemes that are shared between English and Spanish and phonemes that are unique to English both perform best on the English monolingual model when compared to their performance on the Spanish monolingual model. However, we also see that on every model, phonemes that are shared between English and Spanish always have higher accuracy than uniquely English phonemes. 

When testing on Spanish data both phonemes shared between English and Spanish and phonemes unique to Spanish performing best on either the monolingual Spanish model or the 100 / 800 Eng / Spa model. However, for both sets, we again find that phonemes exist in both pretraining datasets perform better on every model. These Findings suggest that individual phonemes are still affected by which language they are spoken in, possibly due to coarticulation between different phonemes. They also show that these phonemes will achieve higher accuracy when the model has seen more examples of that phoneme regardless of the language it is spoken in as with the two shared phonemes groups.

Next we test the same phoneme groups but this time when the data is from languages that were not included in the pre-training data. We chose data from the French and Polish subsets of MLS for this experiment, French because it was an outlier in the previous section, having higher accuracy on the models with more English data and Polish because it is not in the same language family as either English or Spanish. In Figure \ref{ed_fig:phonemes_shared_french_polish} we see that phonemes that exist in both Spanish and English consistently outperform phonemes that exist only in French. The same pattern is also true of Polish to varying ratios with the phonemes that exist in English and Spanish consistently outperforming phonemes that are unique to Polish. These results support our previous findings with the English and Spanish data but also suggest that carefully choosing pretraining data based on which phonemes are included could increase accuracy even for languages that have not been seen in pretraining. Alternatively, choosing pretraining data with less phonetic diversity could harm the model's performance.

Next in Section \ref{ed_sec:phonemes_classes} we further divide our languages into phonemes classes and analyse the trends across the four languages. We analyse the same groups as \cite{english2022domain} but separate out Diphthongs and Taps \& Trills. A full list of the Montreal Forced Aligner (MFA) phoneme tokens used for each category alongside the total number of phonemes in each group for the pretraining data for each model and the total number of each phoneme included in the training data for our probes can be found in Appendix \ref{app_sec:chapter_5_appendix}. 

For most classes English phonemes perform best on models pretrained with more English data and phonemes spoken in Spanish perform best on models pretrained with more Spanish data as seen in Figure \ref{ed_fig:Phonemes_Groups_English_barchart}. This effect is not true for the Nasals or Affricate categories where when spoken in Spanish, they have higher accuracy on the monolingual English model when compared to the monolingual Spanish model. For affricates this may be explained by the number of phonemes in Spanish in this group, there is only one \textipa{tS} and although there is only one more affricate in English, \textipa{dZ} we can see in table \ref{app_tab:phoneme_token_class_counts_k} that the English model was trained on a total of 376k individual affricates whereas the monolingual Spanish model had only seen 90k examples. For Nasals both datasets contain more than 3M tokens in the affricate class, so it seems this unlikely to be the cause of this behaviour. Looking to Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we see that the 100 / 800, Eng / Spa, model actually has higher accuracy than the Spanish monolingual model or the English monolingual model at 61.7\%, compared to 49.4\% and 57.8\%. This suggests that a larger ratio of Spanish data still improves phoneme recognition.

For the Tap \& Trill category phonemes spoken in Spanish have higher accuracy than when spoken either English or Polish. Again we can see in Table \ref{app_tab:phoneme_token_class_counts_k} The pretraining data for the monolingual Spanish contains many more phonemes in this category for pretraining, with more than 2M, than the monolingual English monolingual model at less than 400k. Data for the Dipthong category was only available in English and the accuracy is high for every model when spoken in English. The accuracy on English spoken dipthongs reduces significantly however between the 100 / 800 model, going from 79\% accuracy to 69.6\%. The pretraining data for the monolingual Spanish model contained 0 dipthongs whereas the 100 / 800, Eng / Spa, model (containing only minimal English data) contains 206k diphthongs. This appears to show the importance of phonetic diversity in the pretraining data and that even a relatively small number of examples can dramatically increase accuracy when compared to an unseen phoneme.

We have also seen in the previous Chapter \ref{chapter4}\info{previous chapter on pruning bias}  that a multilingual SSL ASR model relies heavily on weight attributed to English \cite{storeyedbiasSSL}. Some experiments were done collecting the Intersect Over Union (IOU) data for our 7 models similar to the experiments done in Section \ref{IOU_Section}\info{Section in previous chapter on IOUs} but ultimately not included in this cahapter. The results showed that more weights in the English monolingual were attributed to English and more weights in the Spanish monolingual model were attributed to Spanish. When more English data was included in pretraining more of the weights were attributed to English and when more SPanish data was in pretraining more weights were attributed to Spanish. These results confirm the findings in Chapter \ref{WC2}\info{previous chapter on pruning and bias} that the largest dataset in pretraining had the most influence over the weight distribution
\iffalse
Two consistent themes from the research in this chapter have emerged: downstream accuracy and error rates perform best when the pretraining data is the same or a related language to the downstream testing or finetuning language and that seen phonemes always performs better than unseen phonemes. As we have seen in \cite{ashihara2023exploration, poncelet2021comparison} when the pretraining data matches the language of the finetuning data we can achieve much lower error rates with less data and we have replicated this effect with our experiments in Sections \ref{ed_subsec:same_data_finetuning} and \ref{ed_subsec:gain_across_languages}. We then show in Section \ref{ed_sec:shared_vs_not_shared} and Section \ref{ed_sec:phonemes_classes} that this may be an issue of seen and unseen phonemes and phoneme interactions such as coarticulation. These sections show that while pretraining on a related language benefits the finetuning accuracy, so does pretraining on individual phonemes that exist in the finetuning data or finetuning language. 
\fi

%\thoughts{Do I need to include some pruning IOU data that I collected to support this? Maybe I should as it would better link it to the last chapter and make this feel a little less out of the blue I probably don't need to gvie it a whole seciton but it could be the appendix}. 

In this chapter we have proven that related languages are more likely to contain similar phonemes and coarticulations than unrelated languages, thus giving better accuracy. However, we have also shown that including more examples of an individual phoneme can increase the accuracy for that phoneme or group of phonemes even when the testing language is unseen and unrelated to the languages included in pretraining. Given these findings we suggest that multilingual SSL ASR pretraining data should be balanced by phoneme content to include as many and as diverse a phoneme set as reasonably possible. By doing this, models will achieve higher accuracy on a broader range of languages with less pretraining data and on smaller models.


\newpage
\section{Future Work}
While phoneme accuracy and finetuned error rates may be better when language specific features are kept in the pretraining data the discovery that phoneme recognition can increase with language-agnostic training is important. 

For example, if a language, dialect or accent was very low-resource and did not have data from closely related languages in abundance SSL ASR could benefit from increasing the most common phonemes in said language by including data from unrelated languages. If we could design the pretraining data in this way we may be able to increase phoneme accuracy and lower error even for low-resource languages without having to resort to large models trained on large datasets. 

For future work, we would design a pretraining dataset that had the same phonetic makeup as a designated finetuning language. The pretraining data would include only data from languages unrelated to the one used in finetuning. We would expect such a model to achieve lower CER than a monolingual model trained on an unrelated language or a model trained on multilingual data that did not include the downstream language. 

Carefully designed pretraining data may even result in better performance than a model trained on multilingual data that contained small amounts of the finetuning language. This may be true as long as the finetuning target language was not the dominant language in pretraining, as per the behaviour exhibited by XLS-R in Chapter \ref{WC2}\info{previous chapter on pruning and bias}. These models could then all be compared to a monolingual model pretrained on a large dataset of the same language as finetuning, which we would expect to perform the best overall. 

If these experiments were successful we could have a simple method for increasing phoneme accuracy and CER for low-resource languages, dialects and accents. This method would only require the Montreal Forced Aligner and some knowledge of the target finetuning language to achieve.


\newpage
\section{Conclusion}
Over the course of this chapter we have dissected how Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models learn speech representations when pretrained with monolingual and multilingual training data. This gives us more insight into how SSL models learn and how we can leverage this knowledge for future work in this field. We started by pretraining 7 different versions of wav2vec 2.0 \cite{wav2vec} with different ratios of two different languages: English and Spanish. We pretrained 2 monolingual models, one on only English data from the LibriSpeech dataset \cite{librispeech} and one from only Spanish data from the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. We then pretrained 5 more models with different ratios of the English and Spanish datasets.

We started in Section \ref{ed_subsec:same_data_finetuning} by fine-tuning to 100 hours subsets of LibriSpeech and MLS Spanish to observe the effect that our pretraining ratios had on finetuning results. We found that The English LibriSpeech data had the lowest Character Error Rate (CER) on the monolingual English model and the highest CER on the monolingual Spanish model with CER steadily increasing as the amount of English data was reduced in the pretraining data. For Spanish we found the inverse with Spanish data having the lowest CER on the monolingual Spanish model and the highest CER on the monolingual English model, with CER steadily increasing as the amount of Spanish data in pretraining was reduced.

In Section \ref{ed_subsec:gain_across_languages} we reproduced the results found in the previous section by testing the model with 10 hours of unseen English and Spanish data each from the FLEURs dataset \cite{FLEURS}. The same pattern was observed with the 10h English FLEURs dataset having the lowest CER on models with more English data in pretraining and Spanish. The 10h Spanish FLEURs shows the inverse pattern again by having the lowest CER on models with the most Spanish data and the highest CER on models with the least Spanish data. These experiments prove that pretraining on the same language as you are finetuning to gives the best results for ASR tasks.

In the same section we test multiple languages in the same languages families as English and Spanish, Germanic and Romance languages \cite{bauer2007linguistics}. We find that the Germanic languages Danish and German both have a positive difference towards English and the only language with a more positive difference than these languages is English. Similarly the Romance languages Asturian, Catalan and Italian have negative differences showing they perform better on the Spanish monolingual model when compared to the English monolingual model. French is an outlier among Romance languages showing a positive difference towards English. Finally we include 2 Slavic languages Polish and Russian, Polish shows a positive difference whereas Russian shows a negative difference. In this case Polish appears to benefit from English pretraining data and Russian benefits from Spanish Pretraining, which indicates that unrelated languages benefit much less from either pretraining languages. These findings confirm previous work that shows that pretraining on related languages can benefit downstream finetuning \cite{zhang2023fast, gupta2021clsril} however given that it has been shown that SSL ASR models learn phonetic characteristics and not semantic characteristics of speech \cite{pasad2021layer} it was important that we understand how the models are learning phonetic information when pretraining on multilingual data.

To understand how multilingual pretraining learns phoneme data we trained linear layer probes to recognise individual phonemes for each language. We collected the phoneme data using the Montreal Forced Aligner (MFA) and then created small 10 subsets of data from MLS to train the probes for phoneme recognition. In Section \ref{ed_sec:shared_vs_not_shared} after we test phoneme accuracy across all models for phonemes that exist in both English and Spanish data when compared to phonemes that are unique to either English or Spanish. Our findings show that the phonemes have the highest accuracy on the monolingual model trained on the same language they were spoken in, similar to the finetuning experiments in Section \ref{ed_subsec:gain_across_languages}. However, it is also notable that the English shared group has higher accuracy than the English Unique group across all models. It was also true that the Spanish Shared group had higher accuracy on every model when compared to the Spanish Unique group. 

This suggested that phonemes will have higher accuracy if there are more examples of it in pretraining even if those examples are spoken in different languages. We test this by introducing phoneme data collected from 2 new 10 hour subsets of MLS: MLS French and MLS Polish. These languages were chosen because French had outlier behaviour in Figure \ref{ed_fig:multilingual_barchart} and Polish is a Slavic language unrelated to English or Spanish. We split the phoneme accuracy data into groups, phonemes shared by English and Spanish spoken in each language and phonemes unique to French or Polish. The results showed that phonemes that are found in both English and Spanish always perform better than phonemes that are unique to either French or Polish even when the shared phonemes were spoken in French or Polish. This suggests that even when a phoneme is spoken in a language unrelated to either of the pretraining languages accuracy can be higher if the model has seen it in pretraining.

In Section \ref{ed_sec:phonemes_classes} we explored whether phoneme group accuracy trends across multilingual pretrained models behaved differently when spoken in 4 different languages: English, Spanish, French and Polish. We found that for English and Spanish in most cases phonemes would perform better when tested on a model with more data from their native language. The exceptions being Affricates where the models with the most English have the highest accuracy, however only one phoneme exists in the affricate class in Spanish and there are much less examples in the training data for Spanish. French and Polish phonemes were more likely to perform better on the models trained with more English data, with some exceptions. For Polish performance for the Tap \& Trill group was marginally higher for models with more Spanish data, which may be due to Spanish having more examples of the phonemes in this group in the pretraining set. 

Finally with the knowledge learned throughout this chapter we suggest future work that could be undertaken. We suggest that error rates on low-resource languages could be reduced by using data from languages that do not have to have any relation to the target finetuning language. A pretraining dataset could created using any languages as long and an appropriate number of all of the relevant phonemes in your target language were included in pretraining.

The consistent themes that emerged from this chapter were that SSL ASR benefits from pretraining on the same or related languages to the finetuning language and that this is true for phoneme recognition as well. The secondary theme is that when there are more training examples for an individual phoneme, accuracy can be increased on that phoneme, regardless of language specific interactions such as coarticulation. Overall, it has been seen throughout the chapter that selecting your pretraining data with care can be a great benefit in ASR and that there is more to SSL ASR than just the language relationships between your pretraining and finetuning languages.



%\keywords{, hello, world}

\nomenclature[Z]{CER}{Character Error Rate}
\nomenclature[Z]{WER}{Word Error Rate}
\nomenclature[Z]{SSL}{Self-Supervised Learning}
\nomenclature[Z]{ASR}{Automatic Speech Recognition}
\nomenclature[Z]{CNN}{Convolutional Neural Network}
\nomenclature[Z]{MLS}{Multilingual LibriSpeech}
\nomenclature[Z]{MFA}{Montreal Forced Aligner}