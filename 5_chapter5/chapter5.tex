
%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Fifth Chapter **********************************
%*******************************************************************************
%\chapter{Phonetic Understandings of Multilingual Self-Supervised Learning Automatic Speech recognition Models}
\chapter{Analysing the Effect of Data Balance on the Learning of Multilingual Self-Supervised Learning\naominote{shorter title}}\label{chapter5}
\hl{GO THROUGH AND MAKE SPASCING CONSISTENT ON MODEL NAMES EITHER "800 / 100" OR "800/100"}
\section{Introduction}
\perplexnote{[referencing whole section] There are multiple uses of “we then,” which makes the writing feel repetitive. Try varying sentence openers, e.g., “Subsequently,” “Next,”}In the previous chapter, we analysed the multilingual performance of a widely cited open-source multilingual self-supervised Learning (SSL) Automatic Speech Recognition (ASR) model XLS-R \cite{babu2021xlsr}. We found through pruning \cite{frankle2018LTH} that when finetuning the pretrained model to data from languages that were unseen in the XLS-R pretraining data, that weights learned from the English language were used as basis for learning.\perplexnote{[for previous sentence] Suggestion: Split into two sentences. Consider: [perplexity suggestion]} This effect was observed \thiswillnotshow{\naominote{[\textbf{Done}]"observed" I'm not sure exactly what Naomi meant by this}} even when the finetuned language was present but low-data in pretraining.\iffalse \thoughts{doesn't make sense needs completely rewording} When a related language was present and high-data, such as Catalan and Asturian using \thiswillnotshow{\naominote{[\textbf{Done}]Sentence too long.}}English weights rather than Spanish weights to learn from\cite{storeyedbiasSSL}.\perplexnote{[Previous sentence] This is very complex and would benefit from revision.} \fi This effect in XLS-R appears to stem from the imbalance of languages in the pretraining, with English having the most amount of hours when compared to all other languages. For this chapter, we explore in more depth how the balance of languages in pretraining affects SSL ASR.\thiswillnotshow{\naominote{* Is that your question for the chapter?\textbf{ED:} I could change this into more of a question or I could remove or move it}}
\summary{This paragraph summarises the previous chapter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many popular open-source SSL ASR models are pretrained on a single language \cite{wav2vec, hsu2021hubert, chen2022wavlm}. Previous studies have found monolingual pretraining, pretraining an SSL model only on one language before finetuning and evaluation, to be highly efficient and resulting in low error rates, reduced training time, and fewer parameters compared with multilingual pretraining strategies \cite{ashihara2023exploration, poncelet2021comparison}.\thoughts{it needs to be made clear we are finetuning on the same language} Additionally, successful outcomes have been reported when pretraining incorporates linguistically similar languages. Grouping related languages or clustering data by shared speech features can help maintain strong recognition performance even for low resource languages \cite{zhang2023fast, gupta2021clsril}. \summary{This chapter introduces the effectiveness of monolingual pretraining and related langauges in pretraining}

SSL ASR models that are multilingual typically use data with a significant imbalance in language representation, with English as the dominant language \cite{XLSR-53, babu2021xlsr}. Given the findings in Chapter \ref{chapter4}, understanding how the ratio of training hours among languages affects downstream ASR performance is crucial. It will help us understand why monolingual pretraining has been effective, as well as revealing if well designed pretraining data can improve the performance of multilingual SSL ASR. In order to progress, we first need to understand which aspects of speech SSL ASR models are learning in the SSL pretraining stage.


SSL ASR models learn speech representations in two stages: an SSL pretraining stage followed by supervised finetuning. During pretraining, models are trained on large unlabelled speech datasets, which greatly expands the range of usable data sources. Because SSL pretraining does not involve labels, a second stage of supervised finetuning on labelled data is required.\summary{Here I have started to explain how SSL is different to SL} 


Pasad et al. \cite{pasad2021layer} show that the SSL pretrained wav2vec 2.0 model learns local acoustic features and phone identity without needing finetuning. English et al. \cite{english2022domain} show through layer probing that a pretrained wav2vec 2.0 is not only effective at learning phoneme representations, but that the models cluster phonemes by articulatory and positional relationships. With this information, we can gauge firstly that SSL pretraining alone is effective for learning phonetic information. Then secondly, that the phonetic knowledge information it learns has a significant impact on the final ASR performance of the model. However, since both papers experiment with a monolingual English wav2vec 2.0, there are some gaps in our knowledge on what and how these models are learning. This is especially apparent in a multilingual setting where phoneme relations and accuracy have not been measured.


For example, what happens when the ratios of unrelated languages in a multilingual SSL ASR model are not imbalanced? Do languages rely on a different language in pretraining data if English is not the most abundant? \thoughts{dunno if I should keep this in} Are individual phonemes learned as effectively when the pretraining data is multilingual compared to when it is monolingual? \summary{chapter questions} %\hl{DOI WE NEED ANYTHING AFTER THIS / I could end it here OR I CAN HAVE A SHORT EXPLANATION!! AND THEN COVER IT IN THE BACKGROUND}

%\hl{THEN WE NEED TO EXPLAIN WHAT IS MISSING HERE}

%\hl{NEED TO THINK ABOUT HOW TO WORD THIS AND WHY IT RELATED TO WHAT IM DOING}


In this chapter, we answer these question by pretraining multiple iterations of wav2vec 2.0 on varying rations of English and Spanish data. We collect finetuned error rates to show the impact of changing ratios of the two languages. We then probe each of our models in order to analyse how phoneme recognition at the SSL pretraining stage is impacted by different ratios of multilingual data. We analyse how well phonemes that overlap between the languages are recognised when the ratios are shifted. We also break down the data into articulatory groups and language-specific groupings of phonemes.

Overall, our findings show that language-specific information is encoded during pretraining. However, individual groups of phonemes can benefit when they are highly represented in pretraining data even when the languages they are sourced from are unrelated. Through testing we provide evidence that by designing your pretraining dataset by phoneme content instead of language content, phoneme accuracy can improve with SSL training. This is true for phoneme groups even when the testing language is unrelated to the pretraining languages. 

\begin{itemize}
    \item \textbf{The Story of the chapter to keep consistent}
    \item \textbf{Intro and Background:}
    \item Popular SSL ASR are pretrained on a single language OR pretrained on multiple languages but imbalanced towards one or two
    \item We have no examples on scale for models trained with balanced hours or controlled ratios
    \item We know that SSL ASR models perform best when the pretraining language is monolingual and matches or is closely related to the finetuning language
    \item How do SSL ASR models learn their pretraining data then?
    \item They learn acoustic and phonetic information in the intermediate layers \cite{pasad2021layer}
    \item They learn articulatory features in the early layers and fully realised phonemes in the intermediate layers \cite{english2022domain}
    \item They learn phonemes, therefore the information that affects the final performance is their performance on phoneme accuracy, not word accuracy
    \item Since we saw in the previous chapter that multilingual XLS-R is overly reliant on it's most abundant source of data English, we need to know how multilingual data affects phoneme accuracy as this will give insight into how models learn
    \item \textbf{Experiments to test these ideas}
    \item We pretrain multiple multilingual models with varying ratios of mutlilingual data
    \item as expected, monolingual models do best when finetuned to the same language
    \item However, it is notable that error decreases is high-resource match pretraining and decreases in low-resource match pretraining
    \item This effect with matched pretraining languages is recreated when finetuning is in FLEURS and is unseen data
    \item With related-language setting, we see a similar situation to the matched setting
    \item with unrelated language we also see the same effect as the matched monolingual, high resource and low resource pretraining
    \item when the finetuning language does not match either monolingual or any high-resource languages in pretraining, this effect is less clear.
    \item Here we see that the ratios of languages effect downstream performance and that the more high-resource a language is in pretraining the lower the final error rates are, inversely the error rates increase as the matching language becomes more low-resource
    \item Given that our SSL models are learning phonemes, how are multilingual pretraining effecting this.
    \item We introduce our probing experiments here first we want to see the difference between phonemes that exist in both languages vs phonemes that only exist in English and phonemes that only exist in Spanish.
    \item \hl{\textbf{For the background section we may need to mention the limit on datasets that MFA could handle and the pipeline I used for MFA and the OOV words}}
    \item Given the overlapping phonemes will be in high quantities no matter the ratio we would expect to see more significant changes in accuracy between the low to high resource changes for phonemes that exist in the matching language.
    \item We also test these groups when spoken in other languages, French and Polish.
    \item French is related to Spanish and has lots of phonetic overlap with English. While Polish is unrelated
    \item these experiments show us that even when the phonemes are spoken in another language the shared phonemes have higher accuracy 
    \item So we know at this stage that phoneme accuracy can be language-specific but that phonemes that occurr in all models have higher average accuracy even when spoken in an unrelated language
    \item \textbf{Do I need to mention amounts here? Probably I would wait until the next section}
    \item Next, We split our phonemes into multiple sub groups by POA and MOA like in \cite{english2022domain} and then also into two language-specific groups
    \item These tests allow us to separate phonemes into smaller groups and test how MOA and POA affect language-specific accuracy are phonemes 
    \item we can also test here how representation of phonemes (i.e. the total number) affect accuracy, some phonemes have less overall phonemes and other have high numbers on one language but not the other
    \item we also test all of these different angles on French and Polish as well to see if the same patterns hold true for different languages
    \item \textbf{Discussion and Conclusion}
\end{itemize}

\newpage
\textbf{From Perplexity}
A good way to keep this clear and consistent is to define a small set of terms once, then reuse them. For example:

\textbf{1) Pretraining / finetuning language match}
\begin{itemize}
    \item “Matched-language setting”: Pretraining and finetuning use the same language.
    \item “Mismatched-language setting”: Pretraining and finetuning use different languages.
    \item “Related-language setting”: Pretraining and finetuning languages are different but linguistically related (e.g. English → German).
    \item “Unrelated-language setting”: Pretraining and finetuning languages are different and not closely related (e.g. English → Polish).
\end{itemize}

\textbf{2) Amount of matching language in pretraining (for your mixed-ratio models)
You can define thresholds once, then refer back to them:}
\begin{itemize}
    \item “Monolingual pretraining”: 100% of the pretraining data is the finetuning language.
    \item “High-resource match pretraining”: The finetuning language is the majority of the pretraining data (e.g. >50%).
    \item “Low-resource match pretraining”: The finetuning language is present but only a small fraction of the pretraining data (e.g. <30%).
    \item “No-match pretraining”: The finetuning language is absent from the pretraining data.
\end{itemize}

Then you can write short, clean sentences like:
\begin{itemize}
    \item “We compare matched-language and mismatched-language settings.”
    \item “For English finetuning, we test monolingual, high-resource match, and low-resource match pretraining conditions.”
    \item “Spanish finetuning is evaluated under related-language, unrelated-language, and no-match pretraining.” 
\end{itemize}
If you like, paste a short methods paragraph and these labels can be woven in explicitly.

\newpage
\textbf{When we have multilingual pretraining dataset with many languages that have very different numbers of hours per language.}

A short, standard-sounding phrase here is:
\begin{itemize}
    \item “imbalanced multilingual pretraining data”
    \item or more explicitly: “heavily imbalanced multilingual pretraining distribution
\end{itemize}

\textbf{When the languages are unrelated but have an equal number of hours in pretraining how would I refer to this situation.}

A short label plus a brief definition will keep this tidy. For the case you describe, something like:
\begin{itemize}
    \item “Balanced-unrelated pretraining”: the pretraining data consists of two (or more) unrelated languages, each contributing an equal number of hours, and the finetuning language is one of these.
    \item If the finetuning language is not one of them, you could call it “balanced-unrelated, no‑match pretraining”.
\end{itemize}

\textbf{The ideas as a whole, like the concept of studying the different number of hours in languages in pretraining whether that is monolingual high or low resource or unrelated or related.}

A compact, umbrella phrase you can use is something like:
\begin{itemize}
    \item “Pretraining language data balance and match conditions”
\end{itemize}

After defining it once that, you can shorten it in running text to:
\begin{itemize}
    \item “different pretraining language conditions”
    \item “pretraining data balance conditions”
    \item “pretraining language balance and match”
\end{itemize}

Example uses:
\begin{itemize}
    \item “We analyse how different pretraining language conditions affect downstream ASR performance.”
    \item “These experiments compare pretraining data balance conditions (monolingual, high-resource match, low-resource match, balanced-unrelated) for English and Spanish.”
\end{itemize}









\newpage
To answer these questions, we first pretrain multiple versions of the wav2vec 2.0 \cite{wav2vec} model on two different datasets: LibriSpeech \cite{librispeech} and the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. In total, we pretrain seven different models, two are monolingual models in English and Spanish. We then pretrain five more models with different ratios of the two datasets, always adding up to a total of 900 hours. We then finetune each of the models to English and Spanish data. This tests whether ASR error rate is related to pretraining data and how introducing varying amounts of a new language affects the final error rate. Further finetuning is undertaken with multiple languages from multiple language groups, each language is a  subset of the FLEURs dataset \cite{FLEURS}. These languages include languages related to English or Spanish, Germanic or Romance languages, and unrelated Slavic languages are also tested. These experiments test the impact of related and unrelated pretraining data has on the final error rate. They also reveal the impact missing unrelated data in pretraining has on this affect.\summary{this paragraph covers our pretraining ratios and datasets and the FLEURs experiments}

\iffalse
We then finetune each of these models to smaller 100 hour subsets of LibriSpeech and MLS Spanish to test whether ASR error rate is related to pretraining data. We then finetune each of the models to English and Spanish subsets of the FLEURs dataset \cite{FLEURS} to confirm whether the same patterns are observed with unseen data. Further finetuning is undertaken with multiple languages from multiple language groups. These languages are either related to English or Spanish, Germanic or Romance languages, and unrelated Slavic languages are also tested. \summary{this paragraph covers FLEURs experiments} 
\fi
\iffalse
Next,\thoughts{this paragraph needs some rewriting with our new narative and further exploration of Cormac's stuff} we analyse how our pretrained SSL speech models learn phonetic information by training probes for phoneme recognition. Analysing phoneme accuracy at this stage allows us to understand how SSL ASR models learn multilingual speech. If phonemes shared across English and Spanish are well understood across all models, then SSL pretraining is language independent. If phonemes are learned independently of the languages they are spoken in, better pretraining data can be designed. However, if phonemes accuracy is lower for phonemes shared by both languages when the pretraining data contains multiple languages, then SSL pretraining is language dependant. This outcome may shed light on why monolingual pretraining is so effective. Phonemes spoken in English, Spanish, French and Polish will be tested in these experiments.\summary{This chapter explains the shared/ not shared work}
\fi
%\newpage  
Next,\thoughts{this paragraph needs some rewriting with our new narative and further exploration of Cormac's stuff} we analyse how our pretrained SSL speech models learn phonetic information by training probes for phoneme recognition. We analyse the relationship between phonemes that exist in both of our pretraining languages and phonemes that only exist in either English or Spanish as separate groups.\thoughts{may need rewording} Analysing phoneme accuracy at this stage allows us to shed light on why monolingual pretraining is so effective. \summary{This chapter explains the shared/ not shared work}


Finally,\thoughts{this will also need rewriting} we separate out phoneme data into multiple groups of phonemes and analyse their accuracy across each of our models. We choose nine\thoughts{do I usually use 9 or nine?} different groups of phonemes for analysis. These groups are a mixture of groups by manner of articulation and groups that are unique or more frequent in either English or Spanish.\thoughts{do I need to reference Cormac here to justify why it is 9 groups?} Analysing the phoneme accuracy for each language on each model will give us insight into the extent to which language impacts the learning of individual phonemes. Phonemes spoken in English, Spanish, French and Polish will be tested in all the probing experiments.

Overall, our findings in this chapter gives new insight into how SSL pretraining for ASR learns speech features when pretrained with multilingual or monolingual pretraining data. \iffalse why finetuned multilingual and monolingual performance is affect by the SSL stage. \fi We finish by suggesting approaches to data balance that could improve the performance in future work and create efficient and effective training methods going forward.\naominote{Nuance around the relationship between languages has not been brought out. Maybe the related work needs to come first to motivate. Regardless, need a clear statement on what you want to establish. More focussed}\summary{Conclusion, probably needs more of a rework}

\newpage
\section{Background and Related Work}\label{ed_sec:background}

In this section, we will investigate the background of how SSL ASR learns language. We will use previous research to understand which factors may have contributed to the findings in Chapter \ref{chapter4}. Then, finally, we identify the gaps in the current knowledge on multilingual SSL ASR. \thoughts{revist this I think}

\subsection{Multilingual and Cross-Lingual SSL ASR}
Wav2vec 2.0 is an SSL ASR model that has been pretrained on multiple languages, usually in a monolingual setting, and achieves low error when finetuned \cite{wav2vec}. XLS-R \cite{babu2021xlsr} is a large SSL ASR model with the same but expanded architecture as wav2vec 2.0 \cite{vaswani2017attention} and is pretrained on 128 different languages. XLS-R performs well across various languages \cite{FLEURS_XLSR_WHISPER}. However, it has been found that the model relies heavily on weights learned from the most high-resource language, a language with a high number of hours when compared to others, in pretraining \cite{storeyedbiasSSL}. The most high-resource language in pretraining is, in this case, English. English is often the most high-resource language in multilingual SSL ASR pretraining data due to its abundant data sources \cite{librispeech, librilight, MLS, wang-etal-2021-voxpopuli}. \summary{This paragraph introduces XLS-R and compares in to wav2vec 2.0, we also introduce English as the high-resource language and the concept of imbalanced data (although doesn't use the word balance)}

%----------------------------------------------------------------------------------------------------------

Due to the neural scaling principle, we know that the performance of Deep Neural Networks (DNNs)\thoughts{do I need to redefine CNNs and DNNs?} scales with the amount of data they are trained on. If a model is both high in parameter count and is trained on a large data source, it will perform better in more diverse settings than a small model trained on a small dataset \cite{hestness2017deep, kaplan2020scaling}. This scaling effect has also been shown to exist in ASR\cite{zhang2022bigssl, pu21_interspeech}. XLS-R was pretrained on 128 languages from hundreds of thousands of hours of data. The parameter count was therefore  was scaled for its large data multilingual task \cite{babu2021xlsr}. \summary{This paragraph cover neural scaling principle and is currently here to justify using a smaller model later.}\thoughts{maybe move neural scalability to the next section?}

%----------------------------------------------------------------------------------------------------------

However, studies have shown that monolingual pretraining and finetuning on a matching language, the same language as used in pretraining, can yield low error rates. This can be done with fewer parameters than large models pretrained with large multilingual datasets, often with lower finetuned error rates \cite{ashihara2023exploration, poncelet2021comparison}. Other studies have shown that pretraining on related-language datasets, data containing two or more related languages, yields similar low error results for related-language, languages related to those included in pretraining, finetuning \cite{zhang2023fast, gupta2021clsril}. \summary{this paragraph tells the benefits of monolingual and related-language pretraining}

There are examples in literature of monolingual pretraining, multilingual related-language pretraining and multilingual unrelated and imbalanced, the number of hours vary per language, pretraining data. However, to date, there has been no research studying the effect of data balance, how many hours of each language are included in pretraining, for unrelated multilingual SSL pretraining. Neither has there been an extensive study into the effect on accuracy, when an SSL ASR model is pretrained on unrelated but balanced languages, when compared to any other pretraining strategies. In order to investigate these open areas of study in SSL ASR, we will first need to understand how the wav2vec 2.0 family of models learn. \summary{this paragraph covers what research has yet to be done, where the holes in research are}

%\newpage
\subsection{Wav2vec 2.0 and Self-Supervised Speech Pretraining}\label{ed_subsec:background_wav2vec}
Wav2vec 2.0  is an SSL ASR model utilising a Convolutional Neural Network (CNN) \cite{krizhevsky2012imagenet} feature extractor and transformer block \cite{vaswani2017attention} encoder.\naominote{[Done] split into 2 sentences} Wav2vec 2.0 exists as 12 transformer block "Small" and a 24 block "Large" versions of the model \cite{wav2vec}. Wav2vec 2.0 \naominote{[Done] removed "for this study"} shares the same architecture and SSL pretraining loss function as XLS-R \cite{babu2021xlsr} but with fewer transformer blocks and smaller layers, trading lower pretraining time for lower overall accuracy. Wav2vec 2.0 and its successor models XLSR-53 \cite{XLSR-53} and XLS-R \cite{babu2021xlsr} use contrastive loss to pretrain their models on large unlabelled datasets \cite{wav2vec}.\info{Do I need a diagram of the wav2vec architecture in this section? I will have one in the Literature Review when that's done.} \thoughts{a lot of this (maybe all) paragraph can be removed}

Contrastive loss pits the output of the pretrained feature extractor against the output of the transformer encoder. This trains the transformer blocks to recreate the input to block 0 at the output of the final block, block 11.\naominote{[for this previous sentence] Clarity?} This loss objective teaches wav2vec 2.0 to perform an autoencoder \cite{hinton2006reducing} style task. Thereby, learning the features of the input before recreating them at the output. Contrastive loss allows wav2vec 2.0 to be trained on large amounts of unlabelled data. 

For deployment in downstream tasks, wav2vec 2.0 will then be finetuned to a smaller dataset.\naominote{[Done] I have split this into 2 sentences} For ASR, Connectionist Temporal Classification (CTC) Loss \cite{graves2006connectionist} applied to transcribed speech audio data is the most common approach. There is a dissonance between the pretraining loss objective and the final task, autoencoder to transcription. It is therefore important to understand what wav2vec 2.0 learns during the contrastive pretraining stage and what information it is learning\naominote{[for this final sentence] Rephrase}

\subsection{Layer-Wise Analysis of Pretrained Wav2vec 2.0}
Previous studies show significant difference between the tasks learned on each layer\cite{belinkov2017analyzing} for DeepSpeech2 \cite{amodei2016deep}, a previous supervised ASR model. They also show that different categories of phonemes are learned with higher accuracy at different layers. Choi et al. \cite{choi2024self} have found that SSL ASR speech representations exhibit more phonetic similarity than semantic similarity \cite{choi2024self}. \summary{this paragraph introduces layer analysis with deepspeech}

Pasad et al. \cite{pasad2021layer} take these ideas further by assessing the performance of each layer of wav2vec 2.0  in multiple domains: local acoustic features, phone identity, word identity and word meaning. Pasad et al. find that local acoustic features are best represented in the earliest layers of the pretrained but not finetuned wav2vec 2.0. For both the small and large wav2vec 2.0 models, shallow layers represent local acoustic features best. Accuracy then lowers significantly for the middle layers of the  models. They then see a rise in accuracy again in the later layers. Phonetic information is best recognised in the middle layers of the pretrained models, without an increase in the deep layers. \summary{this paragraph give an overview of the phonetic and acoustic findings from Pasad et al.}

Word identity and word meaning are both low in accuracy in the pretrained model when compared to the finetuned model. This suggests that the pretrained model does not learn semantic tasks well during the SSL pretraining stage. \naominote{[referencing whole section] Sentences too long and complex simple technical style needed.} Shallow layers that learn phonetic and acoustic tasks are also shown \hl{to change in accuracy the least}\naominote{This suggests change the least... [Ed: that's correct?]} when comparing the pretrained and finetuned models. Given that this information learned in pretraining affects finetuned ASR performance, we next need to understand how phonetic information is being learned during pretraining.\naominote{use commas![Done]}\summary{this paragraph give an overview of the semantic findings from Pasad et al. and links this to the next section}

It is worth noting that De\naominote{It is worth noting that... [Done]} Heer Kloots et al. \cite{deheerkloots25_interspeech} undertook work published in the Interspeech 2025 proceedings that pretrained a monolingual Dutch wav2vec 2.0 model for analysis. They pretrain a wav2vec 2.0 model using fairseq \cite{ott-etal-2019-fairseq} for 100k steps on Dutch speech data from various sources. They compare this model with the \textbf{fb-en} model trained by meta on LibriSpeech for 400k steps and \textbf{fb-voxp-100k} a multilingual model trained on 100k hours of voxpopuli data \cite{wang-etal-2021-voxpopuli}. They then train probes to analyse for phonetic analysis and lexical analysis and compare these findings against downstream finetuned ASR performance. They found that pretraining on Dutch data gave higher accuracy for  both phonetic and lexical tasks, to varying degrees. Their work was undertaken in parallel with the research in this chapter and published after our work was completed.\naominote{Rephrase. Say in this chapter to distinguish [Done]} It did not influence any decisions made during our research. In Section \ref{ed_sec:discussion} we consider our results in the context of their findings.\thoughts{Don't forget this bit it may need rewording when the discussion is written} \naominote{In Section X, we consider our results in the context of their findings.[Done]}\naominote{Don't just ignore.}

\subsection{Layer Probing for Phoneme Recognition}\label{bac_subsec:layer_probing_articulation}
English et al. devise experiments to study what kind of phonetic information is encoded in the internal representations of wav2vec 2.0 \cite{english2022domain}. They train linear layer probes  \cite{immer2021probing, schneiderleveraging} for phoneme recognition on the outputs of each layer of the pretrained wav2vec 2.0. This model is pretrained on the LibriSpeech 960 dataset \cite{librispeech} but not finetuned on any data, and the phonemes are taken from the TIMIT dataset \cite{timit}. English et al. find that wav2vec 2.0’s internal representations encode rich phonetic information, and that this information is not uniformly distributed across layers. This is similar to Pasad et al. \cite{pasad2021layer} who found that the middle transformer layers of wav2vec 2.0 contained the highest accuracy for phonetic tasks. \summary{This paragraph covers cormac's discover of pretrained being good for phoneme accuracy and the discover of layer 7 as the best layer}

English et al. split phonemes into domain-informed groups such as manner of articulation and place of articulation and then analyse probe performance across these groups. They find that classification accuracy for both manner and place peaks around intermediate transformer layers (around layer 7). Their confusion heatmaps and hierarchical clustering show that the model’s representations increasingly align with phonetic structure as depth increases, up until layer 7. \hl{Cormac papers}\thoughts{I don't know that we need to go into detail here about cormac's other papers maybe we can do that in the background section} \summary{This paragraph summarises cormac's phoneme classes }

\begin{itemize}
    \item For paper 1 \cite{english2024following}
    \item They probe wav2vec 2.0 to find out if learns sounds in the same way that linguistics organises phonological features
\end{itemize}

\hl{COME BACK TO THIS!! I need a paragraph that summarises cormac's other papers, possibly some other probing applications and then leads on nicely to the experimental design}

\newpage
\section{Experimental Design}\label{ed_sec:experimental_design}
In this section, we outline the datasets and configurations used in the experiments in this chapter. The training parameters used for both the SSL pretraining and finetuning for ASR stages of training. We then outline the methods used to obtain phoneme labels for our data, as well as the hyperparameters and training configurations for training linear layer probes. Finally, we assess phoneme accuracy data collected from our trained probes. We organise the results into shared vs non‑shared phoneme sets and manner of articulation classes to compare how different pretraining conditions learn different phonetic groups. 

\subsection{Model}
To answer the open questions in research around how SSL ASR models learn multilingual data, we must choose an appropriate model and appropriate datasets to experiment with. Wav2vec 2.0 was selected as the SSL ASR model for these experiments. It offers strong performance in monolingual settings \cite{wav2vec} and remains small enough that we can pretrain it multiple times under different data balance conditions. XLS‑R, used in chapter \ref{chapter4}, is another candidate for multilingual SSL pretraining. It is built on the same underlying wav2vec 2.0 architecture, but it increases the number of transformer blocks and the width of layers. This makes it much more expensive to train and analyse systematically. Using wav2vec 2.0 therefore lets us train a larger number of models with different pretraining language balances across multiple languages in the time available. 

\subsection{Pretraining Languages}
English and Spanish were selected as the pretraining languages, we did this for several reasons. They come from different language branches: English is a Germanic language and Spanish is a Romance language \cite{bauer2007linguistics}. They differ in grammar and phonology while still being Indo‑European languages. Both languages also have large amounts of clean, read speech data. It is available within single, consistent corpora, which reduces variation in recording conditions and transcription quality \cite{librispeech,MLS}. These corpora provide enough hours of speech to support both monolingual pretraining and mixed‑language conditions without needing to combine multiple datasets. Although English and Spanish share a number of phonemes, but also have several unique to each language. This makes them a useful pair for studying how multilingual SSL models learn shared and language‑specific phonemes. In addition, both languages have existing pronunciation dictionaries for the Montreal Forced Aligner (MFA). This makes it straightforward to obtain time‑aligned phoneme labels for later analyses \cite{mcauliffe2017montreal}.

\subsection{Datasets and Pretraining}\label{ed_subsec:datsets}
For pretraining wav2vec 2.0, we use combinations of the two datasets outlined in Section \ref{DATASET_SECTION_IN_LR}\info{I haven't written this section yet, it will be in the Lit Review}: LibriSpeech \cite{librispeech} and the Spanish subset of MLS \cite{MLS}. First, we pretrain wav2vec 2.0, described in Section~\ref{ed_subsec:background_wav2vec}, on the full LibriSpeech dataset, which contains 960 hours of English speech; this monolingual English model is denoted 960 / 0. Next, we pretrain wav2vec 2.0 on the MLS Spanish subset, which contains 920 hours of Spanish speech; this monolingual Spanish model is denoted 0 / 920.

We also pretrain five mixed-language models with different balances of LibriSpeech (English) and MLS Spanish. Using the notation (English / Spanish), the total pretraining hours are fixed to 900, with the following splits: 800 / 100, 600 / 300, 450 / 450, 300 / 600 and 100 / 800. For each split, we select utterances by iterating through a fixed list, so that the same hours of speech are reused across conditions (for example, the first 100 hours from each dataset are identical wherever they appear). All pretraining sets are balanced by gender, with 50\% male and 50\% female speech.

Next, in Section \ref{ed_sec:experimental_results}, we finetune each pretrained model on 100 hours of LibriSpeech and 100 hours of MLS Spanish separately. These 100-hour subsets are constructed in the same way as the pretraining splits, by selecting from a fixed list of utterances and enforcing a 50\% / 50\% male / female balance. We then finetune each model on several languages from the FLEURS dataset. As outlined in Section~\ref{DATASET_SECTION_IN_LR}\info{I haven't written this section yet, it will be in the Lit Review}, each FLEURS language provides 10 hours of speech, and we use these languages only for finetuning and evaluation. Table~\ref{tab:datasets_overview} summarises the datasets and how we use them.

\begin{table}[b]
    \centering
    \begin{tabular}{l l l l p{4cm}}
        \hline
        Corpus & Lang. & Hours & Setting & Use \\
        \hline
        LibriSpeech & En & 960 & Pretrain & Monolingual model  \\
        MLS & Es & 920 & Pretrain & Monolingual model \\
        LibriSpeech + MLS & En+Es & 900 & Pretrain & Mixed models  \\
        LibriSpeech & En & 100 & Finetune & ASR evaluation \\
        MLS & Es & 100 & Finetune & ASR evaluation \\
        FLEURS & Multi & 10 / lang & Finetune & Additional languages \\
        LibriSpeech & En & 10 & Probe & Layerwise probes \\
        MLS & Es/Fr/Pl & 10 / lang & Probe & Layerwise probes \\
        \hline
    \end{tabular}
    \caption{Overview of corpora, languages, and their roles in pretraining, finetuning, and probing.}
    \label{tab:datasets_overview}
\end{table}

Finally, we use 10-hour subsets of LibriSpeech and of the Spanish, French, and Polish subsets of MLS to train our layerwise probes, as described in Section \ref{ed_seubsec:exp_des_probing}. These 10-hour subsets are constructed in the same way as the 100-hour subsets for LibriSpeech and MLS Spanish, again using a fixed list of utterances and a 50\% / 50\% gender balance. The selected speech is then segmented into windows, and wav2vec 2.0 features are extracted from each pretrained model for use in training the linear probes. 

Our aim in training multiple wav2vec 2.0 models is to isolate the effect of pretraining language balance and language match and mismatch conditions on downstream performance. By varying the proportion of English and Spanish in pretraining while keeping the total number of hours fixed, and then finetuning each model on the same 100-hour and 10-hour subsets and additional languages, we can directly compare how different pretraining conditions affect ASR error rates and, later, the structure of learned phonetic representations. This requires a family of closely matched models that differ primarily in their pretraining language composition rather than in architecture or optimisation settings.




\subsection{Pretraining Hyperparameters} \label{ed_pretraining_configuration}
With our data selected, we next pretrain our wav2vec 2.0 models from random initialisation. We pretrain wav2vec 2.0 \cite{wav2vec} on the 960‑hour LibriSpeech training set, the 920‑hour Spanish subset of MLS and on each of the five 900‑hour English/Spanish mixed pretraining conditions described in Section \ref{ed_subsec:datsets}. We follow the unsupervised pretraining recipe provided in the wav2vec 2.0 repository, implemented in the fairseq framework\footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/README.md}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/README.md}} \cite{ott-etal-2019-fairseq}.

The original wav2vec 2.0 model was trained on 64 V100 GPUs with 16Gb of VRAM each and used a \texttt{max\_tokens} value of 1,400k \cite{wav2vec}. In fairseq, \texttt{max\_tokens} controls the total number of audio frames per batch. Fairseq also includes an \texttt{update\_freq} parameter, which implements gradient accumulation: gradients from several smaller batches are summed before a single optimiser update is applied. Accumulating gradients over multiple forward passes before backropagation has a similar effect to increasing the batch size while using more multiple GPUs, because the weight update is computed from more data before each step.

For the experiments in this chapter, we used a single Nvidia RTX A6000 GPU with 48Gb of VRAM. We experimented with increasing \texttt{max\_tokens} and decreasing \texttt{update\_freq} when pretraining on the 960‑hour LibriSpeech training set. With our final settings, the effective batch size was 6.4 times larger than in the original wav2vec 2.0 training script, and we used an \texttt{update\_freq} of 10 on a single GPU. This combination simulated training with gradients accumulated over the equivalent of 64 smaller GPUs and achieved a similar final pretraining loss at 80k updates to using the original script for 80k updates with a smaller batch size and \texttt{update\_freq} of 64. This setup also reached 80k updates in less than half the time of the original script. We also finetuned both setups on 100 hours of LibriSpeech every 10k updates; the resulting Loss was within an acceptable margin for our purposes.

By default, the fairseq pretraining script runs for 400k updates and with our setup, training took approximately one day per 12k updates. To reduce total training time, we looked for a number of updates at which additional pretraining would give diminishing returns while still achieving an acceptably low WER after finetuning. We monitored performance while running multiple pretraining experiments for several days and finetuned checkpoints on a 100 hour subset of LibriSpeech every 10k updates. The original wav2vec 2.0 model attains a WER of 3.4\% on the LibriSpeech 100 hour clean set after 400k updates \cite{wav2vec}. In our setup, we found that pretraining on the full 960‑hour LibriSpeech dataset for 50k updates was sufficient to reach a WER below 10\% on this evaluation set. Beyond 50k updates, we observed diminishing returns in WER improvements. For our purposes, a WER of 9.6\% represented an acceptable trade off between accuracy and training time, especially when pretraining multiple separate models. The final hyperparameters we changed were: \texttt{max\_tokens} = 8,960k, \texttt{distributed\_world\_size} = 1, \texttt{update\_freq} = 10 and \texttt{max\_update} = 50k; all other settings followed the original wav2vec 2.0 configuration \cite{wav2vec}.




%\newpage
\subsection{Finetuning and Evaluation Languages}\label{ed_subsec:finetuning}
When finetuning our model to 100 hours of data  we follow the Fairseq finetuning scripts for LibriSpeech, regardless of the dataset we are finetuning on. We modify them to run on a single GPU, with all other parameters unchanged \footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_100h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_100h.yaml}}. We double the max\_tokens from 3200k to 6400k and double the update\_freq from -4 to -2, while setting the world\_size, or number of GPUs in use, from 2 to 1. The script for 100 hours runs for 80k updates, this and all other parameters are left unchanged. For finetuning to 10 hour datasets, we use the 10 hour finetuning script for LibriSpeech \footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_10h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_10h.yaml}}. We make the same changes to max\_tokens, update\_freq and world\_size. All other parameters were left unchanged.


When finetuning our models on 100 hours of data, we follow the Fairseq LibriSpeech finetuning configuration and apply it to all datasets. We modify the configuration to run on a single GPU, while leaving all other parameters unchanged.\footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_100h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_100h.yaml}} Concretely, we double \texttt{max\_tokens} from 3{,}200k to 6{,}400k and adjust the gradient accumulation setting so that, on a single GPU, the effective batch size matches the original 2-GPU setup, while setting \texttt{distributed\_world\_size} (number of GPUs) from 2 to 1. The 100-hour finetuning script runs for 80k updates, and this setting, along with all other hyperparameters, is left unchanged.

For finetuning on 10-hour datasets, we use the corresponding LibriSpeech 10-hour finetuning configuration.\footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_10h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_10h.yaml}} We make the same modifications to \texttt{max\_tokens}, the gradient accumulation setting, and \texttt{distributed\_world\_size} as above, and leave all other parameters unchanged.

Once we have the results of our finetuning experiments, we can assess how pretraining language balance influences downstream ASR performance. Based on previous work on multilingual SSL ASR \cite{ashihara2023exploration, poncelet2021comparison,zhang2023fast, gupta2021clsril}, we expect matched-language pretraining to give lower error rates than fully mismatched pretraining. We also expect high-resource matched pretraining to outperform low-resource matched and no-match conditions, with mixed pretraining conditions falling between these extremes.

Throughout this section, we visualise finetuning results with line graphs of CER by model and language. For multilingual evaluation, we plot bar charts of the CER difference between English and Spanish monolingual models. The bar charts indicate whether each test language benefits more from English or from Spanish pretraining. The metrics used in these experiments, including how the CER difference is calculated, are described in Section \ref{ed_subsec:metrics}.

Pasad et al. show that SSL pretraining does not learn semantic or lexical tasks and instead learns acoustic and phonetic structure in the lower and middle layers \cite{pasad2021layer}. If our predictions about the impact of pretraining language balance on finetuned ASR are correct, then these acoustic and phonetic representations have a strong effect on downstream performance. We therefore need to analyse the pretrained models directly, rather than relying only on word-level error after finetuning. Prior work shows that pretrained SSL ASR models can support accurate phoneme recognition when probed with simple classifiers \cite{english2022domain}. We use phoneme-level probing to measure which phoneme groups are learned best under different pretraining language balances. We also use it to test how matched, low-resource, and mismatched pretraining conditions affect the quality of the learned phonetic representations.

\subsection{Forced Alignment and Phoneme Labels}

To evaluate phoneme accuracy in our pretrained models, we require frame-level phoneme labels aligned to the speech signal. We obtain these labels using the Montreal Forced Aligner (MFA) \cite{mcauliffe2017montreal}. As outlined in Section~\ref{DATASET_SECTION_IN_LR}\info{haven't written yet this will be in the literature Review}, the FLEURS transcripts are not text-normalised and contain symbols and numbers, which makes them unsuitable for reliable forced alignment. We therefore restrict our phoneme-level analyses to LibriSpeech \cite{librispeech} and the Multilingual LibriSpeech (MLS) corpora, which provide text-normalised transcripts.

For probing, we apply MFA to 10-hour subsets of LibriSpeech, MLS Spanish, MLS French, and MLS Polish, compiled using the same gender-balanced procedure as in Section~\ref{ed_seubsec:exp_des_probing}. We use the MFA pronunciation dictionaries \texttt{english\_us\_mfa}, \texttt{spanish\_spain\_mfa}, \texttt{french\_mfa}, and \texttt{polish\_mfa}, each of which maps words to their phoneme sequences. Each 10-hour subset contains some out-of-vocabulary (OOV) words not covered by the default dictionaries: 0.75\% of words are OOV for LibriSpeech, 7.7\% for MLS Spanish, 7.0\% for MLS French, and 4.0\% for MLS Polish.

To handle OOV items, we use MFA Grapheme-to-Phoneme (G2P) models\footnote{\href{https://mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}{mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}} to generate pronunciations and merge these with the original dictionaries to form custom pronunciation lexicons. These custom lexicons are then used by MFA to force-align each 10-hour subset and produce frame-level phoneme labels for probe training and evaluation.

We also run MFA on the full 960-hour LibriSpeech training set and the 920-hour MLS Spanish training set in order to analyse the phonetic composition of the pretraining data. In these full training sets, 0.88\% of LibriSpeech words and 8.63\% of MLS Spanish words are OOV, which is close to the OOV rates observed in the corresponding 10-hour subsets. This indicates that our 10-hour subsets are representative of the full corpora with respect to vocabulary coverage and alignment conditions.

%\newpage
\subsection{Layer Probing}\label{ed_seubsec:exp_des_probing}
We follow English et al. \cite{english2022domain} and train layer-wise probes to estimate phoneme accuracy from wav2vec~2.0 representations. Each probe is a two-layer feed-forward network that takes as input the hidden states from a single transformer layer of the wav2vec~2.0 encoder. The input size to each probe is 768, the dimension of each transformer block, and predicts a phoneme label for each frame. During probe training the wav2vec~2.0 encoder is kept frozen, and probes are trained with the Adam optimiser, a learning rate of \(1e-3\) and using cross-entropy loss.

To construct training data for the probes, we segment each utterance into windows and extract frame-level features using the wav2vec~2.0 feature encoder. These features are paired with the aligned phoneme label for each frame and collated into a dataloader. For each pretrained model, we pass the features through the encoder and record the output of each of the 12 transformer layers. We then train 12 separate probes per model, one for each layer, to predict phoneme labels for each frame.

We experimented with training probes on both a 10-hour subset and a 100-hour subset of LibriSpeech. In both cases, probes begin to overfit after five epochs, so we fix the number of training epochs to five. Probes trained on the 100-hour subset achieve higher absolute phoneme accuracy but require substantially longer training time. Probes trained on the 10-hour subset converge much faster, and although their absolute accuracy is lower, the relative accuracy between different phoneme categories is unchanged. As a compromise, we therefore use 10-hour subsets of each corpus to train all probes.

\begin{table}[h]
\centering
\begin{tabular}{l|cc|cc}
\toprule
\textbf{Pretraining Hours:} & \multicolumn{2}{c}{\textbf{English}} & \multicolumn{2}{c}{\textbf{Spanish}} \\
\textbf{LibriSpeech /} & & & & \\
\textbf{MLS Spanish } &  \textbf{Layer } & \textbf{Acc } & \textbf{Layer } & \textbf{Acc } \\
\midrule
960 / 0 & Layer 7 & 62.4\% & Layer 5 & 57.5\% \\
800 / 100 & Layer 7 & 62.4\% & Layer 7 & 61.0\% \\
600 / 300 & Layer 7 & 61.5\% & Layer 5 & 61.6\% \\
450 / 450 & Layer 7 & 60.1\% & Layer 7 & 62.2\% \\
300 / 600 & Layer 7 & 58.4\% & Layer 5 & 60.0\% \\
100 / 800 & Layer 7 & 56.5\% & Layer 7 & 62.4\% \\
0 / 920 & Layer 7 & 46.1\% & Layer 5 & 62.2\% \\
\bottomrule
\end{tabular}
\caption{Layers with the highest average accuracy after probing and the phoneme accuracy found on this layer for both English and Spanish. The wav2vec 2.0 encoder has 12 transformer layers, layer 0 to 11.}
\label{ed_tab:best_layer_average_accuracy}
\end{table}

%\paragraph{Best-performing layers.}
Table \ref{ed_tab:best_layer_average_accuracy} shows, for each pretrained model, the encoder layer with the highest mean phoneme accuracy on English and Spanish, respectively. For English, the highest-accuracy layer is most often layer~7, with layer~6 as the second most frequent. For Spanish, layer~5 is most often best, followed by layer~7. This pattern is consistent with previous work, which also finds peak phoneme recognition performance in middle layers of wav2vec~2.0 \cite{pasad2021layer,english2022domain}. We therefore chose to report results from layer 7 for all phoneme accuracy results across all languages and models.

\subsection{Probing Language-Membership Phoneme Groups}\label{ed_subsec:lang_membership_groups}

Our first set of probing experiments looks at how pretraining language balance affects phoneme recognition. We look at phonemes that English and Spanish share, and phonemes that are specific to one of the two languages. We obtain these groups by comparing the phoneme symbol lists in the English and Spanish MFA pronunciation dictionaries: the intersection gives shared phonemes, and language‑specific symbols form the English‑unique and Spanish‑unique sets. The resulting groups are:

\begin{itemize}
    \item \textbf{English-Shared}: phonemes that are in both the English and Spanish dictionaries, evaluated on English speech.
    \item \textbf{Spanish-Shared}: phonemes that are in both the English and Spanish dictionaries, evaluated on Spanish speech.
    \item \textbf{English-unique}: phonemes that are in the English dictionaries but not in the Spanish dictionary, evaluated on English speech.
    \item \textbf{Spanish-unique}: phonemes that are in the Spanish dictionaries but not in the English dictionary, evaluated on Spanish speech.
\end{itemize}
\thoughts{I will have to remember to regenerate the graphs for this section with this new naming convention}
\begin{table}[h]
    \centering
    \begin{tabular}{l r r r r}
        \hline
        Set & Total & Shared (En+Es) & English-unique & Spanish-unique \\
        \hline
        Pretraining (En) & 80
                         & 26 & 54 & -- \\
        Pretraining (Es) & 36
                         & 26 & --            & 10 \\
        Probe train (En) & 80
                         & 26 & 54 & -- \\
        Probe test (En)  & 77 
                         & 26 & 51 & -- \\
        Probe train (Es) & 36
                         & 26 & -- & 10 \\
        Probe test (Es)  & 36
                         & 26  & --               & 10 \\
        \hline
    \end{tabular}
    \caption{Number of phoneme types per set and per group in the English and Spanish pretraining corpora and probe train/test sets. “Total” counts all phoneme types observed in that set; the remaining columns show how these totals divide into shared and language-unique groups.}
    \label{ed_tab:phoneme_group_sizes_en_es}
\end{table}

In the next step, we extend this analysis to French and Polish. For these languages, we first build a single reference inventory that reflects everything the model could have seen during pretraining. We refer to these groups as French-Shared for phonemes in the reference inventory and French and Polish-Shared for phonemes in the reference inventory and in Polish. We build this inventory by taking the union of the English and Spanish pretraining phoneme sets. This gives us one combined set containing all phonemes that appear in either English or Spanish pretraining data. This is different from the shared and language‑unique groups defined above, which separate phonemes into “in both languages” and “in only one language”. Here, we only care whether a phoneme was present in at least one of the pretraining languages.

Using this combined English+Spanish inventory, we then define two types of groups for French and Polish. The first type captures phonemes that are already familiar from pretraining:
\begin{itemize}
    \item  \textbf{French-Shared}: phonemes that appear both in the combined English+Spanish inventory and in the French inventory, evaluated on French speech.
    \item  \textbf{Polish-Shared}: phonemes that appear both in the combined English+Spanish inventory and in the Polish inventory, evaluated on Polish speech.
\end{itemize}
The second type captures phonemes that are absent from the English and Spanish pretraining data:
\begin{itemize}
    \item \textbf{French-unique}: phonemes that appear in the French inventory but in neither the English nor Spanish pretraining inventories, evaluated on French speech.
    \item \textbf{Polish-unique}: phonemes that appear in the Polish inventory but in neither the English nor Spanish pretraining inventories, evaluated on Polish speech.
\end{itemize}


\begin{table}[h]
    \centering
    \begin{tabular}{l r r r r}
        \hline
        Set & Total & Overlap with En/Es & French-unique & Polish-unique \\
        \hline
        Probe train (Fr) & 45
                         & 35  & 10  & -- \\
        Probe test (Fr)  & 45
                         & 35  & 10  & -- \\
        Probe train (Pl) & 51
                         & 32  & --  & 19 \\
        Probe test (Pl)  & 51
                         & 32  & --  & 19 \\
        \hline
    \end{tabular}
    \caption{Number of phoneme types per group in the French and Polish probe train/test sets. “Overlap with En/Es” counts phonemes that occur both in the combined English+Spanish pretraining inventory and in the French or Polish inventory; “French-unique” and “Polish-unique” count phonemes that occur only in the French or Polish inventory and not in the combined English+Spanish inventory.}
    \label{tab:phoneme_group_sizes_fr_pl}
\end{table}

Tables \ref{ed_tab:phoneme_group_sizes_en_es} and \ref{tab:phoneme_group_sizes_fr_pl} show that English has a much larger MFA phoneme inventory than Spanish, 80 vs 36, and that the English‑unique and Spanish‑unique groups are therefore very different in size. Similarly, for French and Polish, the overlap groups are substantially larger than the language‑unique groups. As a result, group‑level accuracies are averaged over different numbers of phonemes, and smaller groups such as Spanish‑unique, French‑unique and Polish‑unique are more sensitive to the behaviour of individual phonemes, which is important to keep in mind when interpreting the probe results in later sections.

These experiments show how pretraining language balance affects recognition of phonemes defined by their presence or absence in the pretraining languages. However, they still treat all phonemes within each group as if they behaved in the same way. The next subsection therefore focuses on manner-of-articulation groups, using the findings of English et al. as motivation for analysing these classes separately \cite{english2022domain}; see Section \ref{bac_subsec:layer_probing_articulation} for background. This allows us to test whether pretraining language balance has class-specific effects on phoneme accuracy. We also examine how these effects play out in languages where particular manners of articulation are more frequent or occur only in one language. \thoughts{this isn't a good explanation, the first paragraph of the next section does this better!}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\newpage

\subsection{Probing Phonemes by Articulatory Class}

\hl{DO NOT FORGET DELPHINE'S NOTES}

As seen in the Section \ref{ed_subsec:lang_membership_groups}, by grouping phonemes only into shared vs non‑shared sets we are comparing groups that differ greatly in how many total phonemes they contain. To analyse phoneme accuracy more precisely, we can look to dividing them into manner of articulation (MoA) classes. MoA classes provide widely used articulatory categories for grouping phonemes across languages \cite{international1999handbook}. The total number of phonemes in each of these categories is lower than the shared/non‑shared sets, giving more precision. The phoneme inventories are also more comparable in size between languages, allowing for more balanced cross-lingual comparison.\delphinenote{Comparisons. The plural would be more appropriate.} In addition, they allow analysis of some language‑specific categories that may have been obscured in the previous groupings. 

While most MoA classes follow standard articulatory categories, some of the groups in our experiments require additional explanation. We have separated diphthongs from monophthongs in the vowel category because they only occur in English in our data. Dipthongs are sounds formed by combining two vowels, so separating them may  also give some insight into coarticulation in mixed-language pretraining. Closures correspond to the \texttt{sil} token, which signifies silence in MFA, and are therefore treated as their own group rather than merged. 

The Tap \& Trill class\delphinenote{One concern I have is the grouping of tap and trill into a single “Tap \& Trill” category. These correspond to distinct manners of articulation, so it might be worth briefly justifying this choice here. Otherwise, reviewers may question the decision to merge them.} is present in English, Spanish and Polish. However, unlike English and Polish, Spanish has more than one phoneme in this group and has far more examples of both \textipa{r} and  \textipa{R} in the training data as seen in Table \ref{ed_tab:phoneme_token_class_counts_m}. While this category is not language-specific, it is heavily weighted towards Spanish, which may affect the final accuracy of phonemes in this category.\iffalse Due to this, we separated out phonemes from the wider sonorants group into vowels, nasals, approximants and Taps \& Trills. This allows us to observe the language-specific behaviour of all these groups separately.\fi The number of occurrences of every available phoneme across the training data for all four languages tested can be found in Appendix \ref{app_sec:chapter_5_appendix}.

Table \ref{ed_tab:phoneme_groups_by_language} summarises the phonemes included in each MoA class for the four languages in our experiments: English, Spanish, French and Polish. The datasets used in this section are outlined in Section \ref{ed_subsec:datsets}, and, as in Section \ref{ed_subsec:lang_membership_groups}, we use the same phoneme‑level accuracy data for these four languages.
\newpage
The absolute number of phoneme tokens in the pretraining data for each MoA class for each of our models is shown in Table \ref{ed_tab:phoneme_token_class_counts_m}. MoA classes such as vowels, stops and approximants are well represented in all languages in both the pretraining and test data. Other classes, such as Taps \& Trills and Diphthongs, are language-specific or have much higher representation in a single pretraining language. These MoA groupings, therefore, define classes that we might expect to respond differently depending on how many examples were included in the pretraining data. On this basis, we can define several hypotheses for the experiments in this section.


We test the following hypotheses:

\begin{itemize}
    \item \textbf{Data quantity effect:} MoA classes with more examples in the pretraining data will show higher accuracy for that class across all test languages.
    
    \item \textbf{Seen-language trade-off:} When one language is high-resource and contains many examples of a MoA class in pretraining, language-specific examples in that class will improve accuracy for the matching test language. This will, however, reduce accuracy for unmatched-languages in the same class, when compared to the matched-language case.
    
    \item \textbf{Unseen-language impact:} For test languages unseen in pretraining, a higher overall number of examples for a MoA class in pretraining is expected to improve phoneme accuracy. We expect this to be true even when those examples come from multiple languages in pretraining, such as the 450 / 450 model. If this is found to be true, this will suggest that monolingual pretraining offers limited additional benefit for unseen languages.
\end{itemize}


\begin{table}
\centering
\small
\begin{tabular}{l|ccccccc}
\toprule
Phoneme Class & 960 / 0 & 800 / 100 & 600 / 300 & 450 / 450 & 300 / 600 & 100 / 800 & 0 / 920 \\
\midrule
Vowel        & 10.2 & 10.1 & 11.2 & 12.0 & 12.9 & 13.7 & 14.8 \\
Diphthong    &  2.0 &  1.6 &  1.2 &  0.9 &  0.6 &  0.2 &  0.0 \\
Stop         &  5.8 &  5.4 &  5.2 &  5.1 &  4.9 &  4.7 &  4.8 \\
Nasal        &  3.7 &  3.5 &  3.4 &  3.4 &  3.4 &  3.3 &  3.4 \\
Fricative    &  4.6 &  4.4 &  4.6 &  4.7 &  4.8 &  4.9 &  5.1 \\
Affricate    &  0.4 &  0.3 &  0.3 &  0.2 &  0.2 &  0.1 &  0.1 \\
Approximant  &  3.6 &  3.3 &  3.1 &  3.0 &  2.9 &  2.6 &  2.6 \\
Closure      &  2.1 &  2.0 &  2.0 &  2.0 &  2.0 &  2.0 &  2.1 \\
Tap \& Trill &  0.4 &  0.6 &  1.0 &  1.3 &  1.6 &  1.9 &  2.2 \\
\addlinespace
\textbf{Total} & 32.8 & 31.1 & 31.9 & 32.5 & 33.1 & 33.4 & 35.0 \\
\bottomrule
\end{tabular}
\caption{Phoneme token counts (in millions, rounded to one decimal) grouped by phoneme class and model. For more precision, counts rounded to the closest thousand are provided in Appendix \ref{app_tab:phoneme_token_class_counts_k}.}

\label{ed_tab:phoneme_token_class_counts_m}
\end{table}



\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
& Vowel & \textipa{i, {\ae}, 5, A, 6, @, E, \textrevepsilon, \i, u, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g, P} \\
& Nasal & \textipa{m, n, N, M, \textltailn} \\
& Fricative & \textipa{f, h, s, v, z, c, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c, k, p, \j, \j J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, c, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, O, \textschwa, E, \oe} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L, 4} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, E} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g, P} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, x, z, c, C, s} \\
%& Affricate & \textipa{dz, tc, ts} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, t\c{c}, t\:s} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Phonemes per language grouped by articulatory class. This table lists one base symbol per MFA phoneme; a full inventory including all phonetic variants used in our experiments is given in Appendix~\ref{app_sec:chapter_5_appendix}, Table~\ref{app_tab:phoneme_groups_by_language}.}
\label{ed_tab:phoneme_groups_by_language}
\end{table}

\newpage
\hl{Delphine's Note's for phoneme table above}

\begin{itemize}
    \item For whole Table:
    \begin{itemize}
        \item It might also improve readability to list the phonemes following the conventional IPA order (by place of articulation). That’s the format typically used in phonological descriptions :) for example, stop: p b t d c J k g glottal
        \item Ed's note: the order can be found in the IPA handbook. For vowels we read right to left e.g. Close front, Close Central, Close Back, Close-Mid front, Close-mid Central ........  Open Front, Open Back.
    \end{itemize}
    \item English - Fricative: \textipa{c}
    \begin{itemize}
        \item this is a stop, not a fricative
    \end{itemize}
    \item English - Closure
    \begin{itemize}
        \item It might be helpful to briefly explain in the text what “Closure” refers to. I had to think for a moment to recall what it represents and how it fits into the table.
        \item \hl{This has been addressed, there is a line in the subsection that covers this}
    \end{itemize}
    \item French - Vowel - \textipa{\ae}
    \begin{itemize}
        \item not a French phoneme
    \end{itemize}
    \item French - Stop - All phonemes:
    \begin{itemize}
        \item in French, only p b t d k g
    \end{itemize}
    \item French - Affricate - All phonemes:
    \begin{itemize}
        \item In standard French, affricates such as /\textipa{tS}/ and /\textipa{dZ}/ are not considered native phonemes. They mainly occur in loanwords and are typically analyzed as consonant clusters (e.g., /\textipa{t}/ + /\textipa{S}/) rather than independent phonemic affricates.
    \end{itemize}
    \item French - Approximant - \textipa{L}
    \begin{itemize}
        \item Not a French phoneme
    \end{itemize}
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Metrics and Evaluation Criteria}\label{ed_subsec:metrics}
Here we list all of the metrics used to track loss and evaluation during and after pretraining, finetuning and probe training. During SSL pretraining we monitor progress using contrastive loss, and every 10k updates we briefly finetune and evaluate Word Error Rate (WER), following the fairseq pretraining tutorial evaluation protocol. For all finetuning experiments we train using Connectionist Temporal Classification (CTC) loss and evaluate after training using Character Error Rate (CER), to remain consistent with the FLEURs finetuning experiments in Chapter~\ref{chapter4}. For the layer-wise probes we train each probe using cross-entropy loss and evaluate after training using the mean average phoneme accuracy, to remain consistent with previous work \cite{english2022domain}.


\begin{table}[h]
\centering
\begin{tabular}{l|l|l|}
\cline{2-3}
                                  & \multicolumn{1}{c|}{Loss During Training} & \multicolumn{1}{c|}{Evaluation After Training} \\ \hline
\multicolumn{1}{|l|}{Pretraining} & Contrastive Loss                          & WER                                            \\ \hline
\multicolumn{1}{|l|}{Finetuning}  & CTC Loss                                  & CER                                            \\ \hline
\multicolumn{1}{|l|}{Probing}     & Cross Entropy Loss                        & Accuracy                                       \\ \hline
\end{tabular}
\caption{Loss functions for each stage of training and evaluation metrics used after training.}
\end{table}

In the finetuning experiments outlined in Section \ref{ed_subsec:finetuning}, results are shown as line graphs and bar charts. For English and Spanish ASR, we plot CER for each model across the different pretraining conditions as line graphs. For the multilingual FLEURs finetuning, we plot the difference in CER between the English and Spanish monolingual models for each test language as a bar chart. The difference between CER for each language \(L\) is calculated with the following equation:
\begin{equation}
\Delta \mathrm{CER}(L) = \mathrm{CER}_{\text{Eng}}(L) - \mathrm{CER}_{\text{Spa}}(L).
\label{eq:delta_cer}
\end{equation}
Positive values of \(\Delta \mathrm{CER}(L)\) indicate higher CER for the English-pretrained model, and negative values indicate higher CER for the Spanish-pretrained model. The plots show whether performance is better when the model is pretrained on Spanish or on English for each language \(L\), and by how much.





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Experimental Results}\label{ed_sec:experimental_results}

This section presents the results from the experiments outlined in Section \ref{ed_sec:experimental_design}. It covers both ASR finetuning and phoneme probing to show how English and Spanish pretraining balance affects multilingual performance and learned speech representations. We first report CERs for matched, low‑resource, and mismatched finetuning conditions. This allows us to quantify the downstream impact of incrementally introducing mismatched language data into pretraining. We then analyse phoneme accuracy of the pretrained models before finetuning, testing shared vs non‑shared phonemes and phonemes across MoA classes to identify which phonetic groups benefit most from different pretraining mixtures. Together, these experiments reveal when high‑resource or monolingual pretraining offers advantages over balanced or moderately unbalanced bilingual settings.



\subsection{Effect of Pretraining Composition on Finetuning}\label{ed_subsec:same_data_finetuning}
We begin by evaluating matched-language finetuning for each of the pretrained models introduced in Section~\ref{ed_pretraining_configuration}. In this subsection, all models are finetuned on 100~hours of labelled speech drawn from the same corpora used during pretraining. For English, we use LibriSpeech \cite{librispeech}, and for Spanish we use MLS Spanish \cite{MLS}; these datasets are covered in more detail in Section~\ref{ed_subsec:datsets}. This setup mirrors the original wav2vec~2.0 configuration, where a model is pretrained on 960~hours of LibriSpeech and then finetuned on smaller LibriSpeech subsets \cite{wav2vec}. Here we extend that idea to all of our English–Spanish pretraining monolingual and mixed‑language settings. We compare how different pretraining compositions affect downstream CER under otherwise comparable finetuning conditions. For more detailed information on our finetuning setup, see Section~\ref{ed_subsec:finetuning}.

Figure~\ref{ed_fig:fine_tuning_100hours} shows the results of this matched-language finetuning experiment. It shows the CER obtained after finetuning each pretrained model on 100~hours of LibriSpeech or 100~hours of MLS Spanish. The x-axis lists the pretraining composition in hours of English and Spanish, and the y-axis shows the resulting CER. Separate lines indicate finetuning on English and on Spanish, allowing us to compare how each pretraining mixture behaves when finetuned to 100~hours of matched-language data.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_engspamls100h.pdf}
    \caption{CER when models pretrained with varying ratios of English and Spanish data are finetuned to 100 hours of English or 100 hours of Spanish data }
    \label{ed_fig:fine_tuning_100hours}
\end{figure}%\label{fine_tuning_100hours}

We see in Figure~\ref{ed_fig:fine_tuning_100hours} that, for LibriSpeech finetuning, CER is lowest for the English monolingual model and highest for the Spanish monolingual model. Between these models, we see the following: 800/100 has higher CER than 960/0, 600/300 is higher than 800/100, 450/450 is higher than 600/300, 300/600 is higher than 450/450, and 100/800 is higher than 300/600. This is a gradual CER increase with each step from left to right, as English-matched pretraining is replaced by more Spanish-mismatched pretraining. Each increase in mismatched Spanish data during pretraining raises CER on English.

For MLS Spanish finetuning, a similar gradual change in CER appears in the opposite direction. CER decreases consistently as we move from English high-resource pretraining towards Spanish high-resource pretraining. The highest CER is on the 800/100 model, followed by slightly lower CER on the English monolingual model, and the lowest CER on the 100/800 and Spanish monolingual models. The lowest and highest Spanish CER values therefore occur at 100/800 and 800/100 rather than at the monolingual models, unlike English. However, both languages follow the same broader high-resource pattern. CER is lowest when pretraining is high-resource and matched to the finetuning language, and highest when pretraining is high-resource and mismatched to the finetuning language.

With this experiment, we have confirmed that finetuning CER is lowest on high-resource matched-language models and highest on high-resource mismatched-language models, as seen in previous research.\thoughts{reference needed?} However, we have also shown that intermediate mixed-language models exhibit a gradual change in CER that varies in proportion to how much of the finetuning language they have seen during pretraining. CER decreases with more matched-language pretraining and increases with more mismatched-language pretraining. 

All the finetuning data in this subsection come from the same datasets used for pretraining.  Previous work has shown that related-language pretraining, even mixed multilingual related-language pretraining,\thoughts{do I need to add references?} can decrease CER on related unseen languages when compared to mismatched-language pretraining. For our experiments, it is not clear how our mixed-language models will behave on related and unrelated unseen languages. Neither is it currently clear whether the gradual change in CER will remain consistent in these conditions. Therefore, in Section~\ref{ed_subsec:gain_across_languages}, we test our models on unseen multilingual FLEURs~\cite{FLEURS} data.


%\newpage
\subsection{Multilingual and Cross Lingual Transfer Gains}\label{ed_subsec:gain_across_languages}




In this subsection, we extend our analysis to unseen data and unseen languages using the FLEUR dataset \cite{FLEURS}. We finetune each of our pretrained models, as outlined in Section \ref{ed_subsec:datsets}, on 10~hours of English and 10~hours of Spanish from FLEURs. Then, we continue on to additional FLEURs languages from Germanic, Romance, and Slavic language families. This measures how the English and Spanish pretraining balance affects multilingual and cross-lingual transfer of CER.

Figure~\ref{ed_fig:fleurs_engspa_linegraph} shows the CER obtained after finetuning each pretrained model on 10~hours of FLEURs English data and 10~hours of FLEURs Spanish data. We use the same axis as in Figure~\ref{ed_fig:fine_tuning_100hours}. Similarly to the previous seen data experiment, CER on English is lowest for high-resource English pretraining and CER on Spanish is lowest for high-resource Spanish pretraining. There is again a gradual shift in CER across the intermediate models as we move away from high-resource English pretraining. The gradual shift happens through moderately unbalanced and balanced-unrelated models, to high-resource Spanish pretraining. This again indicates that matched-language pretraining produces lower CER as seen in Section \ref{ed_subsec:same_data_finetuning}. However, we can now observe that even when the finetuning data is from unseen FLEURs data, the same relation between finetuning and pretraining data still exists.

\begin{figure}%[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_engspafleurs.pdf}
    \caption{CER after finetuning each model on 10 hours each of unseen FLEURS English and Spanish.}
    \label{ed_fig:fleurs_engspa_linegraph}
\end{figure}

Prior work on multilingual ASR and cross-lingual transfer shows that pretraining on related languages produces lower finetuned CER than pretraining on unrelated languages~\cite{prasad-etal-2021-investigating, singh2020effects}\thoughts{update these references to be consistent wit the background section}. In our experiments so far, we have seen that increasing the ratio of a related language in pretraining to the finetuning language produces a gradual decrease in CER. Next, we must test whether the ratio of related-language pretraining affects finetuned CER. We will explore whether any shift in CER is still gradual or how much of an impact the ratio has when the pretraining language is related, but not matching, the finetuning language.\thoughts{this para could probably be shortened}

%%% Shortened verion of the previous paragraph for if I want it later
\iffalse
"Prior multilingual ASR work shows related-language pretraining lowers CER more than unrelated~\cite{prasad-etal-2021-investigating, singh2020effects}. Our results confirm a gradual CER decrease with increasing related-language ratio; next we test non-matching related ratios."
\fi

Figure~\ref{ed_fig:fleurs_multilingual_linegraph} shows the CER for all of these languages across the range of English and Spanish pretraining balances. For many languages, we again see a gradual shift in CER across the intermediate mixed-language models between the high-resource English and high-resource Spanish models. Germanic languages have lower CER on the high-resource English models when compared to their CER on the high-resource Spanish models. Most of the Romance languages have lower CER on the high-resource Spanish models relative to their CER on the high-resource English models. French and the unrelated Slavic languages do not, however, appear to fit these family-level patterns. \thoughts{I could cut down on the explanation of the graphs}

\begin{figure}%[H]
    \centering
    \includegraphics[width=\linewidth]{appendix/figures/linegraph_allfleurslangs.pdf}
    \caption{CER after finetuning each model on 10 hours of unseen FLEURS languages from Germanic, Romance, and Slavic families\hl{THIS FIGURE NEEDS REGENERATING WITH BETTER LANGAUGE NAMES AND NO ENG+SPA}}
    \label{ed_fig:fleurs_multilingual_linegraph}
\end{figure}

In Figure~\ref{ed_fig:fleurs_multilingual_linegraph}, some languages show much lower CER across all models than other languages, and the lines are densely packed on the graph. This makes it difficult to ascertain which languages benefit most from English or from Spanish pretraining. To make these cross-lingual preferences clearer, we look to Figure~\ref{ed_fig:multilingual_barchart}. It shows the difference between the CER for each language on the English and Spanish monolingual models.



Figure~\ref{ed_fig:multilingual_barchart} makes these cross-lingual effects more explicit. We plot the signed difference in CER between the English and Spanish monolingual models for each FLEURs language. Languages with negative values have lower CER on the Spanish monolingual model, while those with positive values have lower CER on the English monolingual model. Here, English shows the largest positive difference and Spanish shows the largest negative difference when compared to the other languages. This reflects that the gap in CER between the finetuned monolingual models is significantly bigger for matched-languages (English and Spanish), when compared to related and unrelated languages. \thoughts{I could cut down on the explanation of the graphs to save space}

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/barchart_abs_diff.pdf}
    \caption{CER difference (English monolingual minus Spanish monolingual) after finetuning on each FLEURS language.}
    \label{ed_fig:multilingual_barchart}
\end{figure}

The Germanic languages, by comparison, cluster clearly on the English side, showing a positive CER difference with English pretraining. The Romance languages mostly show negative differences, having lower CER for Spanish pretraining. Both Germanic and Romance languages have lower differences than English and higher differences than Spanish. This shows that related language pretraining does not benefit finetuning to the extent that matched language pretraining does. French is an outlier among Romance languages with a positive difference. It is a smaller positive difference than for either of the Germanic languages, though, which may suggest weaker affiliation with English.\thoughts{Maybe I need to justify this better linguistically and/or suggest some sort of tie with the probing experiment} The Slavic languages are close to zero, with Russian having a negative difference, while Polish has a positive difference. This may indicate little consistent advantage from either pretraining language when finetuning on unrelated languages.

Across these multilingual FLEURs experiments, we see a clear pattern for monolingual pretraining. Matched-language pretraining produces the lowest CER. Related-language pretraining gives low CER but with an increase over matched-language pretraining. Unrelated-language pretraining has no clear advantage. For mixed-language pretraining, we show a gradual shift in CER across the mixed English and Spanish models in Germanic and Romance languages. This shows that downstream error depends directly on how much related-language data is included in pretraining. Even small amounts of matched or related data can decrease CER when compared to monolingual unrelated-pretraining. However, we also find that incrementally replacing related data with an unrelated language steadily increases CER for any matched and related finetuning languages.

The influence of SSL pretraining on finetuned CER has been shown to be extensive in this section. However, SSL pretraining learns acoustic and phonetic structure rather than lexical or semantic information, since no word or character level data is included in pretraining~\cite{pasad2021layer, english2022domain}. These patterns in CER after finetuning must therefore be driven by phonetic information learned during pretraining. To further explore how SSL ASR models learn multilingual information, we move from CER to phoneme-level probing. In the next subsection, we examine how pretraining language balance affects phoneme accuracy for related and unrelated phoneme sets. We split these phonemes into groups that appear in both pretraining languages and those unique to each.

\hl{there is probably some redundancy in this subsection that could be removed}

\newpage
%\newpage
\subsection{Shared vs. Unique Phoneme Accuracy} \label{ed_sec:shared_vs_not_shared}

\iffalse
1. Short recap and aim: Link language-membership probes to prior CER results and state goal of comparing accuracy across pretraining mixtures and languages.

2. English results: Report accuracy patterns for English-Shared vs English-unique across pretraining mixtures.

3. Spanish results: Report accuracy patterns for Spanish-Shared vs Spanish-unique across pretraining mixtures.

4. English/Spanish interpretation: Connect shared vs unique accuracy to pretraining exposure and CER patterns.

5. French results: Report accuracy patterns for Overlap with En/Es (Fr) vs French-unique across pretraining mixtures.

6. Polish results: Report accuracy patterns for Overlap with En/Es (Pl) vs Polish-unique across pretraining mixtures.

7. French/Polish interpretation: Link overlap vs unique accuracy to "seen vs unseen" phonemes and FLEURS family patterns.

8. Cross-language summary: Synthesise shared/overlap robustness vs unique sensitivity across all four languages.

9. Transition to MoA: Note limitations of language-membership grouping and introduce manner-of-articulation analysis.
\fi

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_shared_not_shared_eng_spa.pdf}
    \caption{Mean phoneme accuracy at layer~7 for English-Shared, English-Unique, Spanish-Shared, and Spanish-Unique groups across English and Spanish pretraining mixtures. Shared groups contain phonemes that exist in both English and Spanish. English-Unique contains only phonemes unique to English and Spanish-Unique contains only phonemes unique to Spanish.}
    \label{ed_fig:phonemes_shared_not_shared}
\end{figure}


Following the multilingual FLEURs finetuning results (Section~\ref{ed_subsec:gain_across_languages}), we now examine phoneme-level representations in the pretrained models. We extract phoneme information using the MFA \cite{mcauliffe2017montreal} and train linear probes for phoneme recognition on the layer 7 outputs of each of our models, as detailed in Section~\ref{ed_seubsec:exp_des_probing}. These probes evaluate phonemes from English, Spanish, French, and Polish test data across all English and Spanish pretraining mixtures.  

Figure~\ref{ed_fig:phonemes_shared_not_shared} shows phoneme accuracy for English-Shared and English-unique groups across pretraining mixtures. English-Shared exhibits a similar high-resource pattern seen in finetuning in Section~\ref{ed_subsec:same_data_finetuning}. The highest accuracy is on the high-resource English models, a gradual decrease across mixed-language models, then the lowest accuracy on high-resource Spanish models. English-Shared accuracy drops 19.24\% from English monolingual to Spanish monolingual, whereas English-unique accuracy shows a similar 21\% drop between the same models. 

English-unique phonemes have a high average difference in accuracy of 15.34\% across all models when compared to English-Shared. Unlike in previous experiments, accuracy for English-Unique on the 300/600 model falls below the accuracy on the 100/800 model, but does remain above the Spanish monolingual.\thoughts{talk of anomalous results could be moved to discussion} The accuracy for English-Unique does otherwise show a gradual-decline in accuracy from high-resource English models, across the other mixed-language models, to the monolingual Spanish model.
%% linegraph values for English from monolingual en to monolingual spa
%% Shared: 69.67, 65.84, 66.83, 66.07, 62.09, 60.04, 50.43
%% Unique: 54.25, 53.38, 53.02, 51.16, 43.45, 45.09, 33.26
%% difference:   15.42, 12.46, 13.81, 14.91, 18.64, 14.95, 17.17
%% average:    15.34

Figure~\ref{ed_fig:phonemes_shared_not_shared} also shows phoneme accuracy for Spanish-shared and Spanish-unique groups across pretraining mixtures. Spanish-shared accuracy still shows an increase from English high-resource to Spanish high-resource models. However, the increase is less gradual here, plateauing after the high-resource English models, except for a drop on the 300/600 model.\thoughts{talk of anomalous results could be moved to discussion} Spanish-unique shows a clearer gradual increase in accuracy across all mixtures. However, it has a drop in accuracy on the 600/300 model that doesn't fit the overall gradual increase across other models.\thoughts{talk of anomalous results could be moved to discussion} Spanish-unique phonemes have consistently lower accuracy, with a large average difference of 18.7\%, across all models compared to Spanish-shared. Finally, Spanish-Shared shows a difference of 5.32\% between the Spanish-monolingual matched-language and English-monolingual mismatched models. Spanish-Unique has a much larger difference of 14.07\% difference in accuracy between monolingual models. 


%% linegraph values for Spanish from monolingual en to monolingual spa
%% Shared: 67.02, 69.13, 72.35, 72.27, 69.51, 71.28, 72.34
%% Unique: 42.76, 51.78, 49.73, 53.82, 52.11, 54.40, 56.83
%% difference: 24.26, 17.35, 22.62, 18.45, 15.69, 16.88, 15.51
%% average: 18.68

Despite some deviations from a consistent gradual increase in accuracy, all groups still show the same pattern from finetuning. We see the highest accuracy on high-resource matched-language pretraining and lowest on high-resource mismatched pretraining. Then, steadily decreasing accuracy across most mixed-language models as mismatched pretraining data increases. We also see in this experiment that shared phoneme groups have higher average accuracy on all models than unique groups. This is despite the greatly varying size of their phoneme inventories, as seen in Section~\ref{ed_subsec:lang_membership_groups}.\thoughts{possibly this sentence in discussion as well} \hl{This may suggest that shared phonemes can benefit from increased exposure in pretraining even when the pretraining data is mismatched. Language-unique phonemes, on the other hand, suffer in accuracy when mismatched-language pretraining limits exposure to phonemes in this group.}\thoughts{should this be moved to discussion? Or at least the end of the section.} Next, then, we can explore whether the exposure of individual phonemes is still a benefit when those phonemes are spoken in mismatched languages.


\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_shared_not_shared_fre_pol.pdf}
    \caption{Mean phoneme accuracy at layer~7 for French-Shared, French-Unique, Polish-Shared, and Polish-Unique groups across English and Spanish pretraining mixtures. Shared groups overlap with the combined English and Spanish pretraining inventory; French-Unique and Polish-Unique phonemes are absent from both pretraining languages. }
    \label{fig:french_polish_phoneme_groups}
\end{figure}


%% linegraph values for French from monolingual en to monolingual spa
%% Shared:     60.79, 61.23, 61.44, 60.12, 57.73, 58.79, 55.33
%% Unique:     46.50, 47.00, 49.61, 46.87, 45.97, 43.66, 40.88
%% difference: 14.29, 14.23, 11.83, 13.25, 11.76, 15.13, 14.45
%% average diff: 13.56
%% French Shared Monolingual difference: 5.46
%% French Unique Monolingual difference: 5.62

%% linegraph values for Polish from monolingual en to monolingual spa
%% Shared: 70.50, 71.95, 68.39, 68.86, 68.14, 67.53, 61.72
%% Unique: 63.50, 64.46, 62.10, 63.59, 60.98, 58.96, 59.12
%% difference: 7, 7.49,  6.29,  5.27,  7.16,  8.57,  2.6
%% average diff: 6.34
%% Polish Shared Monolingual difference: 8.78
%% Polish Unique Monolingual difference: 4.38

Figure~\ref{fig:french_polish_phoneme_groups} shows phoneme accuracy for French-shared, French-Unique, Polish-shared, and Polish-Unique groups across all of our English and Spanish pretraining mixtures. These groups are outlined in Section~\ref{ed_subsec:lang_membership_groups}.

For French, French-Shared consistently has higher accuracy than French-Unique, with an average difference of 13.56\%. Both French groups have higher accuracy on the English monolingual than the Spanish monolingual model, similarly to the finetuning results in Section~\ref{ed_subsec:gain_across_languages}. However, the difference in accuracy is smaller when compared to English and Spanish, with 5.46\% for French-Shared and 5.62\% for French-Unique. For Polish, the difference between the shared and unique group accuracies is smaller than for French, with an average difference of 6.34\%. The difference in accuracy between the English and Spanish monolingual models is larger, with 8.78\% for Polish-shared and 4.38\% for Polish-Unique.

A summary of the mean accuracy difference between Shared and Unique groups and the accuracy difference between monolingual models for all four languages is given in Table~\ref{tab:shared_unique_summary}. While both French and Polish have higher accuracy on the English monolingual model, the gradual shift in accuracy seen previously is not as clear in either language. Both French groups peak in accuracy at the 600/300 model, and Polish groups show fluctuations in accuracy across mixed models. This suggests that mismatched-language balance correlates less with accuracy for unseen test languages.

For all four languages, however, we see that for each model the Unique groups always have lower accuracy than the Shared groups. This pattern is consistent despite the large differences in phoneme inventory sizes, as described in Section~\ref{ed_subsec:lang_membership_groups}. These results suggest that individual phoneme exposure during pretraining has more correlation to phoneme accuracy than the ratio of mismatched languages when testing unseen languages.

\begin{table}[h]
    \centering
    \begin{tabular}{lccc}
        \hline
        Language &
        Mean gap &
        Mono.\ diff.\ (shared) &
        Mono.\ diff.\ (unique) \\
        \hline
        English & 15.34\% & +19.24\% & +21\% \\
        Spanish & 18.68\% & -5.32\%  & -14.07\% \\
        French  & 13.56\% & +5.46\%  & +5.62\% \\
        Polish  & 6.34\%  & +8.78\%  & +4.38\% \\
        \hline
    \end{tabular}
    \caption{Mean accuracy gap between shared and unique phoneme groups for each language, and accuracy differences between English-monolingual and Spanish-monolingual pretraining (English minus Spanish) for shared and unique groups.}
    \label{tab:shared_unique_summary}
\end{table}

While language-membership grouping reveals certain patterns across languages, it compares groups that differ greatly in total phoneme counts. This may be conflating multiple phonetic properties and obscuring more precise information. We therefore analyse phoneme accuracy by manner-of-articulation (MoA) classes in the next section. MoA classes are widely used in phonetics that we can use for more balanced group sizes, comparable inventories across languages, and clearer isolation of individual phonetic features~\cite{international1999handbook}.


%\newpage
\subsection{Phoneme Accuracy in Phoneme Classes}\label{ed_sec:phonemes_classes}

 %%% FULL TABLE WITH ALL PHONEMES AND ALL VARIATIONS %%%%%%%%%
\iffalse
\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{ Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
%Vowel & \textipa{i, i:, {\ae}, 5, A, A:, 6, 6:, @, \textrhookschwa, E, \textrhookrevepsilon, \i, 0, 0:, U} \\ %% All 
& Vowel & \textipa{i, i:, {\ae}, 5, A, A:, 6, 6:, @, \textrhookschwa, E, \textrhookrevepsilon, \i, 0, 0:, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, b\super j, c, c\super h, c\super w, d, d\super j, \|[{d}, k, k\super h, k\super w, p, p\super h, p\super j, p\super w, t, t\super h, t\super j, t\super w, \|[{t}, \textObardotlessj, \textObardotlessj\super w, g, g\super w, P} \\
& Nasal & \textipa{m, m \super j, \textsyllabic{m}, n, \textsyllabic{n}, N, M, \textltailn} \\
& Fricative & \textipa{f, f\super j, h, s, v, v\super j, z, \c{c}, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \textltilde, \textsyllabic{\textltilde}, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R, R\super j, \~{R}} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c,  \|[{d}, k, p, \|[{t}, \textbardotlessj, \textbardotlessj J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, \c{c}, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, \~{A}, O, \~{O}, \textschwa, E, \~{E}} \\
& Stop & \textipa{b, c, d, k, p, t, \textObardotlessj, g} \\
& Nasal & \textipa{m, m\super j, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, \~{O}, E, \~{E}, 1} \\
& Stop & \textipa{b, b\super j, c, \|[{d}, k, p, p\super j, \|[{t}, t\super j, \textObardotlessj, g, P} \\
& Nasal & \textipa{m, m\super j, \|[{n}, N, \textltailn} \\
& Fricative & \textipa{f, f\super j, \|[{s}, v, v\super j, x, \|[{z}, \c{c}, C, \:s, \textcommatailz, \textctz} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, \|[{d}\|[{z}, t\c{c}, t\:s, \|[{t}\|[{s}} \\
& Approximant & \textipa{j, \~{j}, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, r\super j} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used.}
\label{tab:phoneme_groups_by_language}
\end{table}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% change layout to phoneme groups being in a separate column to language
%\iffalse
%\newpage
\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
& Vowel & \textipa{i, {\ae}, 5, A, 6, @, E, \textrevepsilon, \i, u, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g, P} \\
& Nasal & \textipa{m, n, N, M, \textltailn} \\
& Fricative & \textipa{f, h, s, v, z, c, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c, k, p, \j, \j J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, c, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, O, \textschwa, E, \oe} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L, 4} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, E} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g, P} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, x, z, c, C, s} \\
%& Affricate & \textipa{dz, tc, ts} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, t\c{c}, t\:s} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Phonemes per language grouped by articulatory class. This table lists one base symbol per MFA phoneme; a full inventory including all phonetic variants used in our experiments is given in Appendix~\ref{app_sec:chapter_5_appendix}, Table~\ref{app_tab:phoneme_groups_by_language}. \hl{THIS TABLE HAS BEEN MOVED TO EXPERIMENTAL DESIGN IT CAN PROBABLY BE REMOVED HERE}}


\iffalse
\caption{Phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used, including those reflecting phonetic variations such as extended duration of vowels (\textipa{i:, A:}), labialization (\textipa{\textObardotlessj\super w,  p\super w}), aspiration (\textipa{p\super h, k\super h}), palatalization (\textipa{R\super j, m\super j}), nasalisation (\textipa{\~{E}, \~{A}}), syllabification (\textipa{\textsyllabic{n}, \textsyllabic{\textltilde}}), dental placement \hl{noun?} (\textipa{\|[{d}, \|[{t}}) and rhotacization (\textipa{\textrhookschwa, \textrhookrevepsilon}). \hl{Extra variants} (\textipa{0, \textObardotlessj, \c{c}, \textltilde, \textbardotlessj J, 1, \:s, \textcommatailz, \textctz, d\textcommatailz, d\textctz, t\c{c}, t\:s, C})Phonetic variations are not included in the table, for full list of tokens used see Appendix table \ref{app:phoneme_groups_by_language} \hl{will have to revisit and reqrite this table description, still many variations included}}
\fi
\label{ed_tab:phoneme_groups_by_language}
\end{table}
%\fi

To further test this theory we can break down our phoneme data into sub-groups, this will allow us to explore whether the language or the phoneme groups are more important for recognition. Table \ref{ed_tab:phoneme_groups_by_language} shows the breakdown of phonemes in each phoneme group, the phonemes are selected from all of the phonemes in the MFA dictionaries for English and Spanish. All phonemes modifications, such as \textipa{a:} or \textipa{p\super j,}, are also included in the group with their unmodified phonemes. The full list of phoneme used in each language can be found in Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phoneme_class_acc_with_diff}.

The groups of phonemes chosen follow the same groupings as found in English et al. \cite{english2022domain} with some exceptions. All vowels were grouped together with any stessers also included. Diphthongs, that only exist in the English MFA dictionary, were separated into their own group, in order to test whether coarticulation between the phonemes may be relevant to the pretraining languages. Closure simply includes the "sil" token which denotes a lack of any sound for a short period of time. Finally the phonemes \textipa{r} and  \textipa{R} were separated into their own group, Tap \& Trill, while they could have been included in the Approximant group both \textipa{r} and \textipa{R} are much more frequent in the Spanish data so may be affected by pretraining data more than other approximants.

Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} shows a breakdown of the average accuracy of different phoneme classes from the validation data from the English LibriSpeech dataset, the Multilingual LibriSpeech (MLS) Spanish subset, the MLS French subset and the MLS Polish dataset. The groups show accuracy from layer 7 of each of the 7 pretrained models for the nine different phoneme groups described in Table \ref{ed_fig:Phonemes_Groups_English_linegraph}. Figure \ref{ed_fig:Phonemes_Groups_English_barchart} shows the difference in accuracy of each phoneme group for each language when taken between the accuracy of the monolingual English model trained only on LibriSpeech data and the monolingual Spanish model trained only on the Multilingual LibriSpeech (MLS) Spanish subset as shown in equation \ref{ed_equ:CER_differrence_phoneme_groups}.

\begin{equation}\label{ed_equ:CER_differrence_phoneme_groups}
\centering
    CER\_on\_monolingual\_English - CER\_on\_monolingual\_Spanish
\end{equation}

In the vowel plot on Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we see that Spanish vowels have the highest accuracy across all models being above 80\% in all cases except the English monolingual model (960 / 0). Notably, Spanish has the fewest total individual vowels of all of the languages as seen in Table \ref{app_tab:phoneme_groups_by_language}, Spanish has no variants to the phonemes in the vowels which can occur less frequently in both training and validation data and therefore lower accuracy the average accuracy. It is also notable that the Polish vowels have the highest accuracy on the balanced mixed model (450 / 450), this could imply that when the testing language is unrelated to the pretaining languages individual phonemes are more impactful than the relationships between phonemes. Figure \ref{ed_fig:Phonemes_Groups_English_barchart} confirms that Polish vowels have the least difference between accuracy on each of the monolingual models, whilst English has the highest difference. 

Dipthongs were separated into a separate figure from vowels in order to evaluate whether language specific coarticulation affected accuracy, Dipthongs were only available in the English MFA dictionary. We see in \ref{ed_fig:Phonemes_Groups_English_linegraph} that English Dipthongs have higher accuracy on every model than their vowel counterparts, possibly due to the reduced set. English Dipthongs also have a higher difference between accuracy on monolingual models of 16.2\%. A full list of results for every model and the difference between the monolingual models can be found in Appendix \ref{app_sec:chapter_5_appendix}, Table \ref{app_tab:phoneme_class_acc_with_diff}.

For approximants in both Spanish and English the highest phoneme accuracy exists on their respective monolingual models. While the difference between models for Polish in Figure \ref{ed_fig:Phonemes_Groups_English_barchart} is in favour of the English monolingual model accuracy for Polish approximants is also high for the 800 / 100 model and the 300 / 600 model. French on the other hand has little difference between accuracy on either of the monolingual models and little variation for approximants across all models.

For nasals Figure \ref{ed_fig:Phonemes_Groups_English_barchart} shows all four languages having higher accuracy on the English monolingual model than on the Spanish monolingual model. Spanish has higher accuracy for nasal phonemes on the English monolingual when compared to the Spanish monolingual model, although it should be noted that Spanish nasals have the highest accuracy on the 100 / 800 model, so still a model with mostly Spanish training data. This may however show some nuance to the idea that all speech data benefits from monolingual or single domain pretraining. If a model is better at recognising phonemes from a a certain group this may be more beneficial for downstream recognition than simply pretraining on the same language. Since English, French and Polish also show higher accuracy on the English monolingual model this may support that theory. 

For the Tap \& Trill category Spanish has higher accuracy across all models when compared to both Polish and English and has 9.6\% more accuracy on the Spanish monolingual model when compared to the English monolingual mode. While in Figure  \ref{ed_fig:Phonemes_Groups_English_barchart} English has 15.3\% higher accuracy on the English monolingual model when compared to the Spanish monolingual model. However, on Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we can see that there is also an 11.8\% difference between the English monolingual model (960 / 0) and the model with balanced pretraining data (450 / 450), since the English accuracy for this group does not go above 33\% this suggests that recognition for phonemes in this group is not as dependant on the pretraining data as other groups. Similarly for Polish the average accuracy is low across all groups with the highest accuracy on the Spanish monolingual model at 21.4\%. While Polish does have higher accuracy on the Spanish monolingual model than on the English monolingual model it seems that phonemes in this group have low accuracy regardless of pretraining data. %\thoughts{do I need to come back to this when I know what's happening in the pretraining data?}

For Stops we see the pattern shown in previous sections with English phonemes having higher accuracy on the English monolingual model when compared to the Spanish monolingual model with a difference of 19.3\%. The Inverse is then true for Spanish, although to a lesser degree, with Spanish spoken phonemes having higher accuracy on the Spanish monolingual model and lower on the English model although this time with a difference of 1.3\%. Interestingly, the highest accuracy for Spanish is found on the balanced model (450 / 450), suggesting Spanish is benefitting from balanced monolingual pretraining in this category. French and Polish both have higher accuracy on the English monolingual model when compared to the Spanish monolingual model. This effect may be due to English having a larger variation of phonemes in this group when compared to Spanish, this may be beneficial for accuracy for phonemes in this group. French nasal phonemes have the highest accuracy across all models except the 300 / 600 model. 

For Affricates Polish shows the least variation with only 1.4\% difference between accuracy on the English and Spanish monolingual models. English has the highest accuracy on the English monolingual model and the lowest on the Spanish monolingual model showing preference to pretraining on the same language. Spanish however, has higher accuracy on the English monolingual model when compared to the Spanish monolingual model. The highest recorded accuracy is on the 450 / 450 model while the lowest is on the 300 / 600 model, giving Spanish the most variation of the languages in this category. As seen in Table \ref{app_tab:phoneme_groups_by_language} Spanish only contains one phoneme in this category, \textipa{tS}, and there are only 391 examples of the phoneme in the training data for the probes as seen in Table \ref{app_tab:phoneme_counts_per_language_train_10h_3}, when compared to phonemes that often have more than 10,000 examples. Training probes to recognise a single phoneme, with few examples may contribute to the variation in this case.

For Fricatives we see English again having higher accuracy on the English monolingual model when compared to the accuracy on the Spanish monolingual model. Spanish fricatives then have the highest accuracy on the Spanish monolingual model and the lowest on the English monolingual model. Both French and Polish have higher accuracy across all models than either English or Spanish, which is not true of any other phoneme group.

Finally for the closure group for French, Polish and English the accuracy on every model is higher than the accuracy in any other group for each model and for Spanish the only higher accuracy is found an all model except the English monolingual model in the vowel category. The closure category only contains the silence token (sil) given by the Montreal Forced Aligner when no sound is made. The high accuracy in Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} across all languages and the lack of clear preference to either language in Figure \ref{ed_fig:Phonemes_Groups_English_barchart} suggests that silence is recognised accurately regardless of the model or the language the closure exists in.

Overall the findings in this section show that phoneme accuracy is not consistent across different categories of phonemes. As seen in nasals and affricates the concept that pretraining on the same or a related language always increases accuracy is not always true and in the case of Spanish stops and nasals, a balanced mix of pretraining can give the highest accuracy. In most cases however phonemes spoken in English do have higher accuracy on the models with the most English in pretraining and phonemes spoken in Spanish have the highest accuracy on the models with the most Spanish data in pretraining. 

For French, in all categories except closure phonemes in this language have higher accuracy recorded on the monolingual English model when compared to the monolingual Spanish model, which is consistent with the findings in Figure \ref{ed_fig:multilingual_barchart}. Accuracy is not consistent across all categories with a difference of 38\% between nasals and fricatives on the monolingual English model. For Polish, average accuracy is also inconsistent across categories, but is also less consistent across models with vowels having the highest accuracy on the 450 / 450 and approximants performing well on both the 800 / 100 model and the 300 / 600 model. 

These findings show that when the testing language is not in pretraining they do not necessarily benefit from monolingual training. 

%\thoughts{I feel that I need to expand this, I could possibly do this in the discussion section though}


%\newpage
\begin{figure}[H]
    %\centering
    \includegraphics[height=0.9\textheight, width=0.75\paperwidth]{5_chapter5/figures/Rotate_phoneme_groups_line_graph_wpolish.pdf}
    \caption{Phoneme accuracy for 9 different phoneme groups. Accuracy is shown across all 7 pretrained models for four different languages: English, Spanish, French and Polish.}
    \label{ed_fig:Phonemes_Groups_English_linegraph}
\end{figure}

%\newpage
\begin{figure}[H]
    %\centering
    \includegraphics[height=0.9\textheight, width=0.75\paperwidth]{5_chapter5/figures/Rotate_phoneme_groups_bar_chart_wpolish.pdf}
    \caption{The difference between the accuracy of 9 different phoneme groups when measured between the English monolingual model and teh Spanish monolingual model. Accuracy is shown across all 7 pretrained models for four different languages: English, Spanish, French and Polish.}
    \label{ed_fig:Phonemes_Groups_English_barchart}
\end{figure}


\newpage
\section{Discussion}\label{ed_sec:discussion}
\hl{HERE IS A DELETED DRAFT FOR THE SHARED NOT SHARED EXPERIMENTAL DESIGN SECTION THAT COULD BE USED IN DISCUSSION BUT IS CURRENTLY IFFALSE}

\iffalse
For French and Polish, the overlap group consists of phonemes that appear in the union of the English and Spanish pretraining phoneme sets and also appear in the French (or Polish) phoneme set; the language‑unique groups contain phonemes that appear only in French or only in Polish and not in the combined English+Spanish set.

Tables~\ref{tab:phoneme_group_sizes_en_es} and~\ref{tab:phoneme_group_sizes_fr_pl} summarise how many MFA phoneme tokens belong to each group in the pretraining and probe sets. These tokens include modified symbols such as stress and length variants. For English and Spanish, we use the shared and language-unique groups defined relative to their pretraining inventories. The shared groups contain phoneme tokens that appear in both English and Spanish pretraining data, so they are present under all matched- and mismatched-language finetuning conditions. In contrast, the language-unique groups contain tokens that are missing from one of the monolingual pretraining conditions. Phonemes in this group then occur in much smaller in low‑resource match or mismatched pretraining settings for that language. 

For each pretrained model, we compute an average group-level accuracy. First, we will compute the probe accuracy for each individual MFA phoneme token in the group. Then we take the mean of these phoneme accuracies to obtain the overall accuracy for that group. 

English has the largest number of MFA phoneme tokens, with 80 distinct tokens in the English pretraining and probe sets, with 54 English-unique tokens and 26 tokens that are shared between English and Spanish. Spanish has the smallest set, with 36 phoneme tokens in total, split into 26 shared tokens and 10 Spanish-unique tokens. This difference partly reflects how the MFA dictionaries represent each language: English includes more phonetic variants, such as stress markers or nasalisation etc., creating more token types, whereas Spanish represents a smaller core phoneme inventory with less variation. A full list of MFA phoneme tokens for each language, together with their frequency in the corpora, is provided in Table \ref{app_tab:phoneme_counts_per_language_vaid_1}. 

As a result, group-level accuracies are averaged over different numbers of phonemes, and smaller groups such as Spanish-unique or French-unique are more sensitive to the behaviour of individual phonemes. These differences in coverage are taken into account when interpreting the probe results in later sections. 

For French and Polish, we define groups that are related to, but not identical with, the English–Spanish shared and unique groups. Here the main distinction is whether a phoneme token has been seen in any of the English or Spanish pretraining data. To capture this, we first build a combined pretraining inventory by taking all MFA phoneme tokens that occur in either English or Spanish pretraining. 

For French and Polish, we are interested in how well the model can recognise individual phoneme tokens that were seen during English or Spanish pretraining, compared with phoneme tokens that were never seen in pretraining at all. To capture this, we build a combined pretraining inventory from all MFA phoneme tokens that occur in either English or Spanish pretraining. The overlap groups contain French or Polish tokens that also occur in this combined English and Spanish inventory, while the language-unique groups contain tokens that occur only in French or only in Polish and never occur in English or Spanish. This lets us compare recognition of phonemes that are familiar from pretraining, but evaluated in different languages, with phonemes that are completely unseen in pretraining. In the probe train and test sets, French has 45 MFA phoneme tokens, with 35 in the overlap group and 10 in the French-unique group, while Polish has 51 tokens, with 32 in the overlap group and 19 in the Polish-unique group. The exact token lists and their frequencies are also included in Table \ref{app_tab:phoneme_counts_per_language_train_10h_1}. 
\fi

\begin{itemize}
    \item Things to think about later 
    \item Section \ref{ed_sec:shared_vs_not_shared}:
    \item notice how there is variability in accuracy (300/600 having lower accuracy than 100/800)
    \item this does not relate to phoneme inventory size
    \item note this is surprising but the variability of inventories means we cannot best analyse this data.
    \item more obviously talk abotu how all of the shared groups have higher accuracy than the unique groups despite English-unique having 54 phonemes and Spanish-unique having 10 and shared having 26
    \item phrasing: shared phonemes benefit even when pretraining data is mismatched (because unique phonemes always have less accuracy), although they benefit more when pretraining is matched.
    \item Line taken from Section~\ref{ed_sec:shared_vs_not_shared}, could be used here instead:
    \item "We also see in this experiment that shared phoneme groups have higher average accuracy on all models than unique groups. This is despite the greatly varying size of their phoneme inventories, as seen in Section~\ref{ed_subsec:lang_membership_groups}.This may suggest that shared phonemes can benefit from increased exposure in pretraining even when the pretraining data is mismatched. Language-unique phonemes, on the other hand, suffer in accuracy when mismatched-language pretraining limits exposure to phonemes in this group."
\end{itemize}

\hl{Below are some deleted sentences from the experimental results multilingual finetuning section that could be used as talking points here:}

These findings verify previous research on single languages to be consistent across multiple languages and language groups. The results from our mixed-language models then expand on our understanding of multilingual pretraining.\thoughts{maybe these lines could be expanded on and thought through in the discussion.}

This suggests that mixed language pretraining might be advantageous for specific low-data situations.

\hl{end of talking deleted sentences}

This section started by training 7 models, one model trained only on the data from the LibriSpeech dataset \cite{librispeech}, one model trained only on the Spanish subset of Multilingual LibriSpeech \cite{MLS} and 5 models trained on different ratios of these 2 datasets, always adding up to 900 hours.The ratios of hours of speech data with English of the left and Spanish on the right (Eng / Spa) were as follows: 800 / 100, 600 / 300, 450 / 450, 300 / 600 and 100 / 800. As shown in previous work \cite{ashihara2023exploration, poncelet2021comparison}, we prove in Section \ref{ed_subsec:same_data_finetuning} that the languages in your pretraining affect downstream finetuning. Figure \ref{ed_fig:fine_tuning_100hours} shows that when there is more English data in pretraining the model has lower Character Error Rate (CER) after finetuning to English data. Similarly when more Spanish data is present in pretraining lower CER is found after finetuning to Spanish. This is then replicated with unseen FLEURs data in Section \ref{ed_subsec:gain_across_languages} Figure \ref{ed_fig:multilingual_linegraph}.

We then confirm findings in \cite{zhang2023fast, gupta2021clsril} with our models that prove that languages that are closely related to the language most prominent in pretraining data for a model achieve lower CER when compared to finetuning them on models pretrained on a language that is unrelated to the finetuning data. Figure \ref{ed_fig:multilingual_barchart} Romance languages such as Catalan, Asturian and Italian have lower CER on the monolingual Spanish model when compared to their CER on the monolingual English model. We can also see in Figure \ref{ed_fig:multilingual_barchart} that the Germanic languages such as Danish and German have a lower CER on the monolingual English model when compared to their results on the monolingual Spanish model, showing that languages related to English benefit from more English in pretraining. 

French shows lower error on the English monolingual model but less so than the Germanic languages, possibly due to it's phonetic overlap with English \cite{roth2010explore} despite being classified as a romance language \cite{bauer2007linguistics}. The two Slavic languages Russian and Polish show lower error on different models with Polish having a preference for English pretraining data and Russian having a preference for Spanish pretraining data. This implies that when a language is unrelated to the language most prominent in the pretraining data it does not directly benefit in accuracy.

Next we look to delve deeper into why related languages benefit each other when relating to pretraining and finetuning. The wav2vec 2.0 Self-Supervised Learning (SSL) pretraining stage uses contrastive loss to recreate the input to the encoder's first layer of the transformer encoder to  at the encoder's final layer. Pasad et al. \cite{pasad2021layer} show that wav2vec 2.0 learns different speech tasks in different layers of the encoder such as local acoustic features in the early shallow layers and phone identity in the late middle layers. They also show that when finetuned to a semantic task of producing English sentences the final layers change task from reconstruction of the input to grapheme output, while the middle to early layers do not change to the same extent. English et al. \cite{english2022domain} then use single linear layers to probe the English trained wav2vec 2.0 and test the phone recognition across each layer of the model. Similarly to Pasad et al. English finds the late middle layers having the highest phone accuracy, they however find that different phoneme group have varying accuracy across layers and across the phoneme groups. 

Using this information we can surmise that the phonetic but not semantic information learned during pretraining has a significant impact on the final error rate after finetuning and that this learned information will not change. This means that it is very important to understand how SSL ASR models are learning phonetic information, in our case we can take this idea further and look to discover how mixing languages affects pretrained phone recognition. We use the same technique as English et al. \cite{english2022domain} and probe each layer for phone recognition, we then select one of the highest performing layer (layer 7) and measure how well our models recognise different groups of phonemes. 

In Section \ref{ed_sec:shared_vs_not_shared}, we collect the phoneme accuracy data for all of the phonemes found in the English LibriSpeech dataset and the Spanish subset of MLS. For our first experiment we group phonemes by whether they appear in both datasets and therefore the pretraining data for all models vs phonemes that are unique to either English or Spanish. Figure \ref{ed_fig:phonemes_shared_not_shared} shows that when tested on English data only phonemes that are shared between English and Spanish and phonemes that are unique to English both perform best on the English monolingual model when compared to their performance on the Spanish monolingual model. However, we also see that on every model, phonemes that are shared between English and Spanish always have higher accuracy than uniquely English phonemes. 

When testing on Spanish data both phonemes shared between English and Spanish and phonemes unique to Spanish performing best on either the monolingual Spanish model or the 100 / 800 Eng / Spa model. However, for both sets, we again find that phonemes exist in both pretraining datasets perform better on every model. These Findings suggest that individual phonemes are still affected by which language they are spoken in, possibly due to coarticulation between different phonemes. They also show that these phonemes will achieve higher accuracy when the model has seen more examples of that phoneme regardless of the language it is spoken in as with the two shared phonemes groups.

Next we test the same phoneme groups but this time when the data is from languages that were not included in the pre-training data. We chose data from the French and Polish subsets of MLS for this experiment, French because it was an outlier in the previous section, having higher accuracy on the models with more English data and Polish because it is not in the same language family as either English or Spanish. In Figure \ref{ed_fig:phonemes_shared_french_polish} we see that phonemes that exist in both Spanish and English consistently outperform phonemes that exist only in French. The same pattern is also true of Polish to varying ratios with the phonemes that exist in English and Spanish consistently outperforming phonemes that are unique to Polish. These results support our previous findings with the English and Spanish data but also suggest that carefully choosing pretraining data based on which phonemes are included could increase accuracy even for languages that have not been seen in pretraining. Alternatively, choosing pretraining data with less phonetic diversity could harm the model's performance.

Next in Section \ref{ed_sec:phonemes_classes} we further divide our languages into phonemes classes and analyse the trends across the four languages. We analyse the same groups as \cite{english2022domain} but separate out Diphthongs and Taps \& Trills. A full list of the Montreal Forced Aligner (MFA) phoneme tokens used for each category alongside the total number of phonemes in each group for the pretraining data for each model and the total number of each phoneme included in the training data for our probes can be found in Appendix \ref{app_sec:chapter_5_appendix}. 

For most classes English phonemes perform best on models pretrained with more English data and phonemes spoken in Spanish perform best on models pretrained with more Spanish data as seen in Figure \ref{ed_fig:Phonemes_Groups_English_barchart}. This effect is not true for the Nasals or Affricate categories where when spoken in Spanish, they have higher accuracy on the monolingual English model when compared to the monolingual Spanish model. For affricates this may be explained by the number of phonemes in Spanish in this group, there is only one \textipa{tS} and although there is only one more affricate in English, \textipa{dZ} we can see in table \ref{app_tab:phoneme_token_class_counts_k} that the English model was trained on a total of 376k individual affricates whereas the monolingual Spanish model had only seen 90k examples. For Nasals both datasets contain more than 3M tokens in the affricate class, so it seems this unlikely to be the cause of this behaviour. Looking to Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we see that the 100 / 800, Eng / Spa, model actually has higher accuracy than the Spanish monolingual model or the English monolingual model at 61.7\%, compared to 49.4\% and 57.8\%. This suggests that a larger ratio of Spanish data still improves phoneme recognition.

For the Tap \& Trill category phonemes spoken in Spanish have higher accuracy than when spoken either English or Polish. Again we can see in Table \ref{app_tab:phoneme_token_class_counts_k} The pretraining data for the monolingual Spanish contains many more phonemes in this category for pretraining, with more than 2M, than the monolingual English monolingual model at less than 400k. Data for the Dipthong category was only available in English and the accuracy is high for every model when spoken in English. The accuracy on English spoken dipthongs reduces significantly however between the 100 / 800 model, going from 79\% accuracy to 69.6\%. The pretraining data for the monolingual Spanish model contained 0 dipthongs whereas the 100 / 800, Eng / Spa, model (containing only minimal English data) contains 206k diphthongs. This appears to show the importance of phonetic diversity in the pretraining data and that even a relatively small number of examples can dramatically increase accuracy when compared to an unseen phoneme.

We have also seen in the previous Chapter \ref{chapter4}\info{previous chapter on pruning bias}  that a multilingual SSL ASR model relies heavily on weight attributed to English \cite{storeyedbiasSSL}. Some experiments were done collecting the Intersect Over Union (IOU) data for our 7 models similar to the experiments done in Section \ref{IOU_Section}\info{Section in previous chapter on IOUs} but ultimately not included in this cahapter. The results showed that more weights in the English monolingual were attributed to English and more weights in the Spanish monolingual model were attributed to Spanish. When more English data was included in pretraining more of the weights were attributed to English and when more SPanish data was in pretraining more weights were attributed to Spanish. These results confirm the findings in Chapter \ref{WC2}\info{previous chapter on pruning and bias} that the largest dataset in pretraining had the most influence over the weight distribution
\iffalse
Two consistent themes from the research in this chapter have emerged: downstream accuracy and error rates perform best when the pretraining data is the same or a related language to the downstream testing or finetuning language and that seen phonemes always performs better than unseen phonemes. As we have seen in \cite{ashihara2023exploration, poncelet2021comparison} when the pretraining data matches the language of the finetuning data we can achieve much lower error rates with less data and we have replicated this effect with our experiments in Sections \ref{ed_subsec:same_data_finetuning} and \ref{ed_subsec:gain_across_languages}. We then show in Section \ref{ed_sec:shared_vs_not_shared} and Section \ref{ed_sec:phonemes_classes} that this may be an issue of seen and unseen phonemes and phoneme interactions such as coarticulation. These sections show that while pretraining on a related language benefits the finetuning accuracy, so does pretraining on individual phonemes that exist in the finetuning data or finetuning language. 
\fi

%\thoughts{Do I need to include some pruning IOU data that I collected to support this? Maybe I should as it would better link it to the last chapter and make this feel a little less out of the blue I probably don't need to gvie it a whole seciton but it could be the appendix}. 

In this chapter we have proven that related languages are more likely to contain similar phonemes and coarticulations than unrelated languages, thus giving better accuracy. However, we have also shown that including more examples of an individual phoneme can increase the accuracy for that phoneme or group of phonemes even when the testing language is unseen and unrelated to the languages included in pretraining. Given these findings we suggest that multilingual SSL ASR pretraining data should be balanced by phoneme content to include as many and as diverse a phoneme set as reasonably possible. By doing this, models will achieve higher accuracy on a broader range of languages with less pretraining data and on smaller models.


\newpage
\section{Future Work}
While phoneme accuracy and finetuned error rates may be better when language specific features are kept in the pretraining data the discovery that phoneme recognition can increase with language-agnostic training is important. 

For example, if a language, dialect or accent was very low-resource and did not have data from closely related languages in abundance SSL ASR could benefit from increasing the most common phonemes in said language by including data from unrelated languages. If we could design the pretraining data in this way we may be able to increase phoneme accuracy and lower error even for low-resource languages without having to resort to large models trained on large datasets. 

For future work, we would design a pretraining dataset that had the same phonetic makeup as a designated finetuning language. The pretraining data would include only data from languages unrelated to the one used in finetuning. We would expect such a model to achieve lower CER than a monolingual model trained on an unrelated language or a model trained on multilingual data that did not include the downstream language. 

Carefully designed pretraining data may even result in better performance than a model trained on multilingual data that contained small amounts of the finetuning language. This may be true as long as the finetuning target language was not the dominant language in pretraining, as per the behaviour exhibited by XLS-R in Chapter \ref{WC2}\info{previous chapter on pruning and bias}. These models could then all be compared to a monolingual model pretrained on a large dataset of the same language as finetuning, which we would expect to perform the best overall. 

If these experiments were successful we could have a simple method for increasing phoneme accuracy and CER for low-resource languages, dialects and accents. This method would only require the Montreal Forced Aligner and some knowledge of the target finetuning language to achieve.


\newpage
\section{Conclusion}
Over the course of this chapter we have dissected how Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models learn speech representations when pretrained with monolingual and multilingual training data. This gives us more insight into how SSL models learn and how we can leverage this knowledge for future work in this field. We started by pretraining 7 different versions of wav2vec 2.0 \cite{wav2vec} with different ratios of two different languages: English and Spanish. We pretrained 2 monolingual models, one on only English data from the LibriSpeech dataset \cite{librispeech} and one from only Spanish data from the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. We then pretrained 5 more models with different ratios of the English and Spanish datasets.

We started in Section \ref{ed_subsec:same_data_finetuning} by fine-tuning to 100 hours subsets of LibriSpeech and MLS Spanish to observe the effect that our pretraining ratios had on finetuning results. We found that The English LibriSpeech data had the lowest Character Error Rate (CER) on the monolingual English model and the highest CER on the monolingual Spanish model with CER steadily increasing as the amount of English data was reduced in the pretraining data. For Spanish we found the inverse with Spanish data having the lowest CER on the monolingual Spanish model and the highest CER on the monolingual English model, with CER steadily increasing as the amount of Spanish data in pretraining was reduced.

In Section \ref{ed_subsec:gain_across_languages} we reproduced the results found in the previous section by testing the model with 10 hours of unseen English and Spanish data each from the FLEURs dataset \cite{FLEURS}. The same pattern was observed with the 10h English FLEURs dataset having the lowest CER on models with more English data in pretraining and Spanish. The 10h Spanish FLEURs shows the inverse pattern again by having the lowest CER on models with the most Spanish data and the highest CER on models with the least Spanish data. These experiments prove that pretraining on the same language as you are finetuning to gives the best results for ASR tasks.

In the same section we test multiple languages in the same languages families as English and Spanish, Germanic and Romance languages \cite{bauer2007linguistics}. We find that the Germanic languages Danish and German both have a positive difference towards English and the only language with a more positive difference than these languages is English. Similarly the Romance languages Asturian, Catalan and Italian have negative differences showing they perform better on the Spanish monolingual model when compared to the English monolingual model. French is an outlier among Romance languages showing a positive difference towards English. Finally we include 2 Slavic languages Polish and Russian, Polish shows a positive difference whereas Russian shows a negative difference. In this case Polish appears to benefit from English pretraining data and Russian benefits from Spanish Pretraining, which indicates that unrelated languages benefit much less from either pretraining languages. These findings confirm previous work that shows that pretraining on related languages can benefit downstream finetuning \cite{zhang2023fast, gupta2021clsril} however given that it has been shown that SSL ASR models learn phonetic characteristics and not semantic characteristics of speech \cite{pasad2021layer} it was important that we understand how the models are learning phonetic information when pretraining on multilingual data.

To understand how multilingual pretraining learns phoneme data we trained linear layer probes to recognise individual phonemes for each language. We collected the phoneme data using the Montreal Forced Aligner (MFA) and then created small 10 subsets of data from MLS to train the probes for phoneme recognition. In Section \ref{ed_sec:shared_vs_not_shared} after we test phoneme accuracy across all models for phonemes that exist in both English and Spanish data when compared to phonemes that are unique to either English or Spanish. Our findings show that the phonemes have the highest accuracy on the monolingual model trained on the same language they were spoken in, similar to the finetuning experiments in Section \ref{ed_subsec:gain_across_languages}. However, it is also notable that the English shared group has higher accuracy than the English Unique group across all models. It was also true that the Spanish Shared group had higher accuracy on every model when compared to the Spanish Unique group. 

This suggested that phonemes will have higher accuracy if there are more examples of it in pretraining even if those examples are spoken in different languages. We test this by introducing phoneme data collected from 2 new 10 hour subsets of MLS: MLS French and MLS Polish. These languages were chosen because French had outlier behaviour in Figure \ref{ed_fig:multilingual_barchart} and Polish is a Slavic language unrelated to English or Spanish. We split the phoneme accuracy data into groups, phonemes shared by English and Spanish spoken in each language and phonemes unique to French or Polish. The results showed that phonemes that are found in both English and Spanish always perform better than phonemes that are unique to either French or Polish even when the shared phonemes were spoken in French or Polish. This suggests that even when a phoneme is spoken in a language unrelated to either of the pretraining languages accuracy can be higher if the model has seen it in pretraining.

In Section \ref{ed_sec:phonemes_classes} we explored whether phoneme group accuracy trends across multilingual pretrained models behaved differently when spoken in 4 different languages: English, Spanish, French and Polish. We found that for English and Spanish in most cases phonemes would perform better when tested on a model with more data from their native language. The exceptions being Affricates where the models with the most English have the highest accuracy, however only one phoneme exists in the affricate class in Spanish and there are much less examples in the training data for Spanish. French and Polish phonemes were more likely to perform better on the models trained with more English data, with some exceptions. For Polish performance for the Tap \& Trill group was marginally higher for models with more Spanish data, which may be due to Spanish having more examples of the phonemes in this group in the pretraining set. 

Finally with the knowledge learned throughout this chapter we suggest future work that could be undertaken. We suggest that error rates on low-resource languages could be reduced by using data from languages that do not have to have any relation to the target finetuning language. A pretraining dataset could created using any languages as long and an appropriate number of all of the relevant phonemes in your target language were included in pretraining.

The consistent themes that emerged from this chapter were that SSL ASR benefits from pretraining on the same or related languages to the finetuning language and that this is true for phoneme recognition as well. The secondary theme is that when there are more training examples for an individual phoneme, accuracy can be increased on that phoneme, regardless of language specific interactions such as coarticulation. Overall, it has been seen throughout the chapter that selecting your pretraining data with care can be a great benefit in ASR and that there is more to SSL ASR than just the language relationships between your pretraining and finetuning languages.



%\keywords{, hello, world}
%%%% Ackronyms
\nomenclature[Z]{CER}{Character Error Rate}
\nomenclature[Z]{WER}{Word Error Rate}
\nomenclature[Z]{SSL}{Self-Supervised Learning}
\nomenclature[Z]{ASR}{Automatic Speech Recognition}
\nomenclature[Z]{CNN}{Convolutional Neural Network}
\nomenclature[Z]{MLS}{Multilingual LibriSpeech}
\nomenclature[Z]{MFA}{Montreal Forced Aligner}
\nomenclature[Z]{MoA}{Manner of Articulation}



%%%% Key phrases
\nomenclature[K]{\texbf{Matched-Language Pretraining: }}{Pretraining language is the same as the finetuning (evaluation) language.}
\nomenclature[K]{Mismatched-Language Pretraining: }{Pretraining language is different from the finetuning (evaluation) language.}
\nomenclature[K]{High-resource pretraining}{Pretraining on a single language or a clear imbalance towards one language (e.g., 960/0, 800/100, 100/800, 0/920 hours of English/Spanish).}
\nomenclature[K]{Medium-Imbalance Pretraining: }{Models pretrained on English–Spanish mixtures with a moderate imbalance between languages, referring to the following models in chapter \ref{chapter5}: 600 / 300 and 300 / 600}

%\nomenclature[K]{More balanced bilingual models}{Models pretrained on English–Spanish mixtures that are not strongly skewed towards one language (e.g., 600/300 and 300/600 hours).\hl{THIS NEEDS TO BE UPDATED}}
%\nomenclature[K]{Balanced Pretraining}{Models pretrained on English–Spanish mixtures that are not strongly skewed towards one language, referring to the following models in chapter \ref{chapter5}: 600/300 and 300/600 hours).\hl{NEED TO COME BACK TO THIS}}
\nomenclature[K]{Balanced-Unrelated Pretraining: }{Pretraining where neither pretraining language matches some test languages but both are equally represented, this refers to the 450 / 450 in chapter \ref{chapter5}.}
\nomenclature[K]{Monolingual Model: }{Model pretrained on data from a single language only. In Chapter \ref{chapter5}, this is the 960 / 0 and 0 / 920 models.}
\nomenclature[K]{Mixed-language Pretraining: }{Pretraining on data that combines English and Spanish in fixed hour ratios, referring to the following models in chapter \ref{chapter5}: 800 / 100, 600 / 300, 450 / 450, 300 / 600, 100 / 800.}
\nomenclature[K]{Shared Phonemes: }{Phonemes that occur in the MFA phoneme inventory of more than one test language.}
\nomenclature[K]{Non-Shared Phonemes: }{Phonemes that occur in the MFA phoneme inventory of only a single test language.}
\nomenclature[K]{Manner-of-Articulation (MoA): }{Standard articulatory category grouping phonemes by how they are produced.}
