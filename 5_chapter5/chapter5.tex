%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Fifth Chapter **********************************
%*******************************************************************************
\chapter{Phonetic Understandings of Multilingual Self-Supervised Learning Automatic Speech recognition Models}
\section{Introduction}
\iffalse
\begin{itemize}
    \item reference the last chapter: why is English so dominant
    \item XLS-R is multilingual sure, but it is not balanced the reason it is so dependant on English is that it is the most dominant language  
    \item in the previous chapter Spanish was the consistent outsider, having the lowest IOU with the base model, consistently low CER and one of the largest data shares in the XLS-R pretraining
    \item even with these attributes we found that Asturian and Catalan used more English weights in the learning
    \item What happens when languages in pretraining are balanced
    \item in this section we choose English and Spanish
    \item what happens when we give another language a chance? Will balancing the languages included in pretraining make the model more multilingual in its downstream learning or is there advantages to having a predominant language in the pretraining? 
    \item I should mention here the difference between SSL and SL and how SSL only learns broad speech features and why it is important to investigate SSL specifically
    \item These are questions that cannot be answered with open-source models so here we will need to pretrain our own SSL ASR models
    \item Broad overview of the chapter
    \item NOTE TO SELF: Maybe fully write this section after having done the rest of the chapter because I will probably have to add in more bullet points as I go through and write everything 
\end{itemize}
\fi
In the previous chapter we analysed the multilingual performance of a widely cited open-source multilingual self-supervised Learning (SSL) Automatic Speech Recognition (ASR) model XLS-R \cite{babu2021xlsr}. We found through pruning \cite{frankle2018LTH} that when finetuning the pretrained model to data from languages that were unseen in the XLS-R  pretraining data  that weights learned from the English language were used as basis for learning. This effect was true even when the finetuned language was present but low-data in pretraining and when a related language was present and high-data, such as Catalan and Asturian using English weights rather than Spanish weights to learn from\cite{storeyedbiasSSL}. This effect in XLS-R appears to stem from the imbalance of languages in the pretraining with English having the most amount of hours when compared to all other languages. For this chapter we explore how the same or similar model learns languages if the pretraining data was not imbalanced towards one language.

All popular open-source SSL ASR models are either trained on single language \cite{wav2vec, hsu2021hubert, chen2022wavlm} or have an imbalance in the number of hours of the languages in pretraining, with English as the dominant language \cite{XLSR-53, babu2021xlsr}. It is therefore important we understand how the languages and balance of languages in the pretraining data affect the performance of SSL ASR models. Previous work has suggested that monolingual training, pretraining an SSL model on a single language, can achieve lower error rates when that model is finetuned to the same language or a related language and this can be done with less training time and smaller models \cite{ashihara2023exploration, poncelet2021comparison} and similar low error rates can be achieved by combining data from related languages \cite{zhang2023fast, gupta2021clsril}. 

SSL ASR models learn speech information in two stages, SSL pretraining and then supervised finetuning. In most cases the bulk of the training will be done at SSL pretraining stage as it allows a model to be trained on large datasets without the need for the data to be labelled. Given that the pretraining stage has been shown to greatly impact the final performance of the finetuned model \cite{storeyedbiasSSL} it is important to learn what information is being learned at this stage. Pasad et al. \cite{pasad2021layer} show that the SSL pretrained wav2vec 2.0 model learns local acoustic features and phone identity, but does not learn semantic tasks until it has been finetuned. So to effectively analyse how SSL pretraining affects multilingual learning we must analyse an SSL model at a phonetic level.

In this chapter we pretrain multiple versions of the wav2vec 2.0 \cite{wav2vec} model in the fairseq framework \cite{ott-etal-2019-fairseq} on two different datasets: LibriSpeech \cite{librispeech} and the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. In total we pretrain 7 different models, two monolingual models, one trained only on English LibriSpeech data and one trained only on MLS Spanish. We then train 5 more models with different ratios of the two datasets always adding up to 900 hours in total with English on the left and Spanish on the right (Eng / Spa) the ratios are: 800 / 100, 600 / 300, 450 / 450, \newline300 / 600 and 100 / 800. 

We then finetune each of these models to smaller subsets of LibriSpeech and MLS Spanish to test whether ASR error rate is related to pretraining data. We then finetune each of the models to English and Spanish subsets of the FLEURs dataset \cite{FLEURS} to confirm whether the same patterns are observed with unseen data. Further finetuning is undertaken with multiple languages that are either related to English or Spanish and some languages that are unrelated to either. Next we analyse how SSL speech models learn phonetic information by training linear layer probes for phoneme recognition and test whether phonemes that appear in both English and Spanish have higher accuracy than phonemes that appear in only one. This test is then extended to French and Polish to test the effect on languages unseen in pretraining. Finally we separate out phoneme data into multiple groups of phonemes and analyse their accuracy across every model in English, Spanish, French and Polish. This analysis gives insight into how SSL speech models learn discrete individual phonemes.

Overall our findings in this chapter give new insight into how SSL pretraining for ASR learns speech features, why finetuned multilingual and monolingual performance is affect by the SSL stage. We finish by suggesting approaches to data balance that could improve the performance SSL training in future work and create efficient and effective training methods going forward.


\section{Background and Related Work}
In the previous chapter we analysed how well the multilingual Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) model XLS-R \cite{babu2021xlsr} learns different languages. Through the use of model pruning \cite{frankle2018LTH} we concluded that the large SSL ASR model was using predominantly weights learned from English pretraining data to learn new languages unrelated to English. This section we will investigate further how SSL ASR learns language and what factors may have contributed to the findings in Chapter \ref{workchapter2}.\info{previous chapter on pruning and bias.}

\subsection{Multilingual and Cross-Lingual ASR}
\iffalse
\begin{itemize}
    \item refresh people's memories on xls-r 
    \item Monolingual vs multilingual
    \item Model size vs Error rate
    \item language relationships and cross-lingual pretraining
\end{itemize}
\fi
XLS-R \cite{babu2021xlsr} is a large SSL ASR model with 24 or 48 transformer blocks \cite{vaswani2017attention} that is pretrained using contrastive SSL on 128 different languages. XLS-R shares the same architecture as it's predecessor wav2vec 2.0 \cite{wav2vec} but expands the number of layers and the number of languages it was trained on. XLS-R performs well across various languages \cite{FLEURS_XLSR_WHISPER} but it does not make use of it's multilingual pretraining and relies heavily on weights learned from the most abundant data sources in pretraining, such as English \cite{storeyedbiasSSL}.

Due to the neural scaling principle we know that the performance of neural networks increase scaling with the amount of data they are trained on and the number of parameters in the model \cite{hestness2017deep, kaplan2020scaling} and this is true of ASR as well \cite{zhang2022bigssl, pu21_interspeech}. However, studies have shown that pretraining SSL ASR models on a single language, monolingual pretraining, can yield lower error rates with fewer parameters than pretraining large models with large quantities of data \cite{ashihara2023exploration, poncelet2021comparison}. Other studies have shown that pretraining on combined datasets containing multiple related languages yields similar low error results \cite{zhang2023fast, gupta2021clsril}. While increasing the size of training data and models leads to more accurate results we can see that poorly designed training data can be inefficient for learning. To increase efficiency of our SSL ASR models we first need to understand how they learn and interpret speech data.

\subsection{Wav2Vec2 and Self-Supervised Speech Pretraining}\label{ed_subsec:background_wav2vec}
\iffalse
\begin{itemize}
    \item First give a summary of wav2vec 2.0, the architecture of the model, the loss function and the dataset (Librispeech) that it was trained on
    \item Then go deeper into the loss function and Pasad et al. findings that it is learning high level speech representations
    \item it is important to note Pasad's finding that the shallow layers do not change much during finetuning and that teh deepest layers change task completely
    \begin{itemize}
        \item it may be good to look at whether there is similar findings for CTC based models on this front
    \end{itemize}
    \item this shows that the broad speech information learned during the SSL stage is left largely intact during the finetuning stage, which is the most common application for a model like this
    \item it is therefore important to understand how information learned at this stage affects the final performance of a model
    \item this then opens the floor for the next section talking about mono vs multilingual training
\end{itemize}
\fi
Wav2vec 2.0  is an SSL ASR model utilising a Convolutional Neural Network (CNN) \cite{krizhevsky2012imagenet} feature extractor and transformer block \cite{vaswani2017attention} encoder, for this study we use the 12 transformer block "Small" version of wav2vec 2.0 \cite{wav2vec}. Wav2vec 2.0 was selected for this study as it shares the same architecture and SSL pretraining loss function as XLS-R \cite{babu2021xlsr} but with fewer transformer blocks and smaller layers, trading lower pretraining time for lower overall accuracy. Wav2vec 2.0 and it's successor models XLSR-53 \cite{XLSR-53} and XLS-R \cite{babu2021xlsr} use contrastive loss to pretrain their models on large unlabelled datasets \cite{wav2vec}.\info{Do I need a diagram of the wav2vec architecture in this section? I will have one in the Literature Review when that's done.}

Contrastive Loss pits the output of the pretrained feature extractor against the output of the transformer encoder training the transformer blocks to recreate the input to block 0 at the output of the final block, transformer block 11. This loss objective teaches wav2vec 2.0 to perform an autoencoder \cite{hinton2006reducing} style task, learning the features of the input before recreating them at the output. The use of contrastive loss to create an autoencoder allows wav2vec 2.0 to be trained on large amounts of unlabelled data. For deployment in downstream tasks wav2vec 2.0 will then usually be finetuned to a smaller dataset, for ASR Connectionist Temporal Classification (CTC) Loss \cite{graves2006connectionist} applied to transcribed speech audio data is the most common approach. Due to the dissonance between the pretraining loss objective and the final task, autoencoder to transcription, it is important to understand what wav2vec 2.0 learns during the contrastive pretraining stage and how it learns this information.

Previous studies show significant difference between the tasks learned on each layer\cite{belinkov2017analyzing} for DeepSpeech2 \cite{amodei2016deep}, a previous supervised ASR model. They also show that different categories of phonemes are learned with higher accuracy at different layers. Choi et al. \cite{choi2024self} have found that SSL ASR speech representations exhibit more phonetic similarity than semantic similarity \cite{choi2024self}. 

Pasad et al. \cite{pasad2021layer} take these ideas further by testing each layer of wav2vec 2.0 for their accuracy multiple different domains: local acoustic features, phone identity, word identity and word meaning. Pasad et al. find that local acoustic features are best represented in the earliest layers of the pretrained but not finetuned wav2vec 2.0. Layers 0 to 3 represent local acoustic features best with accuracy falling off for the middle layers of the "Small" 12 block model, but they then see a rise in accuracy again in the later layers 9 and 10. Phonetic information by contrast is best recognised in the middle layer from layers 4 to 8 of the pretrained model without an increase in the deep layers. Word identity and word meaning are both low in accuracy in the pretrained model when compared to the finetuned model especially in the deeper layers 9 to 11 suggesting that the pretrained model does not learn semantic tasks well during the SSL pretraining stage. 

Shallow layers that learn phonetic and acoustic tasks are also shown to change in accuracy the least across when comparing the pretrained and finetuned models. Given this fact and that we know information learned in pretraining affects finetuned ASR performance in SSL ASR it is important that we understand how phonetic information is being learned during SSL pretraining.

De Heer Kloots et al. \cite{deheerkloots25_interspeech} undertook work published in the Interspeech 2025 proceedings that pretrained a monolingual Dutch wav2vec 2.0 model for analysis. They pretrain a wav2vec 2.0 model using fairseq for 100k steps on Dutch speech data from various sources for comparison with the \textbf{fb-en} model trained by meta on LibriSpeech for 400k steps and \textbf{fb-voxp-100k} a multilingual model trained on 100k hours of voxpopuli data \cite{wang-etal-2021-voxpopuli}. They then train probes to analyse for phonetic analysis and lexical analysis and compare these findings against downstream finetuned ASR performance. They found that pretraining on Dutch data gave higher accuracy for  both phonetic and lexical tasks, to varying degrees. This work was undertaken in parallel with the work outlined and published after our work was completed and did not influence any decisions made during our research.

\subsection{Layer Probing for Phoneme Recognition}
To analyse SSL ASR phoneme accuracy we can have to chose our method carefully, as seen in the Section \ref{ed_subsec:background_wav2vec} different layers of a pretrained wav2vec 2.0 model perform different speech recognition tasks to varying degrees of accuracy. English et al. \cite{english2022domain} explore linear layer probing \cite{immer2021probing, schneiderleveraging} of wav2vec 2.0 to analyse how well each layer recognises different groups of phonemes. In this study they extract hidden states from each transformer block in wav2vec 2.0 when the model has been introduced to the TIMIT dataset \cite{timit}, TIMIT includes both grapheme and phonemes transcripts for their utterances. They then train linear layer probes for the task of phoneme recognition with the transformer block outputs for each phoneme as the probe input. English et al. \cite{english2022domain} confirm that the average accuracy of phonemes are found in the middle layers, layers 5 to 9, of the "Small" 12 layer wav2vec 2.0 model similar to the findings in \cite{pasad2021layer}. 

They then separate the phonemes into different groups to test recognition across layers. Most phoneme groups follow the pattern of having higher accuracy in the middle layers, layer 7 having the highest average, and drop off in accuracy during the final layers. However, different phoneme groups are found to higher or lower accuracy when compared to one another across different layers. For the purposes of this study we can use this information to probe each layer for phoneme accuracy. Once we have the phoneme accuracy data we will analyse whether these different groups of phonemes perform differently for models pretrained on multilingual data and monolingual data outside of the English language. With this experiment we can understand how SSL ASR models trained on multilingual data learn phonetic information.

\newpage
\section{Experimental Design}\label{ed_sec:experimental_design}
In this section we outline the datasets used in all experiments, the configurations and training parameters used for both the Self-Supervised Learning (SSL) pretraining and finetuning stages of Automatic Speech Recognition (ASR) training. We then outline the methods used to obtain phoneme labels for our data and the hyperparameters and training configurations for training linear layer probes. Finally we summarise the evaluation metrics used across all of these experiments.

\subsection{Datasets and Pretraining}\label{ed_subsec:datsets}
Here we will outline the datasets used throughout the experiments in this section. For pretraining wav2vec 2.0 we used combinations of the two datasets outlined in Section \ref{DATASET_SECTION_IN_LR}\info{I haven't written this section yet, it will be in the Lit Review} LibriSpeech and the Spanish subset of Multilingual LibriSpeech (MLS). First we pretrain wav2vec 2.0, detailed in Section \ref{ed_subsec:background_wav2vec} on the full LibriSpeech dataset that contains 960 hours of data, denoted in the graphs in Section \ref{ed_sec:experimental_results} as model \textit{960 / 0}. Next, we pretrain wav2vec 2.0 on the MLS Spanish subset which contains 920 hours of Spanish data this model is denoted in the following graphs as model: \textit{0 / 920}. 

We also pretrain five other models with different ratios of the two datasets, LibriSpeech (English) and MLS Spanish. With the number of hours of LibriSpeech on the left and the number of hours of MLS Spanish on the right the ratios are as follows: 800 / 100, 600 / 300, 450 / 450, 300 / 600, 100 / 800. Each of these data splits equal to 900 hours of data in total. The utterances are not randomly chosen but chosen by iterating through a list of predefined utterances, so that each split always contains the same utterances i.e. the first 100 hours of data from either dataset are always the same utterances and included in every split. All of the datasets we create are balanced by gender with a 50\% / 50\% male / female balance. % \hl{not sure if this is necessary, it's definitely not well worded}.

Next in Section \ref{ed_sec:experimental_results} we finetune each of the previously pretrained models to 100 hours of LibriSpeech and MLS Spanish data separately, the 100 hours of each are collected in the same manner that the data was selected for the data splits above, by selecting from a list of utterances and balancing for gender. The next task is to then finetune each model to several languages in the FLEURs dataset. As outlined in section \ref{DATASET_SECTION_IN_LR}\info{I haven't written this section yet, it will be in the Lit Review} each FLEURs language contains 10 hours of data and the transcripts are not normalised for language so contain some symbols and numbers. Due to the lack of normalisation across languages in the FLEURs text it made using the Montreal Forced aligner (MFA) on the dataset difficult for later probing tasks and only data from the text normalised LibriSpeech \cite{librispeech} and Multilingual LibriSpeech. \newline 

Finally we use 10 hour subsets of LibriSpeech, the Spanish, French and Polish subsets of MLS to train our layerwise probes as outlined in Section \ref{ed_seubsec:exp_des_probing}. These 10 hours subsets are collected in the same manner as the 100 hour subsets of LibriSpeech and MLS Spanish by selecting from a list of gender balanced utterances. The data is then split into windows and features from our pretrained wav2vec 2.0 feature extractors for use in training our linear layer probes. % \hl{do I need these details abotu the windows and FE?}.

\subsection{Pretraining Hyperparameters} \label{ed_pretraining_configuration}
When pretraining wav2vec 2.0 \cite{wav2vec} on each of our datasets we followed the fairseq unsupervised pretraining tutorial in the wav2vec 2.0 github repository\footnote{\href{www.github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/README.md}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/README.md}}. The original wav2vec 2.0 model was trained using 64 V100 GPUs with a \texttt{max\_tokens} size of 1400k \cite{wav2vec}, fairseq measures the size of a batch in tokens which in this case refer to a window of audio taken from an utterance. Fairseq includes a parameter for training named \texttt{update\_freq} which refers to the gradient accumulation of each batch processed, if we were simulating 64 GPUs while training on only 1 GPU when training on one we could set the \texttt{update\_freq} to 64, but it would increase the time taken to train by 64 times since 64 gradients would need to be accumulated before backpropogation was applied. 

For the experiments in this chapter the GPUs used were a single Nvidia RTX-A6000 with 48Gb of VRAM. After some experimentation with increasing the \texttt{max\_tokens} and decreasing the \texttt{update\_freq} we were able to have a batch size 6.4 times larger than the original training and an update frequency of 10 while running on a single GPU. The fairseq script would run naturally for 400k updates, an update is measured by when the script has accumulated all of the gradients and then backpropogated or updated the model. To reduce the time taken we ran multiple experiments for several days while finetuning the model to a 100 hour subset of Librispeech every 10k updates on a separate GPU. While the original wav2vec 2.0 model achieved a WER of 3.4\% on LibriSpeech clean 100h set, we found that a Word Error Rate (WER) below 10\% could be achieved by pretraining on the full librispeech 960 hour dataset for 50k updates. Training took approximately 1 day for every 12k updates and after 50k updates there were diminishing returns in the decrease in error per 12k updates, for our purposes 9.6\% was deemed to be acceptably accurate for the trade off with time. The final hyperparameters that were changed were: \texttt{max\_tokens}: 8960k, \texttt{distributed\_world\_size}: 1, \texttt{update\_freq}: 10 and \texttt{max\_update}: 50k. All other hyperparameters were left unchanged. %\info{I could include a barchart or a line graph showing the decreasing gains in WER with increasing updates} Each model used during this chapter was pretrained using the above script and hyperparameters. 
\newpage
\subsection{Finetuning and Evaluation Languages}
When finetuning our model to 100 hours of data  we follow the Fairseq finetuning scripts for LibriSpeech, regardless of the dataset we are finetuning on. We modify them to run on a single GPU, with all other parameters unchanged \footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_100h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_100h.yaml}}. We double the max\_tokens from 3200k to 6400k and double the update\_freq from -4 to -2, while setting the world\_size, or number of GPUs in use, from 2 to 1. The script for 100 hours runs for 80k updates, this and all other parameters are left unchanged. For finetuning to 10 hour datasets, we use the 10 hour finetuning script for LibriSpeech \footnote{\href{https://github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base_10h.yaml}{github.com/dqxiu/fairseq-sh/blob/main/examples/wav2vec/config/finetuning/base\_10h.yaml}}. We make the same changes to max\_tokens, update\_freq and world\_size. All other parameters were left unchanged.

\subsection{Forced Alignment and Phoneme Accuracy}
In Section \ref{ed_seubsec:exp_des_probing} we train linear layer probes to recognise phonemes at a frame level. To obtain frame level phoneme labels we use the Montreal Forced Aligner (MFA) \cite{mcauliffe2017montreal} and apply it to 10 hour subsets of LibriSpeech, MLS Spanish and MLS French that were compiled using the gender balanced approach as in Section \ref{ed_seubsec:exp_des_probing}. The dictionaries used for phoneme alignment were english\_us\_mfa, spanish\_spain\_mfa, french\_mfa and polish\_mfa. For alignment each dictionary has a dictionary of words and the phoneme's used to pronounce them, each 10 hour training subset had some words that were Out Of Vocabulary (OOV). For the LibriSpeech 10 hour subset there were 0.75\% OOV words, for the MLS Spanish 10 hour subset 7.7\% words were OOV, MLS French 10 hour subset 7\% of the words were OOV and for MLS Polish 10 hour subset 4\% of words were OOV. To include OOV words in the dictionary we used the Grapheme to Phoneme (G2P)\footnote{\href{https://mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}{mfa-models.readthedocs.io/en/latest/g2p/index.html\#g2p}} models to create pronunciations dictionaries and added them to a custom dictionary with the all the words from the original dictionaries that was then used to phonetically transcribe each of the datasets. Phoneme transcriptions were done for the whole 960 hour Librispeech training set and the 920 hour MLS Spanish training set in order to analyse the phonetic makeup of the pretraining data. OOV words were also transcribed using G2P models for the LibriSpeech 960 hour training set 0.88\% of words were OOV and for the 920 hour MLS Spanish training set 8.63\% were OOV.
\newpage
\subsection{Layer Probing}\label{ed_seubsec:exp_des_probing}
To train our layer-wise probes for phoneme accuracy in Section \ref{ed_sec:shared_vs_not_shared} and Section \ref{ed_sec:phonemes_classes} we used the same parameters as English \textit{et al} \cite{english2022domain} a two layer probe taking as input the output of each layer of the wav2vec 2.0 encoder and as output the phonemes included in the dataset. The output of each probe is the total number of phonemes in the dataset. The input for all probes is the same size as the output to each transformer block in the wav2vec 2.0 encoder, 768 inputs.

To train the probes we separate each utterance into windowed audio and extract the features with the models feature extractor. The extracted features are then collated into a dataloader alongside the phoneme in that frame. Each set of extracted features is iteratively fed into the encoder of each model and the output of each layer is extracted. The outputs of each of the 12 wav2v2ec 2.0 layers are then used to train 12 separate probes, one for each layer. Cross entropy loss is then used to match the phoneme at that frame with the output of each probe. 

For testing we used both a small 10 hour subset of LibriSpeech and a larger 100 hour subset of LibriSpeech. We found that both datasets started to overfit after 5 epochs of training. We also found that when the probes were trained on the 100 hour subset of LibriSpeech that the average phoneme accuracy was higher but the training time was longer. When training the probes on the 10 hour subset the training time was drastically reduced and while the overall accuracy was lower the relationship between the accuracy of each category of phoneme was unchanged. Due to these findings we used 10 hours subsets of each of the larger datasets as a compromise for time and accuracy for training our probes.

\begin{table}[h]
\centering
\begin{tabular}{r|cc|cc}
\toprule
\multicolumn{1}{r}{\textbf{Pretraining Hours:}} & \multicolumn{2}{c}{\textbf{English}} & \multicolumn{2}{c}{\textbf{Spanish}} \\
\multicolumn{1}{r}{\textbf{LibriSpeech /}} & & & & \\
\multicolumn{1}{r}{\textbf{MLS Spanish}} &  \textbf{Layer } & \textbf{Acc } & \textbf{Layer } & \textbf{Acc } \\
\midrule
960 / 0 & Layer 7 & 44.7\% & Layer 5 & 57.5\% \\
800 / 100 & Layer 6 & 44.6\% & Layer 7 & 61.0\% \\
600 / 300 & Layer 7 & 43.3\% & Layer 5 & 61.6\% \\
450 / 450 & Layer 7 & 42.8\% & Layer 7 & 62.2\% \\
300 / 600 & Layer 7 & 40.9\% & Layer 5 & 60.0\% \\
100 / 800 & Layer 6 & 39.3\% & Layer 7 & 62.4\% \\
0 / 920 & Layer 4 & 34.1\% & Layer 5 & 62.2\% \\
\bottomrule
\end{tabular}
\caption{Table of the layers with the highest average accuracy after probing and the phoneme accuracy found on this layer for both English and Spanish. The wav2vec 2.0 encoder has 12 transformer layers with the first being layer 0 and the 12th being layer 11.}
\label{ed_tab:best_layer_average_accuracy}
\end{table}

Table \ref{ed_tab:best_layer_average_accuracy} shows the layers with the highest mean average accuracy for each model when tested on the two pretraining languages English and Spanish. We see in Table \ref{ed_tab:best_layer_average_accuracy} that the most common layer with the highest accuracy for the English dataset is the eighth layer of the model layer 7 with the second most common being layer 6. For the Spanish dataset the most common highest accuracy layer is the sixth layer layer 5 and the second most common is layer 7. These results are consistent with previous research that suggests that layers in this range have the highest accuracy for phoneme recognition \cite{english2022domain, pasad2021layer}. The probing results in the graphs in Section \ref{ed_sec:shared_vs_not_shared} and Section \ref{ed_sec:phonemes_classes} chose a single layer to take accuracy results from, in this case we chose layer 7 for all languages tested as it has consistently high accuracy for both datasets and is consistent with English et al. \cite{english2022domain}. 

\subsection{Metrics and Evaluation Criteria}
Here we list out all for the metrics used to track loss and evaluation during and after pretraining, finetuning and training our probes. For SSL pretraining we monitor progress on the model through contrastive loss and every 10k updates we finetune to evaluate the Word Error Rate (WER) to keep consistent with the fairseq pretraining tutorial evaluation metrics. For all finetuning experiments we train using Connectionist Temporal Classification (CTC) Loss and evaluate after training using Character Error Rate (CER) to keep consistent with the FLEURs finetuning experiments in Chapter \ref{workchapter2}. For training our layer-wise probes we train each probe using Cross-Entropy Loss and then evaluate the probes after training using phoneme accuracy to keep consistent with previous work \cite{english2022domain}.

\begin{table}[h]
\centering
\begin{tabular}{l|l|l|}
\cline{2-3}
                                  & \multicolumn{1}{c|}{Loss During Training} & \multicolumn{1}{c|}{Evaluation After Training} \\ \hline
\multicolumn{1}{|l|}{Pretraining} & Contrastive Loss                          & WER                                            \\ \hline
\multicolumn{1}{|l|}{Finetuning}  & CTC Loss                                  & CER                                            \\ \hline
\multicolumn{1}{|l|}{Probing}     & Cross Entropy Loss                        & Accuracy                                       \\ \hline
\end{tabular}
\caption{Table of Loss functions for each stage of training and evaluation metrics used after training}
\end{table}
%%%%%%%%%%%%%%%% REMOVED %%%%%%%%%%%%%%%%%%%%
\iffalse
\section{Distillation-Based Pretraining}

\subsection{Motivation and Setup}
\textbf{chatgpt generated!!}

Before arriving at our current approach of multilingual self-supervised pretraining, we explored model distillation as an alternative strategy for compressing speech representations from a large pretrained model into a smaller one. The teacher was the 24-layer XLS-R model, and the student was a 12-layer Wav2Vec2-style model initialized with random weights.

\subsection{Experimental Design and Training Loss Trends}
\textbf{chatgpt generated!!}

The distillation objective was to minimize the Mean Squared Error (MSE) between each layer of the student model and every layer of the teacher. The total loss was computed by summing all pairwise MSEs across layers (e.g., layer 2 of the student with layer 4 of the teacher). Only the student model's parameters were updated during training; the teacher model remained frozen.

This setup was tested using both small and large pretraining datasets. In all cases, the MSE loss declined steadily and reached low values, suggesting the student had learned to approximate the internal structure of the teacher.

\subsection{Downstream ASR Performance}
\textbf{chatgpt generated!!}
Despite the success in minimizing MSE loss, fine-tuning the distilled models for ASR tasks yielded disappointing results. Character Error Rates (CER) were significantly higher than those obtained from XLS-R or standard Wav2Vec2 models pretrained with contrastive loss. Larger distillation datasets did produce lower MSEs than smaller ones, but the CER results after fine-tuning remained similarly poor across both setups. A control experiment with no distillation (random initialization + fine-tuning only) resulted in even worse CER, confirming that distillation was not entirely ineffective—but it was not sufficient either.

\subsection{Limitations of Teacher-Student Distillation}
\textbf{chatgpt generated!!}

These findings highlight a disconnect between internal representational similarity and downstream ASR utility. Although the student learned to mimic the layer-wise structure of the teacher to some extent, the absence of a contrastive or predictive pretraining signal limited the model's capacity to generalize to speech recognition tasks.

This limitation was a major factor in our transition toward multilingual self-supervised pretraining, where task-specific learning objectives better aligned with the demands of downstream ASR. The next sections detail this new approach.
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Experimental Results}\label{ed_sec:experimental_results}

In this section we cover the results of our experiments as described in Section \ref{ed_sec:experimental_design}. First in Section \ref{ed_subsec:same_data_finetuning} we analyse the results of our finetuning experiments to English and Spanish data from data included in pretraining. Next in Section \ref{ed_subsec:gain_across_languages} we test whether English and Spanish data have the same behaviours when they are finetuned from unseen data that was not included in pretraining. We then further analyse these patterns by including data from multiple languages from several different language groups: Germanic, Romance and Slavic. 

In Section \ref{ed_sec:shared_vs_not_shared} we explore how Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models learn speech representations by probing the encoder layer for phoneme accuracy across multiple languages. We test the accuracy of phonemes from several groups: phonemes that exist in both Spanish and English, phonemes unique to English and phonemes unique to Spanish. We test these different groups in both English and Spanish for every one of our pretrained models. We then measure phoneme accuracy for French and Polish and compare phonemes that are shared by both English and Spanish to phonemes unique to French and phonemes unique to Polish. Finally in Section \ref{ed_sec:phonemes_classes} we expand our analysis of English, Spanish, French and Polish phonemes to measuring accuracy across different phoneme groups. The groups include: vowels, diphthongs, approximants, nasals, tap \& trills, stops, affricates, fricatives and closures. 

\subsection{Effect of Pretraining Composition on Finetuning}\label{ed_subsec:same_data_finetuning}

In this section we will evaluate the applied character level recognition when each of the models outlined in Section \ref{ed_pretraining_configuration} are finetuned to data from the datasets that they were pretrained on LibriSpeech and the Spanish subset of Multilingual Librispeech. The original wav2vec 2.0 model trained by meta is evaluated by first pretraining on the entire 960 hour LibriSpeech dataset and then finetuning to various smaller subsets of the same dataset. Figure \ref{ed_fig:fine_tuning_100hours} shows the results of finetuning each of our pretrained models to 100 hours of the either LibriSpeech or MLS Spanish data, both sets of data had already been seen in the pretraining data.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_engspamls100h.pdf}
    \caption{CER when models pretrained with varying ratios of English and Spanish data are finetuned to 100 hours of English or 100 hours of Spanish data }
    \label{ed_fig:fine_tuning_100hours}
\end{figure}%\label{fine_tuning_100hours}

Figure \ref{ed_fig:fine_tuning_100hours} shows the Character Error Rate (CER) results obtained after finetuning each of our pretrained models on 100 hours of labelled data from the same datasets used during pretraining—LibriSpeech (English) and MLS Spanish. When finetuning to English data from the 100 hour LibriSpeech subset we see that the lowest CER is on the model trained only on English LibriSpeech data and the highest CER is on the model is found on the model trained only on Spanish MLS data. When finetuning to the Spanish MLS 100 hour subset the lowest error is found on the model trained on 800 hours of MLS Spanish and 100 hours of English LibriSpeech, while the highest error is found on the model trained on 800 hours of English LibriSpeech and 100 hours of Spanish MLS data. The model trained on only English data also has some of the highest error when finetuning to Spanish and inversely the model trained only on Spanish has some of the lowest.

These results show that while SSL ASR pretraining only learns broad phonetic representations it still has an impact on how the model learns semantic information. However, since the models have been finetuned to subsets of the same data they were trained on we cannot at this stage determine if the pretraining has overfitted to the pretraining data or whether there is linguistic information specific to each language that is being learned during pretraining.

\subsection{Multilingual and Cross Lingual Transfer Gains}\label{ed_subsec:gain_across_languages}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/linegraph_engspafleurs.pdf}
    \caption{CER when each pretrained model is finetuned to unseen English and Spanish data, the finetuning data consists of 10 hours each from the FLEURs dataset}
    \label{ed_fig:multilingual_linegraph}
\end{figure}
To test the impact the SSL ASR pretraining has on downstream language learning we design a new experiment. In this section we fine-tune each of our models to data from multiple different languages from several different language groups in order to observe the effect that learning broad speech representations from one or multiple languages affects the finetuned error rates of unseen data and unseen languages in pretraining. Figure \ref{ed_fig:multilingual_linegraph} shows the results of finetuning our models to 10 hour English and Spanish subsets for the FLEURs dataset, this data has not been seen in pretraining but are in the same languages the models was pretrained on. We see the same pattern replicated here as in Figure \ref{ed_fig:fine_tuning_100hours} with the CER when finetuning to English being lowest on the models with the most English data and the CER when finetuning to Spanish being lowest on the models with the most Spanish data. This shows that the phonetic information learned in pretraining is language-specific and affects the finetuning accuracy even when the finetuning data was unseen in pretraining. It should also be noted that the Spanish CER is lower at every point than the English CER, this is consistent with the results found with FLEURs in Section \ref{workchapter2}.



Having observed the same pattern when finetuning our SSL models on both seen and unseen English and Spanish data, next we can test whether the pretraining data affects the learning of different language families. Figure \ref{ed_fig:multilingual_barchart} shows the difference in CER for multiple languages from the FLEURs dataset. Each of the 2 monolingual are finetuned to the same language and the difference in CER is taken. A positive difference reflects higher CER on the English monolingual model and a negative difference reflects higher CER on the Spanish monolingual model.

\begin{equation}\label{ed_equ:CER_differrence_mutlilingual}
\centering
    CER\_on\_monolingual\_English - CER\_on\_monolingual\_Spanish
\end{equation}

All of the error rates on every model for each FLEURs language can be found on Figure \ref{app_fig:multilingual_linegraph} in Appendix \ref{app_sec:chapter_5_appendix}.

\newpage
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/barchart_abs_diff.pdf}
    \caption{Absolute Character Error Rate (CER) change between the monolingual English model and the monolingual Spanish model when finetuned on multiple downstream languages}
    \label{ed_fig:multilingual_barchart}
\end{figure}
In Figure \ref{ed_fig:multilingual_barchart} we see that the most positive change in CER occurs in English whilst the most negative change in CER occurs in Spanish. These show that the monolingual models react most positively to the languages that they were originally pretrained in. The next most positive languages we tested were Danish and German, these are both Germanic languages the same language family as English. The languages that are most negative are Asturian, Catalan and Italian which are all Romance languages which is the same language family as Spanish. Of the three remaining languages Russian shows the smallest negative change in CER while French gives the lowest positive change and Polish shows lower CER on the English monolingual model but a smaller difference between the two monolingual models than all of the Germanic languages tested. The Slavic languages do not show a clear trend towards either language in the pretraining, with both positive and negative change in CERs. French acts as an outlier among the Romance languages, showing higher error on the English monolingual model. These results seem to suggest that the languages that our SSL ASR models are trained on affect the downstream error of languages within the family of original pretraining language. To investigate these results further we must measure exactly how SSL ASR models learn phonetic information and how language balance in pretraining may impact this. 

\newpage
\subsection{Shared vs. Unique Phoneme Accuracy} \label{ed_sec:shared_vs_not_shared}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/phoneme_shared_notshared_line_graph_updated.pdf}
    \caption{Phoneme accuracy taken at layer 7 of each of our models for English and Spanish data. The phonemes are split into groups of phoneme that are either shared between Spanish and English languages or phonemes that are unique to either English or unique to Spanish.}
    \label{ed_fig:phonemes_shared_not_shared}
\end{figure}

Figure \ref{ed_fig:phonemes_shared_not_shared} shows the phoneme accuracy gathered from our probing experiments outlined in Section \ref{ed_seubsec:exp_des_probing} full lists of the phonemes measured and their accuracy for each language can be found in Appendix \ref{app_sec:chapter_5_appendix}. We separate our phonemes into four different groups of phonemes and take the average accuracy on each model for each group. The phoneme groups are: English-Only, English-Shared, Spanish-Only and Spanish-Shared. English-Only contains phonemes that are only found in the English dataset and Spanish-Only contains phonemes that are only found in the Spanish dataset. The phonemes in the English-Only and Spanish-Only groups are then phonemes that only exist in each language and are only tested on the relevant language, i.e. English-Only are only tested on English data. The values in Figure \ref{ed_fig:phonemes_shared_not_shared} are the mean average of all the phonemes in each group when tested on each of the models outlined in Section \ref{ed_subsec:datsets}. %A list of the phonemes found in the English-Only and Spanish-Only groups can be found in Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phonemes_only_groups}.

In Figure \ref{ed_fig:phonemes_shared_not_shared} we first look at the English-Only group of phonemes, here we see a difference of 21\% between the phoneme accuracy recorded between the monolingual English model (960 / 0) and the Spanish monolingual model (0 / 920). While the accuracy does not consistently decrease from left to right of Figure \ref{ed_fig:phonemes_shared_not_shared} as the amount of English data in pretraining, we do see that the English-Only phonemes have the highest accuracy on the models with the most English data included in pretraining. For the Spanish-Only phoneme group we see the inverse trend with a difference of -14\% between the English monolingual and Spanish monolingual models, showing higher accuracy for Spanish-Only phonemes on the model only trained on Spanish pretraining data. Similarly, we see an increasing of accuracy from right to left of Figure \ref{ed_fig:phonemes_shared_not_shared} as more Spanish data is introduced in pretraining. 

For the phoneme group English-Shared in Figure \ref{ed_fig:phonemes_shared_not_shared} there is an absolute difference of 19.24\% between the accuracy on the monolingual English model and the monolingual Spanish model. This group also exhibits the same behaviour as English-Only with accuracy increasing from right to left of the graph as more English data is introduced to the pretraining. Spanish-Only also exhibits the same pattern, having a change in accuracy of -5.32\% between the English and Spanish monolingual models and an increase in accuracy with models with more Spanish data in pretraining. This shows that even when phonemes exist in both languages they are not learned equally well and the models are learning more than discrete phoneme representations, such as coarticulations between phonemes that might only exist in one language. 

Finally, we can note that in Figure \ref{ed_fig:phonemes_shared_not_shared} that the average phoneme accuracy for the English-Shared group is on every model higher than the average accuracy for the English-Only phoneme group. It is also true that the phoneme accuracy is higher on every model for the Spanish-Shared phoneme group when compared to the Spanish-Only phoneme group. This suggests that while shared phonemes are better recognised when they are spoken in a single language, phonemes that exist in both pretraining datasets still benefit from more examples in training.

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{5_chapter5/figures/barchart_shared_not_shared_fre_pol.pdf}
    \caption{Average phoneme for French and Polish when tested for phonemes that exist in either English or French vs Phonemes that are unique to French or Polish}
    \label{ed_fig:phonemes_shared_french_polish}
\end{figure}

Figure \ref{ed_fig:phonemes_shared_french_polish} shows the average phoneme accuracy for several different groups: phonemes that exist in both the French language and English or Spanish, Phonemes that exist in French but not in English or Spanish, then Phonemes that exist in Polish and also in English or Spanish and finally Phonemes that only exist in Polish but not in English or Spanish. Both languages were taken from the Multilingual LibriSpeech (MLS) dataset \cite{MLS} and phonemes found by using the Montreal Forced Aligner (MFA) \cite{mcauliffe2017montreal}. 

In all models in Figure \ref{ed_fig:phonemes_shared_french_polish} the French language data has higher accuracy on phonemes that exist in both French and English or Spanish while having lower average accuracy on phonemes that are unique to French and do not exist in either of the pretraining languages, English or Spanish. This pattern stays true with the accuracy for Polish data with average accuracy for phonemes that exist in both Polish and English or Spanish having a higher average accuracy when compared to phonemes that only exist in Polish and not in English or Spanish.
\iffalse
Throughout this section we have shown that phonemes that overlap between the English phoneme sets and the Spanish phoneme sets have the highest accuracy due to them existing in all wav2vec 2.0 models we have pretrained. Phonemes that are unique to English have significant accuracy increases when tested on models with high ratios of English data in pretraining. The inverse is true for Spanish with the highest accuracy phonemes that are unique to Spanish being recorded on the model trained only on Spanish pretraining data. Figure \ref{ed_fig:phonemes_shared_french_polish} shows that for both the French language and Polish language phonemes that exist in either language as well as English or Spanish always achieve higher accuracy than phonemes that are unique to each language. We have seen in Section \ref{ed_subsec:gain_across_languages} that the best approach for low error transcription is to pretrain the model on the language you are finetuning or a language closely related to it.
\fi

The information in Figure \ref{ed_fig:phonemes_shared_french_polish} suggests that language is not the only factor in determining whether a phoneme will be recognised with high accuracy. The findings suggest that if we were carefully select pretraining data to include certain phonemes in high number we may be able to achieve higher accuracy even if our testing language was not included in or related to the languages in pretraining. To further dissect this idea we can evaluate performance across different phoneme groups and different languages. %\thoughts{This section could probably be moved to discussion LOOK HERE ED!!!}

%\newpage

\subsection{Phoneme Accuracy in Phoneme Classes}\label{ed_sec:phonemes_classes}

 %%% FULL TABLE WITH ALL PHONEMES AND ALL VARIATIONS %%%%%%%%%
\iffalse
\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{ Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
%Vowel & \textipa{i, i:, {\ae}, 5, A, A:, 6, 6:, @, \textrhookschwa, E, \textrhookrevepsilon, \i, 0, 0:, U} \\ %% All 
& Vowel & \textipa{i, i:, {\ae}, 5, A, A:, 6, 6:, @, \textrhookschwa, E, \textrhookrevepsilon, \i, 0, 0:, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, b\super j, c, c\super h, c\super w, d, d\super j, \|[{d}, k, k\super h, k\super w, p, p\super h, p\super j, p\super w, t, t\super h, t\super j, t\super w, \|[{t}, \textObardotlessj, \textObardotlessj\super w, g, g\super w, P} \\
& Nasal & \textipa{m, m \super j, \textsyllabic{m}, n, \textsyllabic{n}, N, M, \textltailn} \\
& Fricative & \textipa{f, f\super j, h, s, v, v\super j, z, \c{c}, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \textltilde, \textsyllabic{\textltilde}, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R, R\super j, \~{R}} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c,  \|[{d}, k, p, \|[{t}, \textbardotlessj, \textbardotlessj J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, \c{c}, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, \~{A}, O, \~{O}, \textschwa, E, \~{E}} \\
& Stop & \textipa{b, c, d, k, p, t, \textObardotlessj, g} \\
& Nasal & \textipa{m, m\super j, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, \~{O}, E, \~{E}, 1} \\
& Stop & \textipa{b, b\super j, c, \|[{d}, k, p, p\super j, \|[{t}, t\super j, \textObardotlessj, g, P} \\
& Nasal & \textipa{m, m\super j, \|[{n}, N, \textltailn} \\
& Fricative & \textipa{f, f\super j, \|[{s}, v, v\super j, x, \|[{z}, \c{c}, C, \:s, \textcommatailz, \textctz} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, \|[{d}\|[{z}, t\c{c}, t\:s, \|[{t}\|[{s}} \\
& Approximant & \textipa{j, \~{j}, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, r\super j} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used.}
\label{tab:phoneme_groups_by_language}
\end{table}
\fi
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%% change layout to phoneme groups being in a separate column to language
%\iffalse
%\newpage
\begin{table}%[h]
\centering
\begin{tabular}{llp{0.5\linewidth}}
\toprule
\textbf{Language} & \textbf{Phoneme Group} & \textbf{Phonemes} \\
\midrule
\multicolumn{3}{l}{\textbf{English}} \\
%Vowel & \textipa{i, iː, æ, ɐ, ɑ, ɑː, ɒ, ɒː, ə, ɚ, ɛ, ɝ, ɪ, ʉ, ʉː, ʊ} \\
& Vowel & \textipa{i, {\ae}, 5, A, 6, @, E, \textrevepsilon, \i, u, U} \\
& Diphthong & \textipa{aj, aw, ej, ow, @w} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g, P} \\
& Nasal & \textipa{m, n, N, M, \textltailn} \\
& Fricative & \textipa{f, h, s, v, z, c, D, S, Z, T} \\
& Affricate & \textipa{dZ, tS} \\
%Approximant & \textipa{j, l, w, ɫ, ɫ̩, ɹ, ʎ} \\
& Approximant & \textipa{j, l, w, \*r, L} \\
& Closure & \textipa{sil} \\
%Tap \& Trill & \textipa{ɾ, ɾʲ, ɾ̃} \\
& Tap \& Trill & \textipa{R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Spanish}} \\
& Vowel & \textipa{a, e, i, o, u} \\
& Stop & \textipa{b, c, k, p, \j, \j J, g} \\
& Nasal & \textipa{m, n, N, M} \\
& Fricative & \textipa{f, s, x, c, D, G, S, J, B, T} \\
& Affricate & \textipa{tS} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r, R} \\
\addlinespace
\multicolumn{3}{l}{\textbf{French}} \\
& Vowel & \textipa{a, e, i, o, u, y, \o, \ae, A, O, \textschwa, E, \oe} \\
& Stop & \textipa{b, c, d, k, p, t, \j, g} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, z, K, S, Z} \\
& Affricate & \textipa{dZ, ts, tS} \\
& Approximant & \textipa{j, l, w, y, L, 4} \\
& Closure & \textipa{sil} \\
\addlinespace
\multicolumn{3}{l}{\textbf{Polish}} \\
& Vowel & \textipa{a, i, u, O, E} \\
& Stop & \textipa{b, c, d, k, p t, \j, g, P} \\
& Nasal & \textipa{m, n, N, \textltailn} \\
& Fricative & \textipa{f, s, v, x, z, c, C, s} \\
%& Affricate & \textipa{dz, tc, ts} \\
& Affricate & \textipa{d\textcommatailz, d\textctz, t\c{c}, t\:s} \\
& Approximant & \textipa{j, l, w, L} \\
& Closure & \textipa{sil} \\
& Tap \& Trill & \textipa{r} \\
\addlinespace
\bottomrule
\end{tabular}
\caption{Table of phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used, including those reflecting phonetic variations. This table is simplified so that variations are nor included, a full list of all MFA phoneme tokens for each group can be found in the Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phoneme_groups_by_language}.}
\iffalse
\caption{Phonemes per language grouped by articulatory class. All of the available MFA tokens for all languages are used, including those reflecting phonetic variations such as extended duration of vowels (\textipa{i:, A:}), labialization (\textipa{\textObardotlessj\super w,  p\super w}), aspiration (\textipa{p\super h, k\super h}), palatalization (\textipa{R\super j, m\super j}), nasalisation (\textipa{\~{E}, \~{A}}), syllabification (\textipa{\textsyllabic{n}, \textsyllabic{\textltilde}}), dental placement \hl{noun?} (\textipa{\|[{d}, \|[{t}}) and rhotacization (\textipa{\textrhookschwa, \textrhookrevepsilon}). \hl{Extra variants} (\textipa{0, \textObardotlessj, \c{c}, \textltilde, \textbardotlessj J, 1, \:s, \textcommatailz, \textctz, d\textcommatailz, d\textctz, t\c{c}, t\:s, C})Phonetic variations are not included in the table, for full list of tokens used see Appendix table \ref{app:phoneme_groups_by_language} \hl{will have to revisit and reqrite this table description, still many variations included}}
\fi
\label{ed_tab:phoneme_groups_by_language}
\end{table}
%\fi

To further test this theory we can break down our phoneme data into sub-groups, this will allow us to explore whether the language or the phoneme groups are more important for recognition. Table \ref{ed_tab:phoneme_groups_by_language} shows the breakdown of phonemes in each phoneme group, the phonemes are selected from all of the phonemes in the MFA dictionaries for English and Spanish. All phonemes modifications, such as \textipa{a:} or \textipa{p\super j,}, are also included in the group with their unmodified phonemes. The full list of phoneme used in each language can be found in Appendix \ref{app_sec:chapter_5_appendix} Table \ref{app_tab:phoneme_class_acc_with_diff}.

The groups of phonemes chosen follow the same groupings as found in English et al. \cite{english2022domain} with some exceptions. All vowels were grouped together with any stessers also included. Diphthongs, that only exist in the English MFA dictionary, were separated into their own group, in order to test whether coarticulation between the phonemes may be relevant to the pretraining languages. Closure simply includes the "sil" token which denotes a lack of any sound for a short period of time. Finally the phonemes \textipa{r} and  \textipa{R} were separated into their own group, Tap \& Trill, while they could have been included in the Approximant group both \textipa{r} and \textipa{R} are much more frequent in the Spanish data so may be affected by pretraining data more than other approximants.

Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} shows a breakdown of the average accuracy of different phoneme classes from the validation data from the English LibriSpeech dataset, the Multilingual LibriSpeech (MLS) Spanish subset, the MLS French subset and the MLS Polish dataset. The groups show accuracy from layer 7 of each of the 7 pretrained models for the nine different phoneme groups described in Table \ref{ed_fig:Phonemes_Groups_English_linegraph}. Figure \ref{ed_fig:Phonemes_Groups_English_barchart} shows the difference in accuracy of each phoneme group for each language when taken between the accuracy of the monolingual English model trained only on LibriSpeech data and the monolingual Spanish model trained only on the Multilingual LibriSpeech (MLS) Spanish subset as shown in equation \ref{ed_equ:CER_differrence_phoneme_groups}.

\begin{equation}\label{ed_equ:CER_differrence_phoneme_groups}
\centering
    CER\_on\_monolingual\_English - CER\_on\_monolingual\_Spanish
\end{equation}

In the vowel plot on Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we see that Spanish vowels have the highest accuracy across all models being above 80\% in all cases except the English monolingual model (960 / 0). Notably, Spanish has the fewest total individual vowels of all of the languages as seen in Table \ref{app_tab:phoneme_groups_by_language}, Spanish has no variants to the phonemes in the vowels which can occur less frequently in both training and validation data and therefore lower accuracy the average accuracy. It is also notable that the Polish vowels have the highest accuracy on the balanced mixed model (450 / 450), this could imply that when the testing language is unrelated to the pretaining languages individual phonemes are more impactful than the relationships between phonemes. Figure \ref{ed_fig:Phonemes_Groups_English_barchart} confirms that Polish vowels have the least difference between accuracy on each of the monolingual models, whilst English has the highest difference. 

Dipthongs were separated into a separate figure from vowels in order to evaluate whether language specific coarticulation affected accuracy, Dipthongs were only available in the English MFA dictionary. We see in \ref{ed_fig:Phonemes_Groups_English_linegraph} that English Dipthongs have higher accuracy on every model than their vowel counterparts, possibly due to the reduced set. English Dipthongs also have a higher difference between accuracy on monolingual models of 16.2\%. A full list of results for every model and the difference between the monolingual models can be found in Appendix \ref{app_sec:chapter_5_appendix}, Table \ref{app_tab:phoneme_class_acc_with_diff}.

For approximants in both Spanish and English the highest phoneme accuracy exists on their respective monolingual models. While the difference between models for Polish in Figure \ref{ed_fig:Phonemes_Groups_English_barchart} is in favour of the English monolingual model accuracy for Polish approximants is also high for the 800 / 100 model and the 300 / 600 model. French on the other hand has little difference between accuracy on either of the monolingual models and little variation for approximants across all models.

For nasals Figure \ref{ed_fig:Phonemes_Groups_English_barchart} shows all four languages having higher accuracy on the English monolingual model than on the Spanish monolingual model. Spanish has higher accuracy for nasal phonemes on the English monolingual when compared to the Spanish monolingual model, although it should be noted that Spanish nasals have the highest accuracy on the 100 / 800 model, so still a model with mostly Spanish training data. This may however show some nuance to the idea that all speech data benefits from monolingual or single domain pretraining. If a model is better at recognising phonemes from a a certain group this may be more beneficial for downstream recognition than simply pretraining on the same language. Since English, French and Polish also show higher accuracy on the English monolingual model this may support that theory. 

For the Tap \& Trill category Spanish has higher accuracy across all models when compared to both Polish and English and has 9.6\% more accuracy on the Spanish monolingual model when compared to the English monolingual mode. While in Figure  \ref{ed_fig:Phonemes_Groups_English_barchart} English has 15.3\% higher accuracy on the English monolingual model when compared to the Spanish monolingual model. However, on Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we can see that there is also an 11.8\% difference between the English monolingual model (960 / 0) and the model with balanced pretraining data (450 / 450), since the English accuracy for this group does not go above 33\% this suggests that recognition for phonemes in this group is not as dependant on the pretraining data as other groups. Similarly for Polish the average accuracy is low across all groups with the highest accuracy on the Spanish monolingual model at 21.4\%. While Polish does have higher accuracy on the Spanish monolingual model than on the English monolingual model it seems that phonemes in this group have low accuracy regardless of pretraining data. %\thoughts{do I need to come back to this when I know what's happening in the pretraining data?}

For Stops we see the pattern shown in previous sections with English phonemes having higher accuracy on the English monolingual model when compared to the Spanish monolingual model with a difference of 19.3\%. The Inverse is then true for Spanish, although to a lesser degree, with Spanish spoken phonemes having higher accuracy on the Spanish monolingual model and lower on the English model although this time with a difference of 1.3\%. Interestingly, the highest accuracy for Spanish is found on the balanced model (450 / 450), suggesting Spanish is benefitting from balanced monolingual pretraining in this category. French and Polish both have higher accuracy on the English monolingual model when compared to the Spanish monolingual model. This effect may be due to English having a larger variation of phonemes in this group when compared to Spanish, this may be beneficial for accuracy for phonemes in this group. French nasal phonemes have the highest accuracy across all models except the 300 / 600 model. 

For Affricates Polish shows the least variation with only 1.4\% difference between accuracy on the English and Spanish monolingual models. English has the highest accuracy on the English monolingual model and the lowest on the Spanish monolingual model showing preference to pretraining on the same language. Spanish however, has higher accuracy on the English monolingual model when compared to the Spanish monolingual model. The highest recorded accuracy is on the 450 / 450 model while the lowest is on the 300 / 600 model, giving Spanish the most variation of the languages in this category. As seen in Table \ref{app_tab:phoneme_groups_by_language} Spanish only contains one phoneme in this category, \textipa{tS}, and there are only 391 examples of the phoneme in the training data for the probes as seen in Table \ref{app_tab:phoneme_counts_per_language_train_10h_3}, when compared to phonemes that often have more than 10,000 examples. Training probes to recognise a single phoneme, with few examples may contribute to the variation in this case.

For Fricatives we see English again having higher accuracy on the English monolingual model when compared to the accuracy on the Spanish monolingual model. Spanish fricatives then have the highest accuracy on the Spanish monolingual model and the lowest on the English monolingual model. Both French and Polish have higher accuracy across all models than either English or Spanish, which is not true of any other phoneme group.

Finally for the closure group for French, Polish and English the accuracy on every model is higher than the accuracy in any other group for each model and for Spanish the only higher accuracy is found an all model except the English monolingual model in the vowel category. The closure category only contains the silence token (sil) given by the Montreal Forced Aligner when no sound is made. The high accuracy in Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} across all languages and the lack of clear preference to either language in Figure \ref{ed_fig:Phonemes_Groups_English_barchart} suggests that silence is recognised accurately regardless of the model or the language the closure exists in.

Overall the findings in this section show that phoneme accuracy is not consistent across different categories of phonemes. As seen in nasals and affricates the concept that pretraining on the same or a related language always increases accuracy is not always true and in the case of Spanish stops and nasals, a balanced mix of pretraining can give the highest accuracy. In most cases however phonemes spoken in English do have higher accuracy on the models with the most English in pretraining and phonemes spoken in Spanish have the highest accuracy on the models with the most Spanish data in pretraining. 

For French, in all categories except closure phonemes in this language have higher accuracy recorded on the monolingual English model when compared to the monolingual Spanish model, which is consistent with the findings in Figure \ref{ed_fig:multilingual_barchart}. Accuracy is not consistent across all categories with a difference of 38\% between nasals and fricatives on the monolingual English model. For Polish, average accuracy is also inconsistent across categories, but is also less consistent across models with vowels having the highest accuracy on the 450 / 450 and approximants performing well on both the 800 / 100 model and the 300 / 600 model. 

These findings show that when the testing language is not in pretraining they do not necessarily benefit from monolingual training. 

%\thoughts{I feel that I need to expand this, I could possibly do this in the discussion section though}


%\newpage
\begin{figure}[H]
    %\centering
    \includegraphics[height=0.9\textheight, width=0.75\paperwidth]{5_chapter5/figures/Rotate_phoneme_groups_line_graph_wpolish.pdf}
    \caption{Phoneme accuracy for 9 different phoneme groups. Accuracy is shown across all 7 pretrained models for four different languages: English, Spanish, French and Polish.}
    \label{ed_fig:Phonemes_Groups_English_linegraph}
\end{figure}

%\newpage
\begin{figure}[H]
    %\centering
    \includegraphics[height=0.9\textheight, width=0.75\paperwidth]{5_chapter5/figures/Rotate_phoneme_groups_bar_chart_wpolish.pdf}
    \caption{The difference between the accuracy of 9 different phoneme groups when measured between the English monolingual model and teh Spanish monolingual model. Accuracy is shown across all 7 pretrained models for four different languages: English, Spanish, French and Polish.}
    \label{ed_fig:Phonemes_Groups_English_barchart}
\end{figure}


\newpage
\section{Discussion}
This section started by training 7 models, one model trained only on the data from the LibriSpeech dataset \cite{librispeech}, one model trained only on the Spanish subset of Multilingual LibriSpeech \cite{MLS} and 5 models trained on different ratios of these 2 datasets, always adding up to 900 hours.The ratios of hours of speech data with English of the left and Spanish on the right (Eng / Spa) were as follows: 800 / 100, 600 / 300, 450 / 450, 300 / 600 and 100 / 800. As shown in previous work \cite{ashihara2023exploration, poncelet2021comparison}, we prove in Section \ref{ed_subsec:same_data_finetuning} that the languages in your pretraining affect downstream finetuning. Figure \ref{ed_fig:fine_tuning_100hours} shows that when there is more English data in pretraining the model has lower Character Error Rate (CER) after finetuning to English data. Similarly when more Spanish data is present in pretraining lower CER is found after finetuning to Spanish. This is then replicated with unseen FLEURs data in Section \ref{ed_subsec:gain_across_languages} Figure \ref{ed_fig:multilingual_linegraph}.

We then confirm findings in \cite{zhang2023fast, gupta2021clsril} with our models that prove that languages that are closely related to the language most prominent in pretraining data for a model achieve lower CER when compared to finetuning them on models pretrained on a language that is unrelated to the finetuning data. Figure \ref{ed_fig:multilingual_barchart} Romance languages such as Catalan, Asturian and Italian have lower CER on the monolingual Spanish model when compared to their CER on the monolingual English model. We can also see in Figure \ref{ed_fig:multilingual_barchart} that the Germanic languages such as Danish and German have a lower CER on the monolingual English model when compared to their results on the monolingual Spanish model, showing that languages related to English benefit from more English in pretraining. 

French shows lower error on the English monolingual model but less so than the Germanic languages, possibly due to it's phonetic overlap with English \cite{roth2010explore} despite being classified as a romance language \cite{bauer2007linguistics}. The two Slavic languages Russian and Polish show lower error on different models with Polish having a preference for English pretraining data and Russian having a preference for Spanish pretraining data. This implies that when a language is unrelated to the language most prominent in the pretraining data it does not directly benefit in accuracy.

Next we look to delve deeper into why related languages benefit each other when relating to pretraining and finetuning. The wav2vec 2.0 Self-Supervised Learning (SSL) pretraining stage uses contrastive loss to recreate the input to the encoder's first layer of the transformer encoder to  at the encoder's final layer. Pasad et al. \cite{pasad2021layer} show that wav2vec 2.0 learns different speech tasks in different layers of the encoder such as local acoustic features in the early shallow layers and phone identity in the late middle layers. They also show that when finetuned to a semantic task of producing English sentences the final layers change task from reconstruction of the input to grapheme output, while the middle to early layers do not change to the same extent. English et al. \cite{english2022domain} then use single linear layers to probe the English trained wav2vec 2.0 and test the phone recognition across each layer of the model. Similarly to Pasad et al. English finds the late middle layers having the highest phone accuracy, they however find that different phoneme group have varying accuracy across layers and across the phoneme groups. 

Using this information we can surmise that the phonetic but not semantic information learned during pretraining has a significant impact on the final error rate after finetuning and that this learned information will not change. This means that it is very important to understand how SSL ASR models are learning phonetic information, in our case we can take this idea further and look to discover how mixing languages affects pretrained phone recognition. We use the same technique as English et al. \cite{english2022domain} and probe each layer for phone recognition, we then select one of the highest performing layer (layer 7) and measure how well our models recognise different groups of phonemes. 

In Section \ref{ed_sec:shared_vs_not_shared}, we collect the phoneme accuracy data for all of the phonemes found in the English LibriSpeech dataset and the Spanish subset of MLS. For our first experiment we group phonemes by whether they appear in both datasets and therefore the pretraining data for all models vs phonemes that are unique to either English or Spanish. Figure \ref{ed_fig:phonemes_shared_not_shared} shows that when tested on English data only phonemes that are shared between English and Spanish and phonemes that are unique to English both perform best on the English monolingual model when compared to their performance on the Spanish monolingual model. However, we also see that on every model, phonemes that are shared between English and Spanish always have higher accuracy than uniquely English phonemes. 

When testing on Spanish data both phonemes shared between English and Spanish and phonemes unique to Spanish performing best on either the monolingual Spanish model or the 100 / 800 Eng / Spa model. However, for both sets, we again find that phonemes exist in both pretraining datasets perform better on every model. These Findings suggest that individual phonemes are still affected by which language they are spoken in, possibly due to coarticulation between different phonemes. They also show that these phonemes will achieve higher accuracy when the model has seen more examples of that phoneme regardless of the language it is spoken in as with the two shared phonemes groups.

Next we test the same phoneme groups but this time when the data is from languages that were not included in the pre-training data. We chose data from the French and Polish subsets of MLS for this experiment, French because it was an outlier in the previous section, having higher accuracy on the models with more English data and Polish because it is not in the same language family as either English or Spanish. In Figure \ref{ed_fig:phonemes_shared_french_polish} we see that phonemes that exist in both Spanish and English consistently outperform phonemes that exist only in French. The same pattern is also true of Polish to varying ratios with the phonemes that exist in English and Spanish consistently outperforming phonemes that are unique to Polish. These results support our previous findings with the English and Spanish data but also suggest that carefully choosing pretraining data based on which phonemes are included could increase accuracy even for languages that have not been seen in pretraining. Alternatively, choosing pretraining data with less phonetic diversity could harm the model's performance.

Next in Section \ref{ed_sec:phonemes_classes} we further divide our languages into phonemes classes and analyse the trends across the four languages. We analyse the same groups as \cite{english2022domain} but separate out Diphthongs and Taps \& Trills. A full list of the Montreal Forced Aligner (MFA) phoneme tokens used for each category alongside the total number of phonemes in each group for the pretraining data for each model and the total number of each phoneme included in the training data for our probes can be found in Appendix \ref{app_sec:chapter_5_appendix}. 

For most classes English phonemes perform best on models pretrained with more English data and phonemes spoken in Spanish perform best on models pretrained with more Spanish data as seen in Figure \ref{ed_fig:Phonemes_Groups_English_barchart}. This effect is not true for the Nasals or Affricate categories where when spoken in Spanish, they have higher accuracy on the monolingual English model when compared to the monolingual Spanish model. For affricates this may be explained by the number of phonemes in Spanish in this group, there is only one \textipa{tS} and although there is only one more affricate in English, \textipa{dZ} we can see in table \ref{app_tab:phoneme_token_class_counts_k} that the English model was trained on a total of 376k individual affricates whereas the monolingual Spanish model had only seen 90k examples. For Nasals both datasets contain more than 3M tokens in the affricate class, so it seems this unlikely to be the cause of this behaviour. Looking to Figure \ref{ed_fig:Phonemes_Groups_English_linegraph} we see that the 100 / 800, Eng / Spa, model actually has higher accuracy than the Spanish monolingual model or the English monolingual model at 61.7\%, compared to 49.4\% and 57.8\%. This suggests that a larger ratio of Spanish data still improves phoneme recognition.

For the Tap \& Trill category phonemes spoken in Spanish have higher accuracy than when spoken either English or Polish. Again we can see in Table \ref{app_tab:phoneme_token_class_counts_k} The pretraining data for the monolingual Spanish contains many more phonemes in this category for pretraining, with more than 2M, than the monolingual English monolingual model at less than 400k. Data for the Dipthong category was only available in English and the accuracy is high for every model when spoken in English. The accuracy on English spoken dipthongs reduces significantly however between the 100 / 800 model, going from 79\% accuracy to 69.6\%. The pretraining data for the monolingual Spanish model contained 0 dipthongs whereas the 100 / 800, Eng / Spa, model (containing only minimal English data) contains 206k diphthongs. This appears to show the importance of phonetic diversity in the pretraining data and that even a relatively small number of examples can dramatically increase accuracy when compared to an unseen phoneme.

We have also seen in the previous Chapter \ref{workchapter2}\info{previous chapter on pruning bias}  that a multilingual SSL ASR model relies heavily on weight attributed to English \cite{storeyedbiasSSL}. Some experiments were done collecting the Intersect Over Union (IOU) data for our 7 models similar to the experiments done in Section \ref{IOU_Section}\info{Section in previous chapter on IOUs} but ultimately not included in this cahapter. The results showed that more weights in the English monolingual were attributed to English and more weights in the Spanish monolingual model were attributed to Spanish. When more English data was included in pretraining more of the weights were attributed to English and when more SPanish data was in pretraining more weights were attributed to Spanish. These results confirm the findings in Chapter \ref{WC2}\info{previous chapter on pruning and bias} that the largest dataset in pretraining had the most influence over the weight distribution
\iffalse
Two consistent themes from the research in this chapter have emerged: downstream accuracy and error rates perform best when the pretraining data is the same or a related language to the downstream testing or finetuning language and that seen phonemes always performs better than unseen phonemes. As we have seen in \cite{ashihara2023exploration, poncelet2021comparison} when the pretraining data matches the language of the finetuning data we can achieve much lower error rates with less data and we have replicated this effect with our experiments in Sections \ref{ed_subsec:same_data_finetuning} and \ref{ed_subsec:gain_across_languages}. We then show in Section \ref{ed_sec:shared_vs_not_shared} and Section \ref{ed_sec:phonemes_classes} that this may be an issue of seen and unseen phonemes and phoneme interactions such as coarticulation. These sections show that while pretraining on a related language benefits the finetuning accuracy, so does pretraining on individual phonemes that exist in the finetuning data or finetuning language. 
\fi

%\thoughts{Do I need to include some pruning IOU data that I collected to support this? Maybe I should as it would better link it to the last chapter and make this feel a little less out of the blue I probably don't need to gvie it a whole seciton but it could be the appendix}. 

In this chapter we have proven that related languages are more likely to contain similar phonemes and coarticulations than unrelated languages, thus giving better accuracy. However, we have also shown that including more examples of an individual phoneme can increase the accuracy for that phoneme or group of phonemes even when the testing language is unseen and unrelated to the languages included in pretraining. Given these findings we suggest that multilingual SSL ASR pretraining data should be balanced by phoneme content to include as many and as diverse a phoneme set as reasonably possible. By doing this, models will achieve higher accuracy on a broader range of languages with less pretraining data and on smaller models.


\newpage
\section{Future Work}
While phoneme accuracy and finetuned error rates may be better when language specific features are kept in the pretraining data the discovery that phoneme recognition can increase with language-agnostic training is important. 

For example, if a language, dialect or accent was very low-resource and did not have data from closely related languages in abundance SSL ASR could benefit from increasing the most common phonemes in said language by including data from unrelated languages. If we could design the pretraining data in this way we may be able to increase phoneme accuracy and lower error even for low-resource languages without having to resort to large models trained on large datasets. 

For future work, we would design a pretraining dataset that had the same phonetic makeup as a designated finetuning language. The pretraining data would include only data from languages unrelated to the one used in finetuning. We would expect such a model to achieve lower CER than a monolingual model trained on an unrelated language or a model trained on multilingual data that did not include the downstream language. 

Carefully designed pretraining data may even result in better performance than a model trained on multilingual data that contained small amounts of the finetuning language. This may be true as long as the finetuning target language was not the dominant language in pretraining, as per the behaviour exhibited by XLS-R in Chapter \ref{WC2}\info{previous chapter on pruning and bias}. These models could then all be compared to a monolingual model pretrained on a large dataset of the same language as finetuning, which we would expect to perform the best overall. 

If these experiments were successful we could have a simple method for increasing phoneme accuracy and CER for low-resource languages, dialects and accents. This method would only require the Montreal Forced Aligner and some knowledge of the target finetuning language to achieve.


\newpage
\section{Conclusion}
Over the course of this chapter we have dissected how Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models learn speech representations when pretrained with monolingual and multilingual training data. This gives us more insight into how SSL models learn and how we can leverage this knowledge for future work in this field. We started by pretraining 7 different versions of wav2vec 2.0 \cite{wav2vec} with different ratios of two different languages: English and Spanish. We pretrained 2 monolingual models, one on only English data from the LibriSpeech dataset \cite{librispeech} and one from only Spanish data from the Spanish subset of Multilingual LibriSpeech (MLS) \cite{MLS}. We then pretrained 5 more models with different ratios of the English and Spanish datasets.

We started in Section \ref{ed_subsec:same_data_finetuning} by fine-tuning to 100 hours subsets of LibriSpeech and MLS Spanish to observe the effect that our pretraining ratios had on finetuning results. We found that The English LibriSpeech data had the lowest Character Error Rate (CER) on the monolingual English model and the highest CER on the monolingual Spanish model with CER steadily increasing as the amount of English data was reduced in the pretraining data. For Spanish we found the inverse with Spanish data having the lowest CER on the monolingual Spanish model and the highest CER on the monolingual English model, with CER steadily increasing as the amount of Spanish data in pretraining was reduced.

In Section \ref{ed_subsec:gain_across_languages} we reproduced the results found in the previous section by testing the model with 10 hours of unseen English and Spanish data each from the FLEURs dataset \cite{FLEURS}. The same pattern was observed with the 10h English FLEURs dataset having the lowest CER on models with more English data in pretraining and Spanish. The 10h Spanish FLEURs shows the inverse pattern again by having the lowest CER on models with the most Spanish data and the highest CER on models with the least Spanish data. These experiments prove that pretraining on the same language as you are finetuning to gives the best results for ASR tasks.

In the same section we test multiple languages in the same languages families as English and Spanish, Germanic and Romance languages \cite{bauer2007linguistics}. We find that the Germanic languages Danish and German both have a positive difference towards English and the only language with a more positive difference than these languages is English. Similarly the Romance languages Asturian, Catalan and Italian have negative differences showing they perform better on the Spanish monolingual model when compared to the English monolingual model. French is an outlier among Romance languages showing a positive difference towards English. Finally we include 2 Slavic languages Polish and Russian, Polish shows a positive difference whereas Russian shows a negative difference. In this case Polish appears to benefit from English pretraining data and Russian benefits from Spanish Pretraining, which indicates that unrelated languages benefit much less from either pretraining languages. These findings confirm previous work that shows that pretraining on related languages can benefit downstream finetuning \cite{zhang2023fast, gupta2021clsril} however given that it has been shown that SSL ASR models learn phonetic characteristics and not semantic characteristics of speech \cite{pasad2021layer} it was important that we understand how the models are learning phonetic information when pretraining on multilingual data.

To understand how multilingual pretraining learns phoneme data we trained linear layer probes to recognise individual phonemes for each language. We collected the phoneme data using the Montreal Forced Aligner (MFA) and then created small 10 subsets of data from MLS to train the probes for phoneme recognition. In Section \ref{ed_sec:shared_vs_not_shared} after we test phoneme accuracy across all models for phonemes that exist in both English and Spanish data when compared to phonemes that are unique to either English or Spanish. Our findings show that the phonemes have the highest accuracy on the monolingual model trained on the same language they were spoken in, similar to the finetuning experiments in Section \ref{ed_subsec:gain_across_languages}. However, it is also notable that the English shared group has higher accuracy than the English Unique group across all models. It was also true that the Spanish Shared group had higher accuracy on every model when compared to the Spanish Unique group. 

This suggested that phonemes will have higher accuracy if there are more examples of it in pretraining even if those examples are spoken in different languages. We test this by introducing phoneme data collected from 2 new 10 hour subsets of MLS: MLS French and MLS Polish. These languages were chosen because French had outlier behaviour in Figure \ref{ed_fig:multilingual_barchart} and Polish is a Slavic language unrelated to English or Spanish. We split the phoneme accuracy data into groups, phonemes shared by English and Spanish spoken in each language and phonemes unique to French or Polish. The results showed that phonemes that are found in both English and Spanish always perform better than phonemes that are unique to either French or Polish even when the shared phonemes were spoken in French or Polish. This suggests that even when a phoneme is spoken in a language unrelated to either of the pretraining languages accuracy can be higher if the model has seen it in pretraining.

In Section \ref{ed_sec:phonemes_classes} we explored whether phoneme group accuracy trends across multilingual pretrained models behaved differently when spoken in 4 different languages: English, Spanish, French and Polish. We found that for English and Spanish in most cases phonemes would perform better when tested on a model with more data from their native language. The exceptions being Affricates where the models with the most English have the highest accuracy, however only one phoneme exists in the affricate class in Spanish and there are much less examples in the training data for Spanish. French and Polish phonemes were more likely to perform better on the models trained with more English data, with some exceptions. For Polish performance for the Tap \& Trill group was marginally higher for models with more Spanish data, which may be due to Spanish having more examples of the phonemes in this group in the pretraining set. 

Finally with the knowledge learned throughout this chapter we suggest future work that could be undertaken. We suggest that error rates on low-resource languages could be reduced by using data from languages that do not have to have any relation to the target finetuning language. A pretraining dataset could created using any languages as long and an appropriate number of all of the relevant phonemes in your target language were included in pretraining.

The consistent themes that emerged from this chapter were that SSL ASR benefits from pretraining on the same or related languages to the finetuning language and that this is true for phoneme recognition as well. The secondary theme is that when there are more training examples for an individual phoneme, accuracy can be increased on that phoneme, regardless of language specific interactions such as coarticulation. Overall, it has been seen throughout the chapter that selecting your pretraining data with care can be a great benefit in ASR and that there is more to SSL ASR than just the language relationships between your pretraining and finetuning languages.



%\keywords{, hello, world}

\nomenclature[Z]{CER}{Character Error Rate}
\nomenclature[Z]{WER}{Word Error Rate}
\nomenclature[Z]{SSL}{Self-Supervised Learning}
\nomenclature[Z]{ASR}{Automatic Speech Recognition}
\nomenclature[Z]{CNN}{Convolutional Neural Network}
\nomenclature[Z]{MLS}{Multilingual LibriSpeech}
\nomenclature[Z]{MFA}{Montreal Forced Aligner}